[{"body":" Application Deployment 🔗 Traditional Deployment 🔗 Early on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a physical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can be instances where one application would take up most of the resources, and as a result, the other applications would underperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources were underutilized, and it was expensive for organizations to maintain many physical servers.\nVirtualized Deployment 🔗 As a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical server’s CPU. Virtualization allows applications to be isolated between VMs and provides a level of security as the information of one application cannot be freely accessed by another application. Virtualization allows better utilization of resources in a physical server and allows better scalability because an application can be added or updated easily, reduces hardware costs, and much more. With virtualization, you can present a set of physical resources as a cluster of disposable virtual machines. Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware.\nContainer Deployment 🔗 Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications. Therefore, containers are considered lightweight. Similar to a VM, a container has its own file system, share of CPU, memory, process space, and more. As they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions.\nApp Deployment 🔗 Video: Application Deployment Overview Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Application Deployment 🔗 Traditional Deployment 🔗 Early on, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/containers/content/application-deployment/","tags":"","title":"Application Deployments"},{"body":"Get an overview of the existing Kubernetes certifications and what you need to learn for the CKA.\nSeveral certifications available 🔗 Certification Type Badge Kubernetes and Cloud Native Associate (KCNA) MCQ Kubernetes and Cloud Native Security Associate (KCSA) MCQ Certified Kubernetes Application Developer (CKAD) Practice Certified Kubernetes Administrator (CKA) Practice Certified Kubernetes Security Specialist (CKS) Practice * passing the CKA is a requirement before passing the CKS\nIf you pass all those certifications, you become a Kubestronaut.\nExpectation for the CKA 🔗 The following table summarizes the distribution of the CKA questions across 5 main subjects.\nSubject % Cluster Architecture, Installation \u0026 Configuration 25% Workloads \u0026 Scheduling 15% Services \u0026 Networking 20% Storage 10% Troubleshooting 30% CKA Environment 🔗 The CKA is a 2h exam. It contains 15/20 questions and requires at least 66% correct answers. This exam is remotely proctored, so you can take it from home (or any other quiet location) at a time that best suits your schedule.\nBefore launching the exam, which you do via your Linux Foundation Training Portal, you need to perform a couple of prerequisites including making sure the PSI Browser works correctly on your environment. This browser gives you access to the remote Desktop you’ll use during the exam.\nTips \u0026 tricks 🔗 Tools 🔗 Make sure you have a basic knowledge of\nvim openssl # Visualize the content of a certificate openssl x509 -in cert.crt -noout -text systemd / systemctl / journalctl # Restart kubelet systemctl restart kubelet # Check kubelet logs journalctl -u kubelet Aliases 🔗 Defining a couple of aliases at the very beginning of the examination could save time.\nalias k=kubectl export dr=\"--dry-run=client -o yaml\" export fd=\"--grace-period=0 --force\" Imperative commands 🔗 Don’t create specifications manually, instead use --dry-run=client -o yaml as in these examples.\nk run nginx --image=nginx:1.20 --dry-run=client -o yaml \u003e pod.yaml k create deploy www --image=nginx:1.20 --replicas=3 --dry-run=client -o yaml \u003e deploy.yaml k create role create-pod --verb=create --resource=pods --dry-run=client -o yaml \u003e role.yaml Quickly change the current Namespace.\nk config set-context --current --namespace=dev Don’t wait for the grace period to get rid of a Pod.\nk delete po nginx --force --grace-period=0 Reference guide 🔗 The Kubectl quick reference guide is a must-read.\nAccess to exam simulator 🔗 Registering for the CKA gives you access to two sessions of the official Exam simulator. I highly recommend using these sessions once you’re almost ready.\n","categories":"","description":"Get an overview of the existing Kubernetes certifications and what you need to learn for the CKA.","excerpt":"Get an overview of the existing Kubernetes certifications and what you …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/certifications/","tags":"","title":"Certifications"},{"body":"","categories":"","description":"Learn the fundamentals of cloud computing and how to deploy applications in the cloud.","excerpt":"Learn the fundamentals of cloud computing and how to deploy …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud/","tags":"","title":"Cloud"},{"body":" Definition 🔗 Cloud computing is a technology that allows users to access computing resources, such as servers, storage, and applications, over the Internet, without the need for on-site hardware or infrastructure.\nThat enables users to scale their computing resources up or down as needed and pay only for their use. It also allows for greater flexibility, mobility, and collaboration. In addition, users can access their data and applications anywhere with an Internet connection.\nCloud computing is used by businesses, individuals, and organizations of all sizes and across all industries to increase agility, improve efficiency and reduce costs.\n","categories":"","description":"","excerpt":" Definition 🔗 Cloud computing is a technology that allows users to …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud/content/cloud-computing/","tags":"","title":"Cloud Computing"},{"body":" Explained 🔗 Cloud-native refers to a software development approach that leverages cloud computing services and infrastructure to build and deploy applications. It involves designing applications as a collection of microservices that can run independently and scale dynamically, using containerization and orchestration technologies like Kubernetes.\nCloud-native applications are built to be resilient, scalable, and highly available and are often deployed using automated continuous delivery pipelines.\nWhy Cloud-Native? 🔗 Scalability: Cloud-native applications are designed to scale dynamically, allowing organizations to handle increased traffic and demand without downtime or performance issues.\nAgility: Cloud-native applications can be developed, tested, and deployed quickly, allowing organizations to respond rapidly to changing market needs and customer feedback.\nResilience: Cloud-native applications are designed to be fault-tolerant and resilient, with built-in redundancy and failover mechanisms that minimize downtime and data loss.\nFlexibility: Cloud-native applications are designed to run on multiple cloud platforms and environments, allowing organizations to choose the best option for their needs and avoid vendor lock-in.\nCost-efficiency: Cloud-native applications can be deployed and scaled automatically, reducing the need for manual intervention and minimizing operational costs.\nOverall, cloud-native is essential because it enables organizations to deliver software faster, with higher quality and reliability, while reducing costs and increasing agility.\n","categories":"","description":"","excerpt":" Explained 🔗 Cloud-native refers to a software development approach …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud-native/content/cloud-native/","tags":"","title":"Cloud-Native"},{"body":" History 🔗 Kubernetes also called k8s (a “k” followed by 8 chars and a “s”) or simply kube means “Helmsman” in Greek. It is a container orchestrator inspired by Google Borg System which were orchestrating billions of containers on Google infrastructure.\nVersion v1.0.0 of Kubernetes was released in July 2015, the last version as of today (October 2024) is v1.31.1. The release cycle is quite fast with 3 minor releases per year.\nMain functionalities 🔗 Kubernetes is a container orchestrator offering main functionalities, such as:\nManagement of applications running in containers Self-healing Service discovery Usage of Secrets and Configurations Long-running process and batch jobs Role Based Access Control (RBAC) Storage Orchestration Manages applications in production 🔗 Major project in the open-source ecosystem 🔗 Kubernetes is the first graduated project within the CNCF, it was followed by major players like etcd and Prometheus\nWhat is a Kubernetes cluster made of ? 🔗 A Kubernetes cluster is composed of nodes, where a node is either a virtual machine or a bare metal server. A node can belong to the Control Plane which run processes in charge of managing the cluster and the applications running on it. Or, a node can be a Worker dedicated to run Pods, a group of containers sharing a network stack and storage.\nHow to access a cluster 🔗 A cluster usually comes with a kubeconfig file which contains all the information to communicate with the cluster API Server. This file can be used to configure the standard kubectl binary to manage the cluster. The kubeconfig file can also be used with tools like k9s, Mirantis Lens, … which give a higher level view of the cluster.\nVarious workload resources for different use cases 🔗 To run a Pod we often rely on a higher level resource, instead of running it directly. The workload resources are:\nDeployment : web server DaemonSet : one agent per node Job / CronJob : batch StatefulSet : stateful application A request that reaches a Service is load-balanced between the exposed Pods\nA Pod can use several resources\nConfigMap : contains configuration data Secret : contains sensitive data PersistentVolumeClaim / PersistentVolume : storage management Several types of resources 🔗 An application runs in a Namespace 🔗 Resource creation 🔗 Each resource is defined in a YAML specification which is sent to the API Server using the kubectl binary.\napiVersion: v1 kind: Pod metadata: name: www spec: containers: - name: www image: nginx:1.24 kubectl apply -f www.yaml ","categories":"","description":"Kubernetes Basic Concepts","excerpt":"Kubernetes Basic Concepts","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/concepts/","tags":"","title":"Concepts"},{"body":"","categories":"","description":"Manage configuration and sensitive data","excerpt":"Manage configuration and sensitive data","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/configuration/","tags":"","title":"Configuration"},{"body":" Overview 🔗 Terms \u0026 Conditions - client and supplier relations 🔗 Exoscale’s Terms \u0026 Conditions - exoscale.com/terms T\u0026C’s Categories 🔗 Definitions The Services Fees and Payment Modalities Service Level Agreement (SLA) Use of the Services Software Licenses Proprietary Rights Confidentiality Warranties Indemnification Publicity Force Majeure Term and Termination Miscellaneous Provisions Data Privacy - information usage and handling 🔗 Exoscale’s Privacy Policy - exoscale.com/privacy Privacy Policy 🔗 Exoscale collects only information you choose to give us and processes it with your consent or on a legal basis. Therefore, we gather as little personal information as we can to allow us to provide our services.\nPersonal information is not sold to third parties and is used and processed as described on this page. Furthermore, we comply with the EU’s Privacy Shield Framework and the General Data Protection Regulation (GDPR). As a result, we provide the same level of privacy protection regardless of your country of origin or location.\nJurisdiction - laws and courts 🔗 Exoscale’s Jurisdiction \u0026 Governing Law (T\u0026C 14.8) 🔗 These Terms and Conditions and any Order shall be governed by the laws of Switzerland. The Parties irrevocably submit to the exclusive jurisdiction of the courts of the canton of Vaud, district of Lausanne.\n","categories":"","description":"","excerpt":" Overview 🔗 Terms \u0026 Conditions - client and supplier relations 🔗 …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/contractual-setup/content/setup/","tags":"","title":"Contractual Setup"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/corporate-sustainability/","tags":"","title":"Corporate Sustainability"},{"body":" Why? 🔗 Long-term Viability: Businesses prioritizing sustainability are better equipped to navigate the risks associated with resource scarcity, climate change, and evolving regulatory landscapes. A sustainable approach helps ensure long-term viability in a rapidly changing world.\nCompetitive Advantage: As consumers, investors, and other stakeholders increasingly value sustainability, companies committed to sustainable practices can differentiate themselves in the marketplace. This can translate into increased brand loyalty, attract investment, and open new markets.\nOperational Efficiency: Sustainable practices often increase operational efficiency and cost savings. Initiatives like energy conservation, waste reduction, and sustainable sourcing can reduce expenses and improve profitability.\nInnovation and Agility: Focusing on sustainability can drive innovation by pushing companies to rethink products, services, and processes. It encourages a culture of continuous improvement and agility, allowing businesses to adapt and thrive amidst changing conditions.\nRisk Management: Addressing sustainability issues helps companies identify and mitigate risks before they escalate. This includes risks related to climate change, resource depletion, social inequality, and changes in regulatory requirements.\nSocial and Environmental Responsibility: Companies must minimize adverse environmental and societal impacts. Adopting sustainable practices demonstrates a commitment to being a responsible corporate citizen, benefiting the company and the wider community.\nDefinition 🔗 Corporate Sustainability is a way of doing business that focuses on creating value for everyone involved, not just short-term profits. This holistic approach recognizes the interdependence of companies with societal and ecological systems.\nIt’s a positive and hopeful approach that can help companies make a difference.\nIt implements strategies focusing on business’s ethical, social, environmental, cultural, and economic dimensions.\nIt surpasses compliance with regulatory requirements and strives to enhance a business’s social and environmental performance alongside its financial performance.\nIt strives to minimize negative impacts while maximizing positive contributions, paving the way for a sustainable economy.\n","categories":"","description":"","excerpt":" Why? 🔗 Long-term Viability: Businesses prioritizing sustainability …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/corporate-sustainability/content/corporate-sustainability/","tags":"","title":"Corporate Sustainability (CS)"},{"body":" Introduction 🔗 Creating designs in Kanvas is a seamless process that enables you to visualize, configure, and manage cloud-native infrastructure efficiently. This chapter will guide you through various methods of creating designs, whether starting from scratch, importing configuration files like Helm charts or Kubernetes manifests, or utilizing published designs from the Layer5 Cloud catalog.\nAccessing Designer Mode 🔗 Log in to Meshery Playground. After successfully logging in, you’ll land on the dashboard.\nIn the Cloud Native Playground tile, click Explore to navigate to Kanvas.\nOn the left sidebar, click the Kanvas icon to open Kanvas in Designer mode. Starting from Scratch 🔗 Think of Designer mode as your blueprint studio. Just like architects use specific tools to design buildings, you use Kanvas to design your cloud-native applications.\nStep 1: Create And Name Your Design\nClick New at the top right of the screen to open a new design canvas.\nGive your design a meaningful name, reflecting its purpose or key components.\nStep 2: Browsing Available Components\nExplore the extensive Dock in Designer mode, which offers a wide range of Kubernetes and other integration components.\nThink of the Dock as a palette of building blocks for your cloud-native app. Just like selecting different materials for a construction project, you choose components for your app’s architecture.\nComponents can be found in the Dock at the bottom of the design canvas.\nStep 3: Selecting Specific Versions\nTo ensure precision in your design, select the specific version of each component you need. Kanvas supports a variety of versions for different components, allowing you to tailor your design to meet your exact requirements. A compatibility check is conducted, ensuring that selected versions align seamlessly within your design.\nChoosing component versions is similar to selecting software versions for your cloud-native app. It’s like picking the right versions of libraries to ensure your app functions smoothly.\nStep 4: Building Complex Cloud Native Deployments\nWith the Designer toolbox at your disposal, effortlessly construct complex cloud-native deployments. Simply drag and drop components onto the Kanvas canvas, arranging and configuring them in a way that suits your architecture.\nStep 5: Configuring Each Component\nConfigure your design by adjusting each component through the Configuration tab. Click on the component to open up its Configuration tab. Kanvas’s intuitive interface allows you to enter settings and parameters directly on the design canvas. As you modify these configurations, your design updates in real-time. Kanvas’s auto-sync feature ensures that configuration changes are instantly reflected in the design.\nStep 6: Design Actions and Interactions\nExplore additional design actions by right-clicking on components. Group components together or establish connections between them according to your needs. Kanvas responds in real-time to design actions, providing a fluid and responsive user experience.\nStarting from existing infrastructure files 🔗 When working with Kanvas, you can effortlessly integrate various design files to start building and managing your cloud-native applications. Whether you’re working with Helm charts, Docker Compose files, or Kubernetes manifests, Kanvas provides a unified interface to import these files and visualize your infrastructure setup. This versatility allows you to streamline the design process, ensure consistency, and tailor your cloud-native environments to meet specific needs.\nImporting a Design\nClick the Import design option under the Designs tab in Kanvas. Enter a name for the design in the Design File Name field.\nChoose the appropriate Design Type for the file you want to import (Helm Chart, Kubernetes, Manifest, etc).\nSelect your preferred import method: either URL or File Upload.\nExample: Use the Meshery Server Helm chart at Meshery Helm Chart. Review and Modify: You should now have a Kanvas design of the chart. Continue to make any changes if required or deploy it. Cloning a Design 🔗 Steps to Clone a Design from Catalog using Kanvas:\nSwitch to Kanvas Designer mode, if not already in it.\nIn the left navigation panel, click the “Catalog” menu tab.\nSelect a design from the list that appears in the panel.\nA modal will pop up requesting you to clone (create a copy of) the design you’ve selected.\nClick “clone” and a copy of the design will appear on your Kanvas canvas. You can then configure the design to suit your purposes.\nFind your newly cloned design in the list of designs in the left navigation panel.\nGithub Integrations 🔗 Integrating your GitHub account with Meshery unlocks the ability to import files as designs directly from your repositories, enhancing your collaboration and version control processes. This integration allows you to streamline your workflow, ensuring that your design infrastructure is always up-to-date and aligned with your source code. To get started with integrating GitHub into Meshery, follow the comprehensive guide available here.\nAfter importing your designs, they will appear in the catalog.\nTo view the design on the canvas:\nClick on the Catalog Card.\nClick on Open in Playground. This will display the design on the canvas.\nMerging Designs 🔗 For enhanced collaboration and scalability, merge existing designs into your canvas by dragging and dropping. This feature enables you to consolidate multiple designs, creating a unified and comprehensive view of your infrastructure.\nKanvas orchestrates the merging process internally, creating a unified design that incorporates components from the dragged designs. Importantly, users do not lose their original designs during the merging process. Kanvas retains records of each design, preserving their individuality. To learn how to merge designs, see Merging Designs.\nConclusion 🔗 In this chapter, we explored the comprehensive process of creating cloud-native designs using Kanvas. We covered how to start a design from scratch by naming, configuring, and arranging components in Designer mode. We also demonstrated how to import existing infrastructure files, such as Helm charts or Kubernetes manifests, directly into your design canvas for efficient deployment and management.\nAdditionally, we learned how to clone existing designs from the Layer5 Cloud catalog and integrate them into our workspace. The chapter also delved into the integration of GitHub, allowing seamless import of design files from your repositories, enabling better collaboration and version control.\n","categories":"","description":"This chapter covers the essential steps for creating cloud-native designs in Kanvas. It explores how to start from scratch, import existing infrastructure files, and clone or merge designs from the Layer5 Cloud catalog. Additionally, it provides guidance on integrating GitHub for seamless version control and collaboration, allowing users to manage and configure their cloud-native deployments effectively.","excerpt":"This chapter covers the essential steps for creating cloud-native …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-meshery/introduction-to-meshery/creating-designs/","tags":"","title":"Creating Designs"},{"body":" Explained 🔗 Integrating ethical, social, environmental, and economic dimensions into the business model is the ultimate corporate sustainability quest, and cloud computing is a powerful ally.\nCorporate Sustainability and Cloud Computing intersect significantly, influencing how businesses commit to reducing their environmental footprint while optimizing operational efficiency and innovation.\nHere’s how cloud computing intersects with and can dramatically enhance corporate sustainability efforts:\nEnergy Efficiency and Carbon Footprint Reduction: Cloud data centers often run on highly optimized and energy-efficient infrastructures. By pooling resources for multiple clients, these centers drive higher utilization rates and lower energy consumption per workload than traditional on-premises data centers. This operational efficiency translates to a smaller carbon footprint for cloud services businesses.\nGreen Energy Usage: Many leading cloud providers are increasingly powering their data centers with renewable energy sources. Businesses leveraging cloud services indirectly contribute to a higher demand for renewable energy, thus supporting the growth of green energy sectors.\nResource Optimization: Cloud computing provides scalable resources, meaning businesses only use what they need. This leads to more efficient resource use and reduces wasteful spending on idle IT resources. It also helps companies avoid over-provisioning and underutilizing physical hardware, which can conserve energy and materials.\nDematerialization: By moving to cloud-based solutions, companies can reduce their reliance on physical products and infrastructure. This reduces the raw materials, energy, and waste associated with the entire lifecycle of these products—from manufacture to disposal. Services like cloud storage, online collaboration tools, and virtual meetings decrease the need for physical products and spaces.\n**Enabling Remote **Work: Cloud computing facilitates remote work, significantly reducing the need for commuting and the associated greenhouse gas emissions. This has been particularly evident during and following the global shifts caused by the COVID-19 pandemic, showcasing cloud computing’s role in sustaining business operations in an environmentally friendly manner.\n**Data-Driven Sustainability **Initiatives: Cloud computing’s analytical capabilities allow businesses to collect, store, and analyze large volumes of data to monitor and improve their sustainability efforts. This includes real-time monitoring of energy consumption, optimizing supply chains for sustainability, and predicting trends to better align with sustainability goals.\nLifecycle Management and e-Waste Reduction: Transitioning to cloud services can help companies extend the lifecycle of their IT assets and reduce e-waste. Cloud providers are responsible for maintaining, upgrading, and responsibly disposing of or recycling hardware, following more standardized and potentially greener practices than companies might achieve.\nFor businesses looking to enhance their corporate sustainability, selecting cloud providers that prioritize environmental sustainability, use renewable energy, and have a transparent sustainability reporting process is essential. This ensures their move to the cloud aligns with broader sustainability goals and contributes positively to their overall environmental impact.\n","categories":"","description":"","excerpt":" Explained 🔗 Integrating ethical, social, environmental, and economic …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/cs-and-cloud/content/cs-and-cloud/","tags":"","title":"CS \u0026 Cloud"},{"body":" Explained 🔗 The European Union’s Corporate Sustainability Reporting Directive (CSRD) is a significant regulatory development aimed at enhancing and standardizing the sustainability reporting by companies.\nBuilding on the foundations of the Non-Financial Reporting Directive (NFRD), the CSRD was proposed by the European Commission in April 2021 as part of the EU’s Green Deal and Sustainable Finance Agenda. Its objective is to improve the flow of sustainability information in the global economy, thus making it easier for investors, consumers, policymakers, and other stakeholders to assess the sustainability performance of companies. This, in turn, empowers these stakeholders to make more informed decisions and drive sustainable practices.\nThe CSRD’s implementation timeline was initially set for reporting in 2024, covering the financial year 2023, for companies already subject to the NFRD. However, due to the complexity of the standards and the need for companies to adopt and prepare legally, there has been an indication of potential delays. New companies are expected to report under the CSRD from 2025 (covering the financial year 2024) and onwards, depending on their size and whether they are currently subject to the NFRD.\nThe CSRD is a cornerstone of the EU’s agenda for sustainable finance. It is expected to be crucial in moving towards a greener and more sustainable economy by ensuring transparency and accountability and ultimately driving corporate behavior toward sustainability.\n","categories":"","description":"","excerpt":" Explained 🔗 The European Union’s Corporate Sustainability Reporting …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd/content/explained/","tags":"","title":"CSRD"},{"body":" Experience 🔗 Exoscale, a European cloud service provider, has developed a pioneering tool, CloudAssess, in collaboration with Resilio and Kleis, to show the environmental impact of customer’s cloud usage transparently.\nExoscale integrated CloudAssess into its system, enriching billing data with lifecycle assessment (LCA) sustainability insights. Despite challenges in data acquisition and real-time reporting, they adapted processes to meet LCA needs, enhancing sustainability.\nThis effort showcases Exoscale’s commitment to environmental responsibility and positions it as a sustainability leader in cloud services. Their success demonstrates the viability of sustainable operations and serves as an example for other providers.\n","categories":"","description":"","excerpt":" Experience 🔗 Exoscale, a European cloud service provider, has …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd-and-exoscale/content/experience/","tags":"","title":"CSRD \u0026 Exoscale"},{"body":" Explained 🔗 The CSRD promotes enhanced transparency and accountability, urging cloud providers to adopt greener practices and align with global sustainability goals, aiding the shift to a sustainable digital economy.\nImpacts 🔗 Expanded Reporting Preparing for Compliance Elevating Sustainability Realignments and Opportunities The CSRD is not just a regulatory hurdle for cloud providers but a doorway to innovation and competitive advantage in a market that increasingly values sustainability. By aligning their operations with the goals of the CSRD, cloud providers can enhance their corporate reputation, tap into new customer segments, and contribute significantly to the transition towards a sustainable digital economy.\nExpanded Reporting 🔗 Broader Disclosure Requirements 🔗 Large or EU-listed cloud computing providers must now create detailed reports on their environmental, social, and governance (ESG) efforts, focusing on energy use, carbon footprint, and resource efficiency.\nAssurance and Reliability 🔗 With the CSRD requiring verified sustainability reports, cloud providers must prove their claims about energy efficiency and renewable energy use. This leads to closer examination of their reports and potentially better environmental results.\nPreparing for Compliance 🔗 Internal Processes and Controls 🔗 Cloud computing providers can improve their tracking and managing sustainability methods, such as using better environmental metrics, external audits, and eco-focused management systems. With good practices in place, meeting CSRD requirements can benefit a business’s long-term sustainability.\nCollaboration and Innovation 🔗 Providers are urged to work more closely with energy companies, tech partners, and industry groups to find and use new solutions that lessen environmental harm, like using advanced cooling systems or building more efficient data centers.\nElevating Sustainability 🔗 Emphasis on Double Materiality 🔗 Cloud providers must report on how environmental factors affect their finances and how their operations impact society and the environment, including data center operations, energy use, and carbon emissions. Focus on Digital Reporting\nDue to CSRD’s digitalization rules, cloud providers must implement digital tools for gathering, analyzing, and reporting sustainability information. This will likely lead to more automated and efficient reporting, fitting their tech-focused operations.\nRealignments and Opportunities 🔗 Sustainable Cloud Solutions 🔗 The CSRD offers cloud providers a chance to innovate and provide greener cloud services. By focusing on renewable energy, energy efficiency, and waste reduction, providers can meet CSRD standards and attract eco-aware customers, potentially increasing their market presence.\nCompetitive Differentiation 🔗 Adhering to the CSRD’s strict sustainability requirements can set cloud providers apart in the competitive market, attracting customers and giant firms aiming to boost their environmental credentials by selecting eco-friendly suppliers.\n","categories":"","description":"","excerpt":" Explained 🔗 The CSRD promotes enhanced transparency and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd-and-providers/content/csrd-and-providers/","tags":"","title":"CSRD \u0026 Providers"},{"body":" Data 🔗 Data is the fuel of our society. Can we agree on that? There were many bold statements over the years to underline the importance of data. Data is the new oil; Data is the new gold; all variations you have probably come across if you follow the news, media, or social media. Data is the new water - is a fresh analogy you can find in researching the data topic.\n\" Like water, data needs to be accessible, it needs to be clean, and it is needed to survive.\" - Dan Vesset (IDC)\nData Word Cloud\nThe water analogy is probably more appealing because we should be more fond of water than gold or oil for many reasons, but it is pretty evident from a pure survival point of view. We are getting back to our core topic data and databases. The collection of (important) data in the past and storing it in databases for reuse and archiving it for preservation is as old as humanity. For example, the Sumerians already used clay tablets to keep the index of medical prescriptions as a form of database, so databases started long before computers were even invented. The increase in the volume of data we produce every year has reached an almost frightening value. The digital revolution induces the reason for this exponential increase in data production. Keeping track and oversight in this data situation, we have to improve and invent new data tools, or otherwise, we would get lost in data.\n","categories":"","description":"","excerpt":" Data 🔗 Data is the fuel of our society. Can we agree on that? There …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/databases/content/data/","tags":"","title":"Data"},{"body":"In this tutorial, you will learn how to deploy a WordPress site and a MySQL database with Persistent Volumes using Meshery Playground. Meshery Playground is an interactive and collaborative live cluster environment that simplifies the deployment process and enhances user experience by providing visual tools for managing Kubernetes resources.\nNOTE: If this is your first time working with Meshery Playground, consider starting with the Exploring Kubernetes Pods with Meshery Playground tutorial first.\nPrerequisites 🔗 Basic understanding of Kubernetes concepts. Access to the Meshery Playground. If you don’t have an account, sign up at Meshery Playground. Lab Scenario 🔗 Import the WordPress and MySQL manifest files into Meshery Playground. Create persistent volumes and a secret for the resources using the visual tools provided by Meshery. Deploy these resources on the playground. In this lab, you will import the WordPress and MySQL manifest files into Meshery Playground. You will visualize these Kubernetes resources and create persistent volumes for them using the visual tools provided by Meshery. Finally, you will deploy these resources on the Playground.\nObjective 🔗 Learn how to import manifest files, visualize Kubernetes resources, create new resource components, and deploy the application using Meshery Playground.\nSteps 🔗 Download the Kubernetes Configuration Files 🔗 Go ahead and download these yaml files mysql-deployment.yaml and wordpress-deployment.yaml\nThese YAML files contain the Service definitions, Persistent Volume Claims, and Deployment configurations for the WordPress site and the MySQL database.\nAccess Meshery Playground 🔗 Log in to the Meshery Playground using your credentials. On successful login, you should be at the dashboard. Press the X on the Where do you want to start? popup to close it (if required). Click Explore in the Cloud Native Playground tile to navigate to Kanvas ","categories":"","description":"Get started with Meshery playground","excerpt":"Get started with Meshery playground","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/deploying-wordpress-and-mysql-with-persistent-volumes-with-meshery/sql/introduction/","tags":"","title":"Deploying WordPress and MySQL with Persistent Volumes with Meshery"},{"body":" History 🔗 Why Exoscale? 🔗 Exoscale is a European cloud provider focusing on scalability, simplicity and security. When running mission critical production workloads in the cloud, a partner you can rely on makes all the difference.\nSKS - Managed Kubernetes! 🔗 A strong reason to choose Exoscale.\n","categories":"","description":"","excerpt":" History 🔗 Why Exoscale? 🔗 Exoscale is a European cloud provider …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/exoscale/content/exoscale/","tags":"","title":"Exoscale"},{"body":" Exoscale DBaaS Benefits 🔗 Exoscale DBaaS is an excellent solution for everyone looking for a diverse portfolio of open-source data services used in all types of applications and business solutions.\nIf you want to know more about the actual service offers, please go to Exoscale DBaaS on our website.\nExoscale DBaaS is deeply integrated into the Infrastructure as a Service platform and is easy to use with well-known user interfaces like the CLI, the API, and Terraform automation to-do your Infrastructure as Code (IaC) as usual.\nQuick Start 🔗 Set up your database within minutes. Focus on your application; we take care of the rest. Open Source\nChoose between a wide range of open-source databases: PostgreSQL, Apache Kafka, Redis and many more to come. Fully Managed\nYou don’t have to care about maintenance or upgrades. Launch your database; we take care of the rest.\n","categories":"","description":"","excerpt":" Exoscale DBaaS Benefits 🔗 Exoscale DBaaS is an excellent solution for …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/exoscale-dbaas/content/benefits/","tags":"","title":"Exoscale DBaaS Benefits"},{"body":" The Five Pillars 🔗 Frameworks \u0026 Certifications 🔗 ","categories":"","description":"","excerpt":" The Five Pillars 🔗 Frameworks \u0026 Certifications 🔗 ","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/exoscale-compliance/content/strategy/","tags":"","title":"Exoscale's Strategy"},{"body":" Why Containers? 🔗 Many people have a complex job building, testing, and running systems based on old concepts and technology in today’s IT — the results you can see below (Experiences we want to change!). Modern ways to build, test, and run IT systems are not adopted as broadly as possible. Hence, a lot of pain is still out there that we could ease by changing IT processes and utilizing the new ways.\nExperiences we want to change! 🔗 BUILD - Life of a Developer 🔗 Here you see a developer who brought her code into production, only to find out that it didn’t run like on her laptop …\nTEST - Life of System Architects 🔗 Here you can see system architects who mistakenly made a load test on a production server …\nRUN - Life of a System Administrator 🔗 Here you can see a system administrator who was woken at 3 am to restart a process …\n","categories":"","description":"","excerpt":" Why Containers? 🔗 Many people have a complex job building, testing, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-containers/content/experiences-we-want-to-change/","tags":"","title":"Experiences we want to change!"},{"body":" Overview 🔗 In this tutorial, you will explore Edge Stack using Meshery Playground. You’ll get hands-on experience importing YAML files that contain Edge Stack configurations and Custom Resource Definitions (CRDs).\nYou’ll use Kanvas to view, configure, and deploy Kubernetes resources, including Edge Stack with its custom resources and a sample application. Additionally, you’ll leverage Meshery’s visualization tool to filter the deployed resources in your cluster.\nObjectives 🔗 This tutorial will guide you through exploring Edge Stack using Meshery Playground. You’ll cover:\nImporting YAML Files: Learn how to import YAML files containing Edge Stack configurations and CRDs (Custom Resource Definitions) into Kanvas. Design Configuration: Configure a Kubernetes component on the Playground Canvas. Deployment: Deploy Edge Stack alongside a sample application and custom resources. Visualization: Get hands-on with Kanvas to understand the components and their interactions within Edge Stack. Design Interpretation: Explore some of the relationships between Edge Stack resources with a Kanvas design, explaining their functionalities and roles within the system. Meshery Playground and Kanvas 🔗 Meshery Playground is an interactive and collaborative live cluster environment that simplifies the deployment process and enhances user experience by providing visual tools for managing Kubernetes resources.\nKanvas is a visual tool within Meshery that allows users to interact with and manage Kubernetes resources. It provides a graphical interface for viewing, configuring, and deploying resources, making it easier to understand the relationships and interactions between different components in a Kubernetes cluster.\nIf this is your first time working with Meshery Playground, consider starting with the Exploring Kubernetes Pods with Meshery Playground tutorial first.\nAmbassador Edge Stack 🔗 Ambassador Edge Stack is a Kubernetes-native API Gateway built on Envoy Proxy. It streamlines Kubernetes workflows for configuration, deployment, and management of APIs, efficiently and securely managing high traffic volumes and distributing requests across multiple services in your cluster.\nThe “stack” includes security functionalities such as automatic TLS, authentication, and rate limiting, as well as load balancing, ingress control and observability all integrated to ensure scalability and flexibility in Kubernetes environments.\nPrerequisites 🔗 Basic understanding of Kubernetes concepts. Access to the Meshery Playground. If you don’t have an account, sign up at Meshery Playground. Ambassador Cloud account. Access Meshery Playground 🔗 Log in to the Meshery Playground using your credentials. Click Explore in the Cloud Native Playground tile to navigate to Kanvas. ","categories":"","description":"Ambassador Edge Stack is a powerful API gateway built on Envoy Proxy. It simplifies the configuration, deployment, and management of APIs in Kubernetes environments. With features like automatic TLS, authentication, rate limiting, load balancing, and observability, Ambassador Edge Stack ensures scalability and flexibility for high traffic volumes and distributed requests across multiple services in your cluster.","excerpt":"Ambassador Edge Stack is a powerful API gateway built on Envoy Proxy. …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/ambassador-edge-stack-api-gateway-with-meshery/edge/introduction/","tags":"","title":"Exploring Edge Stack with Meshery Playground"},{"body":" Setup Istio 🔗 Now that we have a Kubernetes cluster and Meshery, we are ready to download and deploy Istio resources.\nSteps 🔗 Install Istio Verify install Optional (manual install of Istio):\nDownload Istio resources Setup istioctl Install istio Install Istio 🔗 Using Meshery, select Istio from the Management menu.\nIn the Istio management page:\nType istio-system into the namespace field. Click the (+) icon on the Install card and select Latest Istio to install the latest version of Istio. Alternative:Manual installation 🔗 Perform the below steps if the above steps doesn’t work for you.\nDownload Istio 🔗 You will download and deploy the latest Istio resources on your Kubernetes cluster.\nNote to Docker Desktop users: Please ensure your Docker VM has at least 4GiB of Memory, which is required for all services to run.\nOn your local machine, execute:\ncurl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.7.3 sh - Setting up istioctl 🔗 On a *nix system, you can setup istioctl by doing the following:\nbrew install istioctl Alternatively, change into the Istio package directory and add the istioctl client to your PATH environment variable.\ncd istio-* export PATH=$PWD/bin:$PATH Verify istioctl is available:\nistioctl version Check if the cluster is ready for installation:\nistioctl verify-install Install Istio 🔗 To install Istio with a demo profile, execute the below command.\nistioctl install --set profile=demo Alternatively, with Envoy logging enabled:\nistioctl install --set profile=demo --set meshConfig.accessLogFile=/dev/stdout Verify install 🔗 In the Istio management page:\nClick the (+) icon on the Validate Service Mesh Configuration card. Select Verify Installation to verify the installation of Istio. Alternatively: 🔗 Istio is deployed in a separate Kubernetes namespace istio-system. To check if Istio is deployed, and also, to see all the pieces that are deployed, execute the following:\nkubectl get all -n istio-system ","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/advance-concepts-of-service-meshes/getting-started/","tags":"","title":"Getting Started with Istio"},{"body":" Setup Istio 🔗 Now that we have a Kubernetes cluster and Meshery, we are ready to download and deploy Istio resources.\nSteps 🔗 Install Istio Verify install Optional (manual install of Istio):\nDownload Istio resources Setup istioctl Install istio Install Istio 🔗 Using Meshery, select Istio from the Lifecycle menu.\nIn the Istio management page:\nType istio-system into the namespace field.\nClick the (+) icon on the Install card and click on Istio Service Mesh to install latest version of Istio.\nClick the Deploy button on the confirmation modal.\nAlternative:Manual installation 🔗 Perform the below steps if the above steps doesn’t work for you.\nDownload Istio 🔗 You will download and deploy the latest Istio resources on your Kubernetes cluster.\nNote to Docker Desktop users: Please ensure your Docker VM has at least 4GiB of Memory, which is required for all services to run.\nOn your local machine, execute:\ncurl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.7.3 sh - Setting up istioctl 🔗 On a *nix system, you can setup istioctl by doing the following:\nbrew install istioctl Alternatively, change into the Istio package directory and add the istioctl client to your PATH environment variable.\ncd istio-* export PATH=$PWD/bin:$PATH Verify istioctl is available:\nistioctl version Check if the cluster is ready for installation:\nistioctl verify-install Install Istio 🔗 To install Istio with a demo profile, execute the below command.\nistioctl install --set profile=demo Alternatively, with Envoy logging enabled:\nistioctl install --set profile=demo --set meshConfig.accessLogFile=/dev/stdout Verify instal 🔗 In the Istio management page:\nClick the (+) icon on the Validate Service Mesh Configuration card. Select Verify Installation to verify the installation of Istio. Alternatively: 🔗 Istio is deployed in a separate Kubernetes namespace istio-system. To check if Istio is deployed, and also, to see all the pieces that are deployed, execute the following:\nkubectl get all -n istio-system ","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/introduction-to-service-meshes/getting-started/","tags":"","title":"Getting Started with Istio"},{"body":" Important Building Blocks 🔗 An application running on Kubernetes is a workload. Whether your workload is a single component or several that work together, on Kubernetes, you run it inside a set of Pods. In Kubernetes, a Pod represents a set of running containers on your cluster.\nA critical fault on the node where your Pod runs means that all the Pods on that node fail. Kubernetes treats that level of failure as final: you would need to create a new Pod to recover, even if the node later becomes healthy. However, to make life easier, you don’t need to manage each Pod directly.\nInstead, you can use workload resources that address a set of Pods on your behalf. These resources configure controllers that ensure the correct number and right kind of Pods are running to match the state you specified. Kubernetes provides several built-in workload resources: Pods, ReplicaSet, Deployment, DaemonSet, Ingress, and CronJob, to name a few of those building blocks.\n","categories":"","description":"","excerpt":" Important Building Blocks 🔗 An application running on Kubernetes is a …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-building-blocks/content/imp-building-blocks/","tags":"","title":"Important Building Blocks"},{"body":"","categories":"","description":"New to container and container orchestration? This INTRO Kubernetes - Learning Path covers the foundational topics for a non-technical audience and conveys the benefits of containers and container orchestration for modern IT scenarios. It will help you learn the basics of terminology associated, understand the essential components' functions, and understand why these new technologies are so important.","excerpt":"New to container and container orchestration? This INTRO Kubernetes - …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/","tags":"","title":"INTRO Kubernetes"},{"body":"","categories":"","description":"If you're seeking an entry-level understanding of Exoscale's storage solutions, our Level 100 course is an ideal starting point for non-technical individuals. This course lays the foundation for all the pertinent general and cloud storage topics. You'll dive into the advantages and considerations regarding data storage, including performance, scalability, and integrity. We'll cover essential concepts and terminology to understand storage types and capabilities and explain why efficient and secure storage is crucial in modern IT infrastructures.","excerpt":"If you're seeking an entry-level understanding of Exoscale's storage …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/","tags":"","title":"INTRO Storage"},{"body":"In this tutorial, you will learn how to Install a Scalable PostgreSQL Distribution on Kubernetes with Cloud Native PostgreSQL using Meshery Playground. Meshery Playground is an interactive and collaborative live cluster environment that simplifies the deployment process and enhances user experience by providing visual tools for managing Kubernetes resources.\nNOTE: If this is your first time working with Meshery Playground, consider starting with the Exploring Kubernetes Pods with Meshery Playground tutorial first.\nPrerequisites 🔗 Basic understanding of Kubernetes concepts. Access to the Meshery Playground. If you don’t have an account, sign up at Meshery Playground. Lab Scenario 🔗 Import the CloudnativePG manifest files Import manifest files for sample application Deploy these resources on the playground Objective 🔗 Learn how to deploy and manage postgreSQL and a Python sample application using CloudNative PG and Meshery Playground. This tutorial will demonstrate how to import manifest files, visualize kubernetes resources, and observe the dynamic provisioning capabilities of CloudNativePG. By the end of this tutorial, you will have a clear understanding of how to leverage Meshery Playground for deployments and visualization.\nCloudNativePG 🔗 CloudNative PG is a level 5 Kubernetes operator that efficiently manages PostgreSQL clusters, ensuring high availability throughout their lifecycle. It offers seamless Kubernetes API integration, declarative configuration, advanced observability, and is secure by default. CloudNative PG encapsulates PostgreSQL within a Kubernetes-native framework, adhering to cloud-native principles for deployment, scaling, and management. It uses CRDs and Operator patterns for seamless integration with Kubernetes, enabling automated provisioning, scaling, and management of PostgreSQL clusters.\nAccess Meshery Playground 🔗 Log in to the Meshery Playground using your credentials. On successful login, you should be at the dashboard. Press the X on the Where do you want to start? popup to close it (if required). Click Explore in the Cloud Native Playground tile to navigate to Kanvas ","categories":"","description":"","excerpt":"In this tutorial, you will learn how to Install a Scalable PostgreSQL …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/scalable-postgres-with-cloudnativepg/postgres/introduction/","tags":"","title":"Introduction"},{"body":" ","categories":"","description":"Explore Meshery comprehensively in this introductory course. Cover foundational concepts, architectural components, and logical structures. Learn how to create, deploy, and interpret designs. Gain practical insights into configuring Meshery through workspaces and leverage its collaborative attributes to manage your infrastructure.","excerpt":"Explore Meshery comprehensively in this introductory course. Cover …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-meshery/introduction-to-meshery/","tags":"","title":"Introduction to Meshery"},{"body":"A course to help you setup a service-mesh and start using it to build real-life applications using Meshery as the cloud native management plane.\n","categories":"","description":"Learn how to setup a service mesh and deploy an application using it.","excerpt":"Learn how to setup a service mesh and deploy an application using it.","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/introduction-to-service-meshes/","tags":"","title":"Introduction to Service Meshes"},{"body":" Kubernetes ? 🔗 Google open-sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google’s experience running production workloads at scale with best-of-breed ideas and practices from the community.\nThe name Kubernetes originates from Greek, meaning helmsman or pilot.\nKubernetes is a portable, extensible, open-source platform for managing containerized workloads and services that facilitates both automation and declarative configuration.\n","categories":"","description":"","excerpt":" Kubernetes ? 🔗 Google open-sourced the Kubernetes project in 2014. …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-details/content/kubernetes/","tags":"","title":"Kubernetes ?"},{"body":" Kubernetes Concepts 🔗 In this section, we cover various concepts and explain how to leverage the declarative orchestration powers of Kubernetes. We show different usage scenarios of initial configurations and necessary update operations in an SKS Cluster that provides a convenient managed Kubernetes Service to run containerized workloads at scale.\nIngress Cluster Manifests Namespaces Updates ","categories":"","description":"","excerpt":" Kubernetes Concepts 🔗 In this section, we cover various concepts and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/kubernetes-concepts/","tags":"","title":"Kubernetes Concepts"},{"body":" Kubernetes Storage 🔗 In this section, we cover various aspects of storage in the context of Kubernetes. We show different usage scenarios of temporary and persistent storage supplied with respective volume versions and other storage technologies and the configurations of these variants.\nVolumes Persistent Volumes Storage Technologies ","categories":"","description":"","excerpt":" Kubernetes Storage 🔗 In this section, we cover various aspects of …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/storage/content/kubernetes-storage/","tags":"","title":"Kubernetes Storage"},{"body":"","categories":"","description":"This learning path provides an introduction to Kubernetes, focusing on its architecture, components, and how to manage clusters effectively.","excerpt":"This learning path provides an introduction to Kubernetes, focusing on …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/","tags":"","title":"Kubernetes Workshop"},{"body":" Managed Kubernetes 🔗 Suppose you don’t have the time, the budget, and the human resources to master all the complexity of Kubernetes on your own. In that case, your best option is to select a managed alternative to benefit from the power of Kubernetes in your developer and infrastructure platforms.\nA simple who is responsible for what and pain versus gain comparison from available solutions should paint a clear picture of the golden mean, which is managed Kubernetes for most newcomers to the topic at hand.\nTo say that Kubernetes is the leading container orchestration platform is an understatement. It’s the only container orchestration platform that counts. However, to say that Kubernetes is complex is also an understatement. The learning curve is both steep and long.\n","categories":"","description":"","excerpt":" Managed Kubernetes 🔗 Suppose you don’t have the time, the budget, and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-managed-kubernetes/content/managed-kubenetes/","tags":"","title":"Managed Kubernetes"},{"body":" ","categories":"","description":"Learn all about Meshery","excerpt":"Learn all about Meshery","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-meshery/","tags":"","title":"Mastering Meshery"},{"body":" What is Meshery? 🔗 Meshery is a self-service engineering platform that enables collaborative design and operation of cloud and cloud-native infrastructure. It’s a versatile tool designed to help engineers manage and operate their infrastructure visually, collaboratively, and confidently. Whether you are a platform engineer, a site reliability engineer, or part of a DevSecOps team, Meshery has something to offer.\nMeshery’s Purpose 🔗 Meshery’s primary purpose is to facilitate the collaborative design, operation, and management of cloud and cloud-native infrastructure. By providing a unified platform that integrates with various tools and technologies, Meshery aims to streamline infrastructure management tasks for engineers. Its goals include:\nEnhancing Collaboration: Meshery allows teams of engineers to work together more effectively by providing a visual and intuitive interface for managing infrastructure. Simplifying Complexity: It abstracts the complexity of managing multiple Kubernetes clusters and cloud-native environments, making it easier for engineers to deploy, monitor, and manage applications. Ensuring Extensibility: As an open-source project, Meshery is highly extensible, allowing users to customize and extend its functionality to meet their specific needs. Improving Performance Management: Meshery provides tools for performance analysis, helping teams understand and optimize the performance of their applications and infrastructure. Promoting Best Practices: Through its various features and capabilities, Meshery encourages the adoption of best practices in cloud-native infrastructure management, such as using standardized designs and patterns. Meshery is designed to be a versatile and powerful tool that addresses the modern needs of infrastructure management in cloud-native environments, making it easier for engineers to manage their infrastructure with confidence and efficiency.\nArchitectural Components 🔗 Architectural components in Meshery are the physical and functional building blocks that enable the platform to operate and manage infrastructure. These include elements like the Meshery Server, Adapters, MeshSync, Broker, and Operator. Each architectural component has a distinct function, such as facilitating communication between cluster components (Broker), managing the lifecycle of deployed components (Operator), or synchronizing state information (MeshSync). Together, these components form a cohesive system that ensures Meshery can effectively integrate with various tools and environments, providing a robust and scalable infrastructure management solution.\nAdapters - Adapters extend Meshery’s management capabilities in any number of ways, including lifecycle, configuration, performance, governance, identity… Architecture - overview of different individual components of Meshery architecture and how they interact as a system. Broker - Meshery broker component facilitates data streaming between kubernetes cluster components and outside world. Catalog - Browsing and using cloud native patterns Database - Meshery offers support for internal caching with the help of file databases. This has been implemented with several libraries that supports different kinds of data formats. MeshSync - Meshery offers support for Kubernetes cluster and cloud state synchronization with the help of MeshSync. Operator - Meshery Operator controls and manages the lifecycle of components deployed inside a kubernetes cluster Logical Components 🔗 Logical components in Meshery refer to the conceptual elements that define and organize how infrastructure is managed and operated. These include components like Designs, Patterns, Policies, Environments, Models, and Workspaces. Each logical component has a specific role, such as defining the desired state of infrastructure (Designs), grouping resources (Environments), or enforcing governance rules (Policies). Logical components help structure the management process, making it more systematic and efficient by providing a clear framework for configuration, operation, and collaboration.\nComponents - Meshery Components identify and characterize infrastructure under management. Connections - Meshery Connections are managed and unmanaged resources that either through discovery or manual entry are managed by a state machine and used within one or more Environments. Credentials - Meshery uses one or more Credentials when authenticating to a managed or unmanaged Connection. Designs - Meshery Designs are descriptive, declarative characterizations of how your Kubernetes infrastructure should be configured. Environments - Environments are how you organize your deployment targets (whether on-premises servers or cloud services) into resource groups. Models - Meshery uses a set of resource models to define concrete boundaries to ensure extensible and sustainable management. Patterns - Meshery Patterns are descriptive, declarative characterizations of how your Kubernetes infrastructure should be configured. Policies - Meshery Policies enable you with a broad set of controls and governance of the behavior of systems under Meshery’s management. Registry - Meshery Registry is a database acting as the central repository for all capabilities known to Meshery. These capabilities encompass various entities, including models, components, relationships, and policies. Relationships - Meshery Relationships identify and facilitate genealogy between Components. Workspaces - Meshery Workspaces act as a central collaboration point for teams. ","categories":"","description":"This chapter delves into the fundamental concepts of Meshery, explaining its purpose, and the architectural and logical components that form its foundation.","excerpt":"This chapter delves into the fundamental concepts of Meshery, …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-meshery/introduction-to-meshery/meshery-concepts/","tags":"","title":"Meshery Concepts"},{"body":" IaaS+ 🔗 Infrastructure as a Service is the cloud service model we support with the Exoscale platform and a bit more, hence, IaaS+.\nPlatform 🔗 A state-of-the-art IaaS platform providing the building blocks for your application infrastructure.\nThe following questions arise if you think about the breadth of the cloud offerings in the market today:\nWhy IaaS? Who uses IaaS? Why Exoscale? Why IaaS? 🔗 It is the cloud computing model to extend or replace your data center.\nLet’s look at the trends of the worldwide IT infrastructure market.\nThe IDC figures show clearly that the infrastructure game is changing. Even though there are still markets where cloud adoption is low and slow, there is a clear trend that the public cloud will be a significant part of IT infrastructure.\nBy 2028:\nprivate cloud (cloud \u0026 dedicated) share will grow from 15.7% to 16.9% public cloud (cloud \u0026 shared) part will be up from 49.0% to 58.1% on-premises part (non-cloud \u0026 dedicated) will be down from 35.3% to 25.0% Who uses IaaS? 🔗 IaaS users, in general, are looking for TWO significant benefits:\nMORE DYNAMIC, servers can be started and stopped as needed without the up-front cost. REDUCED COMPLEXITY, of buying, repairing, or dealing with physical hardware. IaaS Exoscale Users\nWhich audiences apply IaaS functions to solve their IT tasks?\nWhy Exoscale? 🔗 Trusted by engineers across Europe and beyond - A reliable partner is key for mission-critical cloud workloads. Our customer success engineers have supported hundreds of European clients in migrating, running, and scaling cloud-native applications.\nData security, GDPR compliance \u0026 data privacy laws - We prioritize security. Exoscale ensures compliance with strict Swiss data privacy laws. Workloads started in Austria or Germany remain there, guaranteeing GDPR compliance.\nNot your typical Swiss cloud hosting company - Though primarily known as a Swiss cloud provider, we’ve expanded our data centers to Austria, Germany, and Bulgaria.\nMore than Swiss, Austrian, and German data centers - We’ve assisted numerous European customers in seamlessly migrating and scaling workloads across Austria, Germany, Switzerland, and beyond. With ISO27001, Cloud Security Alliance, and ISO27018 certifications, we simplify due diligence and focus on delivering top-tier infrastructure for your applications.\nExoscale for public and private cloud\nExoscale provides building blocks for deployment/delivery models with classic cloud computing instances (VMs virtual machines) or dedicated cloud servers in a VPC (Virtual Private Cloud) format, depending on your computing and compliance needs.\nOur Data Centers\nA data center near you in Europe featuring:\nMulti-homed locations Several peering connections​ Internal 400 Gbps backbone GDPR-compliant Exoscale Interfaces\nOur simple and intuitive interfaces make powerful concepts easy to use for teams of any size. Easily use anti-affinity groups and spawn virtual servers in different data centers to ensure high availability. Securely configure firewall rules across any number of instances using security groups. Manage team members and control access to your infrastructure with organization, keypairs, and multi-factor authentication.\nPowerful concepts. Simple interfaces. Built for teams.\nExoscale Web UI\nAll the platform components/features are automatable via various methods, such as the Exoscale CLI, Terraform plugins, the OpenAPI, and IAM-Key configurations. Support for creating and using custom templates is available to reuse more complex machine configurations regularly.\nExoscale CLI\n","categories":"","description":"","excerpt":" IaaS+ 🔗 Infrastructure as a Service is the cloud service model we …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/products/content/overview/","tags":"","title":"Overview"},{"body":"","categories":"","description":"This section provides an introduction to Exoscale pricing, including how to calculate costs for various products and services.","excerpt":"This section provides an introduction to Exoscale pricing, including …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/pricing/pricing/","tags":"","title":"Pricing"},{"body":" Overview 🔗 At our cloud platform, we understand that running operations can be a challenging task. That’s why we offer hands-on, direct contact with experienced engineers who power our cloud platform. In addition, we believe in building solid relationships with our customers and providing them with the support they need to succeed.\nWhether you’re a small business or a large enterprise, we care about your success. We are committed to helping you achieve your goals. Our team of engineers is always available to answer your questions, provide guidance, and assist you with any issues you may encounter. We take pride in the support we offer our customers and believe that direct contact with our engineers is essential to providing the best possible service. Our engineers are not only experts in cloud computing, but they also have a deep understanding of the challenges businesses face when running operations.\nWe are committed to building long-term relationships with our customers and helping them grow their businesses. We aim to provide a reliable, secure, and scalable cloud platform that meets the needs of companies of all sizes. With our hands-on, direct contact approach, you can be confident that you have a trusted partner invested in your success.\nFree support included 🔗 Free customer support is included. Just open and track tickets in the portal.\nAccess to our engineers 🔗 Our experts will handle your problems and help you to run and scale production workloads.\nChoose your plan 🔗 Pick the most suitable option for your requirements.\nSupport Plans - your success is our goal 🔗 Hands-on, direct contact with the engineers powering our cloud platform. We know running operations is challenging. We care about you no matter what size your company is.\nAvailable Support Plans 🔗 Built-In - included for all customers. Perfect for testers, developers, and non-critical applications. Starter - for startups and SMEs running production infrastructures. Everything from Built-In, the four-hour guaranteed response time, additional authentication options, and detailed usage reporting. Pro - the Pro plan will be available soon. One-hour response time, out-of-the-box services to detect and track events on the platform. Enterprise - the Enterprise plan will be available soon. Response time of 30 minutes (24/7), ticket, phone, or chat support, and your dedicated Customer Success Manager. ","categories":"","description":"","excerpt":" Overview 🔗 At our cloud platform, we understand that running …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/response-support/content/response-support/","tags":"","title":"Response \u0026 Support"},{"body":" Responsibility and Expertise 🔗 Is running a data center your core business? If the answer to this question is yes, you will have a different opinion and see managed databases in a different light. But the majority here will probably answer with no, and then a common view will open regarding the entry question.\nIt is like with utilities. Do you run a wind farm or a hydroelectric power plant to produce your needed electricity? Or do you rely on a managed service from a utility provider? Information Technology (IT) services have become a utility for many areas anyway. So, specialists are defining and often dominating the market. Use those specialists to become a better specialist in your field of expertise.\n","categories":"","description":"","excerpt":" Responsibility and Expertise 🔗 Is running a data center your core …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/managed-databases/content/responsibility/","tags":"","title":"Responsibility and Expertise"},{"body":" Scalable, On-demand Kubernetes Cluster 🔗 Start with a privacy-minded public cloud to host from single applications to complex architectures. Deploy a production-ready cluster in 90 seconds and manage it with a simple web portal, CLI, API or your choice of tools (Terraform).\nSKS Features 🔗 Access to all compute instance types\nIt is your cluster on your terms. Size the Node-Pools as needed, using all instance types available. Attach one Node-Pool with Memory-Optimized instances and another one with CPU-Optimized ones.\nFull cluster lifecycle management\nKubernetes is a fast-changing platform, and versions roll out fast. SKS has built-in commands to upgrade your control plane seamlessly – minimizing downtimes and errors.\nIntegration with NLB and Instance Pools\nManage ingress traffic with Exoscale Network Load Balancer support directly integrated into SKS management. Scale pools from Kubernetes using Instance Pools.\nHigh-Availability control plane With the PRO SKS plan, benefit from a resilient HA Kubernetes control plane, so your workloads are always up.\nSimple and transparent pricing\nWith a free STARTER version and an affordable PRO plan, SKS provides straightforward pricing. Nodes are charged their usual rate with no extra cluster management fee.\nAll zones\nDeploy your cluster where you need to, in the region that suits your latency, privacy or redundancy strategy.\n","categories":"","description":"","excerpt":" Scalable, On-demand Kubernetes Cluster 🔗 Start with a privacy-minded …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-exoscale-sks/content/scalable-kubernetes-service/","tags":"","title":"Scalable Kubernetes Service"},{"body":"","categories":"","description":"Welcome to the SKS Advanced Learning Path","excerpt":"Welcome to the SKS Advanced Learning Path","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/sks-welcome/","tags":"","title":"SKS Welcome"},{"body":" Welcome 🔗 In this SKS Advanced training, we shift gears and dig into the more advanced Kubernetes (K8s) areas: storage, routing, and debugging. Again, we will take a practical approach to these advanced K8s topics and show you more of the technology’s magic.\nWelcome 🔗 Video: Welcome Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Welcome 🔗 In this SKS Advanced training, we shift gears and dig into …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/sks-welcome/content/sks-welcome/","tags":"","title":"SKS Welcome"},{"body":"","categories":"","description":"Welcome to the SKS Starter Learning Path","excerpt":"Welcome to the SKS Starter Learning Path","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/sks-welcome/","tags":"","title":"SKS Welcome"},{"body":" Welcome 🔗 In this SKS Starter training, we cover how Kubernetes (K8s) works, why we want to use K8s, and, very importantly, we will take a practical approach to K8s to show you the real magic of the technology.\nWelcome 🔗 Video: Welcome Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Welcome 🔗 In this SKS Starter training, we cover how Kubernetes (K8s) …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/sks-welcome/content/sks-welcome/","tags":"","title":"SKS Welcome"},{"body":" Overview 🔗 At Exoscale, we understand that sustainability is a crucial concern for our customers and society.\nAs a public cloud provider, we are responsible for operating in an environmentally responsible manner, reducing our carbon footprint, and minimizing our environmental impact.\nFocus Areas 🔗 Renewable Energy Extended Server Fleet Life High-Density Rack Design Energy-Efficient Design Direct Shipping Data Center Exoscale’s Believes 🔗 Remote-First Approach Waste Heat Utilization Personal Commitment Leading Sustainability Exoscale’s Goals 🔗 Multi-Criteria Life Cycle Assessment by 2023 Certified Energy Management by 2024 Impact Calculator for Our Clients by 2024 100% Renewable Energy by 2025 ","categories":"","description":"","excerpt":" Overview 🔗 At Exoscale, we understand that sustainability is a …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/sustainable-cloud/content/cloud/","tags":"","title":"Sustainable Cloud"},{"body":" Overview 🔗 Secure Protect Control Secure Dev Bug Bounty Program Regular Pentest Secure - secured by default 🔗 Secure Code Data Center Redundancy Network Redundancy Power-Supply Redundancy Protect - protected by default 🔗 Instances and Security Groups are closed by default Databases fully encrypted and filtered by default Secured Access Control Lists on all storage buckets Included DDOS Protection of the network Control - fine-grained access control 🔗 Role-Based Access Control to Exoscale organizations Two-Factor Authentication supported Optional SSO configuration Full IAM for all API calls Audit Trail for all events Secure Dev - world-class engineering 🔗 Code Review Change Management Secure Development Practices Bug Bounty Program - crowdfunded security 🔗 Competitive Bounties 300 active Researchers taking part in the program Bugcrowd a crowdsourced security platform. It is one of the internet’s largest bug bounty and vulnerability disclosure companies. Regular Pentest - timeboxed assessments 🔗 Annual Pentest Performed by Third Parties Full Product Coverage ","categories":"","description":"","excerpt":" Overview 🔗 Secure Protect Control Secure Dev Bug Bounty Program …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/technical-security/content/security/","tags":"","title":"Technical Security"},{"body":" Troubleshooting 🔗 In this section, we cover various techniques to stay ahead of the game running workloads with Kubernetes. We keep in mind the cycle of Development, Operations, Monitoring, DevOps, and Debugging in agile environments powered by modern software solutions. We are looking at necessary tasks and how to get things done with graphical and CLI tools.\nk8slens.dev kubectl ","categories":"","description":"","excerpt":" Troubleshooting 🔗 In this section, we cover various techniques to …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/troubleshooting/content/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" Typical Web Application 🔗 Our example architecture consists of the following explained components, and it demonstrates the practical usage of several products together and the associated costs.\nApplication Servers 🔗 run the web application. The application reads from the DB servers via the Elastic IP v2, and users access this web service via another Elastic IP v2 that distributes traffic evenly among them. Upload user files to the Public File Bucket. Installed in an Anti-Affinity group.\nDatabase Server 🔗 operate a shared database (MySQL, MongoDB, etc.) that is capable of replicating data. Installed in an anti-affinity group to ensure that the individual components are never on the same physical host.\nBackup Server 🔗 responsible for reading the data and uploading it to the Backup Bucket object storage.\nPublic File Bucket 🔗 stores and publishes user files, such as profile pictures, and makes them publicly available.\nBackup Bucket 🔗 holds the backups of the DB servers and the Public File Bucket.\nElastic IP 🔗 in v2 is used as a simple load balancer in this scenario that distributes traffic evenly.\nExoscale DNS 🔗 responsible for resolving the service domain name (example.com).\nVideo: Containers Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Typical Web Application 🔗 Our example architecture consists of the …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/pricing/pricing/content/typical-web-application/","tags":"","title":"Typical Web Application"},{"body":" Introduction 🔗 In this tutorial, you will explore how Dapr(Distributed Application Runtime) operates within a Kubernetes cluster.\nUsing Meshery you will deploy the Dapr Helm chart along with sample applications: a Python application that generates messages and a Node.js application that consumes and stores those messages in a Redis state store, managed through Dapr’s state store component model.\nMeshery’s Kanvas visualization capabilities allow you to examine the components involved in this architecture and understand their relationships. This breakdown helps you see how Dapr interacts with the applications, enhancing your comprehension of its functionality.\nThis tutorial expands on the Hello Kubernetes tutorial with Dapr, providing a visual approach to learning Dapr in a Kubernetes environment.\nDapr Architecture 🔗 In a Dapr-enabled application, the sidecar architecture is central to how Dapr provides its functionalities. Each application pod in your Kubernetes cluster gets a Dapr sidecar injected, which acts as a proxy and mediator for service-to-service calls, state management, and other capabilities. This architecture allows Dapr to offer a set of APIs that simplify common microservice tasks such as service invocation, state management, and pub/sub messaging.\nWhen one service needs to call another, the request is made to the local Dapr sidecar, which routes the request to the appropriate sidecar in the target service’s pod. The target sidecar then forwards the request to its application.\nSimilarly, when your application makes a state management request (e.g., saving or retrieving state), it communicates with its local Dapr sidecar. The sidecar then uses the configuration retrieved from the Dapr control plane to interact with the configured state store.\nThis architecture ensures that your application remains loosely coupled and can leverage Dapr’s capabilities without being tightly integrated with the infrastructure details.\nThe diagram below illustrates this setup, providing a visual representation of how Dapr components interact within the architecture. We will explore more on these concepts in subsequent chapters.\nPrerequisite\nAccess to Meshery (Self-Hosted or Meshery Playground). Kubernetes Cluster connected to Meshery. Available Clusters If you are using a self-hosted Meshery deployment, connect to your Kubernetes cluster using this Guide. Alternatively, Meshery Playground users can use the live pre-registered Kubernetes connection. This tutorial uses a self-hosted Meshery deployment with a connected Minikube cluster. Learning Objectives 🔗 Gain hands-on experience in deploying Dapr on a Kubernetes cluster using Meshery. Learn how to visualize these components using Meshery and get a better understanding of the Dapr architecture and interactions. Understand how to use Dapr’s state management capabilities to persist data in a Redis state store, and see how the Node.js app interacts with the state store to save and retrieve data. ","categories":"","description":"Learn how Dapr works by deploying Dapr and sample applications in a Kubernetes Cluster using Meshery.","excerpt":"Learn how Dapr works by deploying Dapr and sample applications in a …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/explore-dapr-with-meshery/dapr/introduction/","tags":"","title":"Understanding How Dapr Works in a Kubernetes Cluster: A Visual Guide with Meshery"},{"body":" Introduction 🔗 This guide walks through how to effectively organize your Meshery workspace using a real-world scenario. Meet Alero, an Engineering Manager at Mango company, and two DevOps engineers, Musa and Hargun. Alero’s responsibilities include assigning workspaces, teams, and environments, as well as managing access control for Musa and Hargun. Each engineer leads their own team with distinct responsibilities.\nThe goal here is to show you how Alero can structure their workspaces to streamline collaboration, maintain robust access control, and optimize workflow deployment across these teams. By following along, you’ll learn how to set up a similar structure for your own organization.\nSetting Up Your Organization 🔗 Understanding Organizations 🔗 In Layer5 Cloud, an Organization can house multiple teams, and these teams are where you’ll group users. It’s through these teams that you’ll manage access to specific resources like Workspaces and their contents. Think of the Organization as the top-level boundary that makes this structured collaboration and resource management possible. For our scenario, Alero will start by creating an Organization for her engineering team, laying the groundwork for their work in Meshery.\nHow to Create an Organization 🔗 To create your organization, navigate to Layer5 Cloud Organizations on the Layer5 Cloud dashboard. Click on Add Organizations and Fill in the details for your Organization. Editing Your Organization’s Details 🔗 Once your organization is created, you might need to make changes or retrieve its invitation link.\nTo edit details, find your organization in the list and click on the pencil icon under the “Actions” column. A modal window will appear, allowing you to modify various settings for your organization. Inviting Users with a Link: Inside this modal, under “Invitations,” you’ll find an option to Copy your organization link. Any user who signs up using this unique link will automatically be added to your organization, simplifying the onboarding process. After making any necessary changes to the organization’s details, click on Update Organization to save them. Adding Users to Your Organization 🔗 With your organization set up, the next step is to bring your team members on board. Alero will now add Musa and Hargun.\nMethods for Adding Users 🔗 You have two ways to add users:\nUsing the Invite Link: As covered in the “Editing Your Organization’s Details” section, you can share a direct invitation link.Learn more about Invite Link Manual Addition via Layer5 Cloud Users: This method is useful for adding existing Layer5 Cloud users or creating new accounts directly. How to Add Users Manually 🔗 Navigate to Layer5 Cloud Users\nIn the Users section, you have several options for adding users to your chosen organization:\nCreate New User: This is to create a new user account and email new user with account setup instructions. Optionally, add the new user to your organization with or without assigned roles. Add Existing User: This is to add an existing user to your organization. Invite New User: This option sends an invitation to an email address for someone who does not yet have a Layer5 Cloud account. To add existing users (like Musa and Hargun):\nClick on Add Existing User. In the modal that appears, select the Organization name you want to add them to (e.g., Mongo). Search for the users by name that you wish to add. Once selected, click Update Organization to add them. To learn more about creating Users, See Layer5 Users Docs.\nAssigning Roles to Users 🔗 Users added to an organization receive a set of default permissions. However, to grant them specific capabilities, such as managing teams, you’ll need to assign them appropriate roles. Alero wants Musa and Hargun to be able to administer their respective teams, so she will assign them the “Team Admin” role.\nHow to Assign Roles 🔗 Click the pencil icon . In the “Edit User Profile” modal, locate the Organization Roles dropdown menu. Select the desired role from the list (e.g., “team admin”). Click Save Changes. After you have completed assigning roles, your organization’s user overview might look similar to the following:\nHow to Create Teams 🔗 Alero will create two teams, one for Musa’s responsibilities and one for Hargun’s, and add each of them as members to their respective teams.\nNavigate to Layer5 Cloud Teams. Click on Add Team. Enter the details of your new team and select new team members from your organization users. After creating the teams, they will appear in the Teams list. You can click on a team to see its members.\nTo learn more about Teams Roles, See Teams Roles Docs.\nCreating a Workspace 🔗 With your users organized into teams and assigned appropriate roles, you’re ready to set up Workspaces. Workspaces are where the actual work happens – they group designs, environments, and connections for specific projects or purposes.\nHow to Create a Workspace 🔗 Navigate to Workspaces Click on Create. Enter the detail Your new workspace will be created and will typically appear as an empty container, ready for you to assign environments, teams and resources.\nAssigning a Team to a Workspace 🔗 To allow team members to collaborate within a workspace, you need to assign their team(s) to it. Alero will assign the “Engineers Team” to the “Developer Workspace”.\nHow to Assign a Team 🔗 Select the workspace you want to configure,click the “Teams”. You’ll typically see two columns: “Available Teams” and “Assigned Teams”. From the “Available Teams” list on the left, select the team you wish to assign (e.g., “Engineers”). Click the \u003e button to move the selected team to the “Assigned Teams” list on the right. Click Save to confirm the assignment. Now, members of the assigned team(s) will have access to this workspace according to their roles and permissions. This enables them to view and collaborate on any designs and views that are part of this workspace, facilitating teamwork.\nTo learn more about Workspace, See Workspace Docs.\nManaging Integrations for Teams 🔗 Team Members will need certain integrations/Connections to carry out their tasks.\nHow to Configure Connections for Teams 🔗 Navigate to Integrations in Layer5 Cloud. Next, configure the connections that a particular team will require to work. For more detailed instructions, consult dedicated guides for specific integrations (e.g., Integrating GitHub)\n","categories":"","description":"Learn how to effectively organize and utilize Workspaces in Meshery by setting up Organizations, Teams, and managing access for streamlined collaboration and deployment.","excerpt":"Learn how to effectively organize and utilize Workspaces in Meshery by …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-meshery/introduction-to-meshery/using-workspaces-effectively/","tags":"","title":"Using Workspaces Effectively"},{"body":" Definition 🔗 Compliance is adhering to rules, such as laws, standards, codes, and policies. Governments usually establish these rules to ensure that all organizations legally conduct their business activities.\nCompanies can also set rules and standards to ensure employees perform their activities according to specific parameters and guidelines.\nGovernments and organizations can enforce rules through regulatory bodies, agencies, or departments that monitor each business activity and assess how employers and employees follow the set rules.\nExample 🔗 For instance, a government can use a health and safety program to oversee and regulate construction sites within a province or territory. This program can check the construction company’s safety provisions in its work setting and ensure those precautions align with the provincial regulations. In addition, the company can create a department or a position, such as a safety officer or manager, to oversee how its employees use the machinery and equipment in their workplace. They can also create a safety committee and ensure all employees can access training.\n","categories":"","description":"","excerpt":" Definition 🔗 Compliance is adhering to rules, such as laws, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/what-compliance/content/definition/","tags":"","title":"What is Compliance?"},{"body":" What is it? 🔗 Google open-sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google’s experience running production workloads at scale with best-of-breed ideas and practices from the community.\nThe name Kubernetes originates from Greek, meaning helmsman or pilot.\nKubernetes is a portable, extensible, open-source platform for managing containerized workloads and services that facilitates both automation and declarative configuration.\n","categories":"","description":"","excerpt":" What is it? 🔗 Google open-sourced the Kubernetes project in 2014. …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/kubernetes/content/what-it-is/","tags":"","title":"What is it?"},{"body":"","categories":"","description":"Explains the advantages of cloud storage, including scalability, accessibility, durability, and cost efficiency.","excerpt":"Explains the advantages of cloud storage, including scalability, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/why-cloud-storage/","tags":"","title":"Why Cloud Storage ?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/why-compliance/","tags":"","title":"Why Compliance?"},{"body":" Is essential for business. 🔗 Companies that comply with regulations are usually more competitive than those that don’t, as they can offer their products through any marketplace without any legal restriction.- Customers have more trust in those companies because they know they’re following quality standards and using safe materials for consumption.- Learning more about compliance and why it is essential can help implement workplace programs that follow established policies, codes, and standards. ","categories":"","description":"","excerpt":" Is essential for business. 🔗 Companies that comply with regulations …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/why-compliance/content/why/","tags":"","title":"Why Compliance?"},{"body":"","categories":"","description":"This section introduces the concept of containers, their benefits, and why they are essential in modern application development and deployment.","excerpt":"This section introduces the concept of containers, their benefits, and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-containers/","tags":"","title":"Why Containers?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/databases/","tags":"","title":"Why Databases?"},{"body":"","categories":"","description":"Kubernetes is a powerful platform for managing containerized applications at scale. This section explores the reasons why Kubernetes has become the de facto standard for container orchestration, its benefits, and how it can help streamline application deployment and management.","excerpt":"Kubernetes is a powerful platform for managing containerized …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-kubernetes/","tags":"","title":"Why Kubernetes?"},{"body":"","categories":"","description":" Introduces the importance of storage in computing, enabling persistent data retention, accessibility, and performance for applications.","excerpt":" Introduces the importance of storage in computing, enabling …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/why-storage/","tags":"","title":"Why Storage ?"},{"body":" Why Kubernetes? 🔗 Running services in containers tend to produce numerous containers pretty quick. Handling many containers with no additional aid is a very cumbersome job. Hence, an orchestration solution is necessary to run services in containers efficiently.\nWithout Container Orchestration 🔗 Think about scaling up services; it increases manual work. Think about fixing crashing nodes; it increases manual work. Think about the complexity of running new stuff in production; it increases. Think about the human cost of running those services; it increases. Scaling services/applications becomes more and more difficult. Public cloud provider resource usage becomes more and more expensive. ","categories":"","description":"","excerpt":" Why Kubernetes? 🔗 Running services in containers tend to produce …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-kubernetes/content/without-container/","tags":"","title":"Without Container Orchestration"},{"body":"Chapters available\n","categories":"","description":"A course which teaches advanced concepts of service meshes.","excerpt":"A course which teaches advanced concepts of service meshes.","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/advance-concepts-of-service-meshes/","tags":"","title":"Advance Concepts of Service Meshes"},{"body":" Why is compliance essential for businesses? 🔗 Here are five reasons why compliance is imperative for any business:\nEnhancing customer trust Increasing business revenue Protecting from legal action Improving internal processes Avoiding fines and penalties Enhancing customer trust 🔗 When customers know a company complies with the law and quality standards, they’re more likely to trust the brand and purchase its products and services. This is possible because they understand that the law protects their interests and health, and the standards set by the company consider their preferences and expectations. Enhancing customer trust is one of the most important goals of an organization, as this can improve brand awareness and avoid complaints and product returns. This can help a business increase its revenue, improve its reputation, avoid adverse publicity, and expand its customer base.\nIncreasing business revenue 🔗 When a company demonstrates compliance, other businesses might purchase a high volume of its products, allowing it to increase its business-to-business operations. This is possible because most companies want to buy regulated products and avoid the risks associated with non-regulated items. Customers also tend to buy more products and services from companies that follow set rules because they understand they are not offering illegal or dangerous products. In addition, they can verify the quality standards the company is using and how it’s meeting them.\nProtecting the company from legal action 🔗 Lawsuits are usually filed against a company if unsatisfied customers or the public discover the business is avoiding regulations to save money in operations costs or generate more profit. By complying with regulations, companies can protect themselves from legal action and demonstrate that they’ve been operating according to the law. Inspections from regulatory agencies are necessary, as they can corroborate that the company is following all legal requirements. It’s also imperative that the company ensures its employees have the proper certifications and receive adequate training.\nImproving internal processes 🔗 Compliance officers can improve the company’s internal processes by ensuring employees understand and follow the policies. They create training programs or ask suppliers and vendors to provide each employee with the training required to operate specific equipment. Companies usually establish standards, codes, and procedures to benefit their operations, guarantee quality levels, and ensure the safety of their employees. This way, the company provides that the processes involved in its daily operations follow a procedure and that emergency protocols are in place to prevent accidents and non-programmed shutdowns.\nAvoiding fines and penalties 🔗 When government agencies find a company violating a law, they can impose fines and penalties. Businesses can avoid these penalties and access government programs or legally granted benefits by complying with regulations. They can also evade legal costs or any other punishment that can stop their operations, affecting their marketing campaigns, sales, employees’ morale, customer service, and investment in new equipment.\n","categories":"","description":"","excerpt":" Why is compliance essential for businesses? 🔗 Here are five reasons …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/why-compliance/content/bussiness/","tags":"","title":"Business Aspects of Compliance"},{"body":" Calculate Product Pricing 🔗 Usually, you want to know the cost for a resource on a monthly basis, like you know your cost for other subscriptions like your mobile data plan, Spotify, Netflix and so forth.\nThe official pricing can be found on the web exoscle.com/pricing and in the official price list. There you can find hourly pricing for the different products. In the Exoscale realm, we calculate with 720 hours per month, and other cloud providers use, e.g. 730 hours per month, this information is relevant if you want to compare monthly pricing.\nApplication Server Instances Calculation 🔗 2 x 720 x (100 x 0.00014 + 0.04666) = €87.35/month\n2x Medium (€0.04666/h) 100 GB disk (€0.00014/h/GB) 720 hours per month Database Server Instances Calculation 🔗 3 x 720 x (400 x 0.00014 + 0.04666) = €221.75/month\n3x Medium (€0.04666/h) 400 GB disk (€0.00014/h/GB) 720 hours per month Backup Server Instance Calculation 🔗 1 x 720 x (50 x 0.00014 + 0.01458) = €15.54/month\n1x Tiny (€0.01458/h) 50 GB disk (€0.00014/h/GB) 720 hours per month Elastic IP Calculation 🔗 2 x 720 x 0.01389 = €20.00/month 2x Elastic IP v2 (€0.01389/h) 720 hours per month Exoscale DNS Calculation 🔗 1x SMALL = €1/month\n1x SMALL monthly subscription With DNS you enrol to a monthly recurring subscription, automatically renewed. Every package entitles you to register up to the indicated number of zones.\nPlan Zones Cost (€/month) SMALL 1 Zone €1.00 MEDIUM 10 Zones €5.00 LARGE 50 Zones €25.00 ","categories":"","description":"","excerpt":" Calculate Product Pricing 🔗 Usually, you want to know the cost for a …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/pricing/pricing/content/calculate-product-pricing/","tags":"","title":"Calculate Product Pricing"},{"body":" Details 🔗 Cloud Sustainability Transparency 🔗 CloudAssess, an open-source tool, evaluates the carbon footprint of cloud services in line with EU Product Environmental Footprint and ADEME specifications. It offers users a detailed environmental balance sheet, encouraging responsible cloud usage and compliance with CSRD-2025. This fosters informed, sustainable decisions, supporting users in navigating the complexities of environmental responsibility in the cloud computing sphere.\nCloud Environmental Transparency 🔗 Exoscale’s CloudAssess initiative enhances environmental transparency, fostering trust and enabling informed sustainability decisions in cloud services. Utilizing Life Cycle Analysis and reliable standards, it provides a solid framework for understanding cloud usage impacts. This aligns customer actions with sustainability goals, establishing Exoscale as a frontrunner in corporate sustainability by highlighting the value of detailed environmental data.\nEnvironmental Assessment Backbone 🔗 CloudAssess uses Life Cycle Analysis (LCA) to evaluate the environmental impact of cloud services across their life cycle, including raw material extraction, production, and disposal. It assesses ecological criteria like emissions, water use, and resource consumption, in compliance with ISO:14040 and ISO:14044 standards. This ensures reliable, standardized assessments, enhancing the tool’s credibility by providing accurate data on cloud services’ environmental footprint.\nInformed Decision Making 🔗 CloudAssess transforms environmental impact assessment by calculating the carbon footprint of cloud services hourly, daily, or monthly. Utilizing an API for seamless data integration into platforms like invoices and dashboards, it enables Exoscale customers to monitor their cloud usage’s environmental effects accurately. Starting with monthly assessments, the goal is to advance to real-time tracking, empowering users to make informed decisions to reduce their ecological footprint.\n","categories":"","description":"","excerpt":" Details 🔗 Cloud Sustainability Transparency 🔗 CloudAssess, an …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd-and-exoscale/content/cloud-assess/","tags":"","title":"CloudAssess"},{"body":" Details 🔗 Exoscale offers a range of products related to cloud servers, also known as virtual machines or Compute Instances. These products allow users to easily manage virtual machines, create instance pools, and ensure fault tolerance through anti-affinity groups. SKS and Block Storage options provide scalability, increased computational power, and storage flexibility. IAM and organization management tools allow for secure access control and user management. With various networking products available, users can configure workloads to meet their specific requirements, including secure private networking, firewall management, Network Load Balancer, Elastic IP addresses, and IPv6 support.\nExoscale’s cloud infrastructure is simple, fast-performing, and can scale with businesses’ needs. Overall, Exoscale offers a comprehensive and flexible cloud computing platform suitable for organizations of all sizes and industries.\nNOTE! Here, you can find all the details in the online documentation for COMPUTE.\nInstances 🔗 For better requirement matching, various instance types are available to use:\nStandard: Provide a balanced mix of CPU cores, RAM, and SSD local storage to cover a variety of use cases and allow you to implement your architecture. CPU Optimized: These are optimized for CPU-intensive applications, offering a higher CPU-to-memory ratio. They offer a more significant computational advantage for workloads like batch processing, media decoding and encoding, network appliances, or high-performance web servers. Memory Optimized: These are the best performance-to-cost ratio for memory-intensive workloads and are ideal for RAM-intensive applications. They double the memory per core with a price reduction of up to almost 25 % compared to Standard Instances. Storage Optimized: These are the same mix of CPU and RAM as our Standard Instances but use larger drives, greatly expanding the overall data capacity. Consequently, they lower the cost per GB by more than 60 %. GPU1: Provides up to 4 dedicated NVIDIA Tesla P100 graphic cards to perform deep learning, high-performance computing, or other types of intensive computation. Save up to 75% compared to the competition, and no long-term commitment. GPU2: Based on Tesla V100, offers nearly double single-precision and double-precision teraflops compared to GPU1, as well as 640 dedicated Tensor Cores to train AI models that would consume weeks of computing resources in a few days. GPU3: Is the all-rounder for AR, VR, Simulations, Rendering, AI, and more. Combining the latest Ampere RT Cores, Tensor Cores, and CUDA Cores with 48 GB of graphics memory allows the A40 to deliver a unique set for visual computing workloads. Instance Pools 🔗 Exoscale Instance Pools are a service to automatically provision groups of identical Compute instances. You can define several instances in the pool, and the service will keep the required number up and running for you to achieve.\nHigh Availability: Using an Instance Pool ensures that the target quantity of instances is running. Elasticity: Instance Pools can be scaled up and down dynamically. Hence, the number of instances matches the actual load for better cost efficiency. NOTE! Here, you can find more details on Instance Pools. SKS (Scalable Kubernetes Service) 🔗 Exoscale’s SKS is a managed Kubernetes offering, which consists of:\nManaged Kubernetes control planes Dynamic Nodepool attachment Control Plane access management facilities Full API support Exoscale’s Scalable Kubernetes Service (SKS) provides a powerful and efficient way to deploy and manage your applications quickly. With this fully managed K8s service, you can quickly scale up and down your worker nodes and have complete control over the entire life cycle of your cluster. Exoscale provides various integration options, including CLI, API, portal, Terraform support, and deep NLB integration.\nNOTE! Here, you can find more details on SKS.\nBlock Storage 🔗 Exoscale’s Block Storage offers a robust and distributed block device solution for Exoscale Compute instances, known for its redundancy and reliability. A Volume, a singular storage unit, can be partitioned and formatted to accommodate directories and files. One of the critical features of Block Storage is the Snapshot, which captures the state of a volume at a specific moment, allowing users to create new volumes based on that state.\nNOTE! Here, you can find more details on Block Storage.\nTemplates 🔗 Exoscale provides various Compute instance templates from which to choose. However, you can customize templates to suit your needs further. In addition to using a Cloud-Init configuration via an instance’s user data or a configuration management tool such as Puppet, Ansible, or Terraform, you can also create customized templates. You can use custom templates to launch a custom operating system or custom template configuration on Exoscale, which allows you to deploy ready-to-go instances with minimal startup configuration.\nNOTE! Here, you can find more details on Custom Templates.\nSecurity Groups 🔗 Exoscale Security Groups provide a modular way to define and compose firewall rules. The rules are managed at the hypervisor level to restrict incoming and outgoing network traffic.\nNOTE! Here, you can find more details on Security Groups.\nElastic IP 🔗 All Exoscale instances include a native IPv4 address leased from a global pool. This address is strongly coupled to the Compute instance itself. When you destroy the instance, you release the IP address to the global pool without guarantee that you will ever get the same IP address again. However, there are various cases where you may want an IP address to persist. By creating an Elastic IP, you can have a specific IP address for your organization. You can then attach it to one or several instances besides their native IP address.\nThe simplest use case for this feature is to use an Elastic IP as a persistent IP address you can move between instances. This allows you to circumvent the IP address change when destroying an instance. You can always switch the underlying instance and point traffic to the same address with an Elastic IP.\nNOTE! Here, you can find more details on Elastic IPs.\nLoad Balancers 🔗 A Network Load Balancer (or NLB) is a Layer 4 (TCP/UDP) load balancer that distributes incoming traffic to Compute instances managed by an Instance Pool. An NLB comprises several services, each bound to an Instance Pool in the same zone as the NLB. Services will efficiently forward connections reaching the NLB’s IP address to the member instances of the Instance Pool.\nWhile the instances remain individually accessible through their public IP, the NLB will expose a single IP address for all services and distribute the incoming traffic across the members of the Instance Pool following the service’s rules. NLB services will update automatically when the Instance Pool scales up or down, distributing traffic across all reachable member instances of the pool and excluding unreachable ones using an integrated health check functionality.\nNLB acts only on incoming traffic, so all return traffic from the backend to the client that originated the request goes out directly from the pool member instance.\nNOTE! Here, you can find more details on Network Load Balancer.\nPrivate Networks 🔗 The Private Network is a classic layer 2 segment: it is as if your instances were attached to a dedicated switch. This means:\nYou can use any ethernet-compatible protocol (IPv4, IPv6, NetBIOS). Security group rules do not apply to traffic inside private networks. Multicast and broadcast are authorized. Only your instances are attached to the segment. No encryption is performed, but your packets do not leave our data center. Private Networks can be managed. Private Networks do not span across several zones. Each instance may provision one or more additional unmanaged and managed network interfaces. This interface is bound to a private network segment shared only with your other instances. NOTE! Here, you can find more details on Private Networks.\nSSH Keypairs 🔗 SSH keypairs can authenticate to your Compute instances running Linux without a password, leveraging SSH Public-Key authentication’s added security. Public-key authentication is both:\nSecure: Breaking an SSH key requires so much time and computational power that these attacks are impractical in the real world. SSH keys are much more secure than even very strong passwords. Convenient: Instead of managing per-instance passwords or sharing them across your organization, every person who needs access to your servers gives you their public key. You can then set up granular access control by adding those keys only to the relevant instances. Suppose you need to revoke someone’s access. In that case, simply revoking their key prevents them from logging in without affecting other people’s workflow. NOTE! Here, you can find more details on SSH Keypairs.\nAnti-Affinity 🔗 Anti-Affinity groups let you specify which instances should run on separate hosts. For example, in an HA (high availability) cluster, you could keep your instances on distinct hypervisors to ensure more reliable fault tolerance.\nNOTE! Here, you can find more details on Anti-Affinity Groups.\n","categories":"","description":"","excerpt":" Details 🔗 Exoscale offers a range of products related to cloud …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/products/content/compute/","tags":"","title":"Compute"},{"body":"","categories":"","description":"This section covers the concepts of the SKS Advanced Learning Path, including storage, routing, and debugging in Kubernetes.","excerpt":"This section covers the concepts of the SKS Advanced Learning Path, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/","tags":"","title":"Concepts"},{"body":" Summary 🔗 The CSRD marks a shift towards a sustainable digital economy, urging cloud providers to align growth with environmental goals. It enhances reporting, compliance, and sustainability, offering a chance to innovate with green solutions. This move towards detailed ESG disclosures presents both challenges and opportunities, allowing providers to stand out by meeting eco-conscious demands. As crucial players in global business, cloud services are key to a greener digital future.\n","categories":"","description":"","excerpt":" Summary 🔗 The CSRD marks a shift towards a sustainable digital …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd-and-providers/content/summary/","tags":"","title":"Conclusion"},{"body":"","categories":"","description":"This section provides an introduction to containers, their architecture, and how they are used in modern software development.","excerpt":"This section provides an introduction to containers, their …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/containers/","tags":"","title":"Containers"},{"body":"This section guides you in creating of a 3-nodes Kubernetes cluster using kubeadm bootstrapping tool. This is an important step as you will use this cluster throughout this workshop.\nThe cluster you’ll create is composed of 3 Nodes named controlplane, worker1 and worker2. The controlplane Node runs the cluster components (API Server, Controller Manager, Scheduler, etcd), while worker1 and worker2 are the worker Nodes in charge of running the containerized workloads.\nProvisioning VMs 🔗 Before creating a cluster, it’s necessary to provision the infrastructure (bare metal servers or virtual machines). You can create the 3 VMs on your local machine or a cloud provider (but this last option will come with a small cost). Ensure you name those VMs controlplane, worker1, and worker2 to keep consistency alongside the workshop. Please also ensure each VM has at least 2 vCPUs and 2G of RAM so it meets the prerequisites.\nIf you want to create those VMs on your local machine, we recommend using Multipass, a tool from Canonical. Multipass makes creating local VMs a breeze. Once you have installed Multipass, create the VMs as follows.\nmultipass launch --name controlplane --memory 2G --cpus 2 --disk 10G multipass launch --name worker1 --memory 2G --cpus 2 --disk 10G multipass launch --name worker2 --memory 2G --cpus 2 --disk 10G Cluster initialization 🔗 Now that the VMs are created, you need to install some dependencies on each on them (a couple of packages including kubectl, containerd and kubeadm). To simplify this process we provide some scripts that will do this job for you.\nFirst, ssh on the controlplane VM and install those dependencies using the following command.\ncurl https://luc.run/kubeadm/controlplane.sh | VERSION=\"1.32\" sh Next, still from the controlplane VM, initialize the cluster.\nsudo kubeadm init The initialization should take a few tens of seconds. The list below shows all the steps it takes.\npreflight Run pre-flight checks certs Certificate generation /ca Generate the self-signed Kubernetes CA to provision identities for other Kubernetes components /apiserver Generate the certificate for serving the Kubernetes API /apiserver-kubelet-client Generate the certificate for the API server to connect to kubelet /front-proxy-ca Generate the self-signed CA to provision identities for front proxy /front-proxy-client Generate the certificate for the front proxy client /etcd-ca Generate the self-signed CA to provision identities for etcd /etcd-server Generate the certificate for serving etcd /etcd-peer Generate the certificate for etcd nodes to communicate with each other /etcd-healthcheck-client Generate the certificate for liveness probes to healthcheck etcd /apiserver-etcd-client Generate the certificate the apiserver uses to access etcd /sa Generate a private key for signing service account tokens along with its public key kubeconfig Generate all kubeconfig files necessary to establish the control plane and the admin kubeconfig file /admin Generate a kubeconfig file for the admin to use and for kubeadm itself /super-admin Generate a kubeconfig file for the super-admin /kubelet Generate a kubeconfig file for the kubelet to use *only* for cluster bootstrapping purposes /controller-manager Generate a kubeconfig file for the controller manager to use /scheduler Generate a kubeconfig file for the scheduler to use etcd Generate static Pod manifest file for local etcd /local Generate the static Pod manifest file for a local, single-node local etcd instance control-plane Generate all static Pod manifest files necessary to establish the control plane /apiserver Generates the kube-apiserver static Pod manifest /controller-manager Generates the kube-controller-manager static Pod manifest /scheduler Generates the kube-scheduler static Pod manifest kubelet-start Write kubelet settings and (re)start the kubelet upload-config Upload the kubeadm and kubelet configuration to a ConfigMap /kubeadm Upload the kubeadm ClusterConfiguration to a ConfigMap /kubelet Upload the kubelet component config to a ConfigMap upload-certs Upload certificates to kubeadm-certs mark-control-plane Mark a node as a control-plane bootstrap-token Generates bootstrap tokens used to join a node to a cluster kubelet-finalize Updates settings relevant to the kubelet after TLS bootstrap /enable-client-cert-rotation Enable kubelet client certificate rotation addon Install required addons for passing conformance tests /coredns Install the CoreDNS addon to a Kubernetes cluster /kube-proxy Install the kube-proxy addon to a Kubernetes cluster show-join-command Show the join command for control-plane and worker node Several commands are returned at the end of the installation process, which you’ll use in the next part.\nRetrieving kubeconfig file 🔗 The first set of commands returned during the initialization step allows configuring kubectl for the current user. Run those commands from a shell in the controlplane Node.\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You can now list the Nodes. You’ll get only one Node as you’ve not added the worker Nodes yet.\n$ kubectl get no NAME STATUS ROLES AGE VERSION controlplane NotReady control-plane 5m4s v1.32.4 Adding the first worker Node 🔗 As you’ve done for the controlplane, use the following command to install the dependencies (kubectl, containerd, kubeadm) on worker1.\ncurl https://luc.run/kubeadm/worker.sh | VERSION=\"1.32\" sh Then, run the join command returned during the initialization step. This command allows you to add worker nodes to the cluster.\nsudo kubeadm join 10.81.0.174:6443 --token kolibl.0oieughn4y03zvm7 \\ --discovery-token-ca-cert-hash sha256:a1d26efca219428731be6b62e3298a2e5014d829e51185e804f2f614b70d933d Adding the second worker Node 🔗 You need to do the same on worker2. First, install the dependencies.\ncurl https://luc.run/kubeadm/worker.sh | VERSION=\"1.32\" sh Then, run the join command to add this Node to the cluster.\nsudo kubeadm join 10.81.0.174:6443 --token kolibl.0oieughn4y03zvm7 \\ --discovery-token-ca-cert-hash sha256:a1d26efca219428731be6b62e3298a2e5014d829e51185e804f2f614b70d933d You now have cluster with 3 Nodes.\nStatus of the Nodes 🔗 List the Nodes and notice they are all in NotReady status.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION controlplane NotReady control-plane 9m58s v1.32.4 worker1 NotReady \u003cnone\u003e 58s v1.32.4 worker2 NotReady \u003cnone\u003e 55s v1.32.4 If you go one step further and describe the controlplane Node, you’ll get why the cluster is not ready yet.\n… KubeletNotReady container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized Installing a network plugin 🔗 Run the following commands from the controlplane Node to install Cilium in your cluster.\nOS=\"$(uname | tr '[:upper:]' '[:lower:]')\" ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-$OS-$ARCH.tar.gz{,.sha256sum} sudo tar xzvfC cilium-$OS-$ARCH.tar.gz /usr/local/bin cilium install After a few tens of seconds, you’ll see your cluster is ready.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION controlplane Ready control-plane 13m v1.32.4 worker1 Ready \u003cnone\u003e 4m28s v1.32.4 worker2 Ready \u003cnone\u003e 4m25s v1.32.4 Get the kubeconfig on the host machine 🔗 To avoid connecting to the controlplane Node to run the kubectl commands, copy the kubeconfig file from the controlplane to the host machine. Make sure to copy this file into $HOME/.kube/config so it automatically configures kubectl.\nIf you’ve created your VMs with Multipass, you can copy the kubeconfig file using the following commands.\nmultipass transfer controlplane:/home/ubuntu/.kube/config config mkdir $HOME/.kube mv config $HOME/.kube/config You should now be able to direcly list the Nodes from the host machine.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION controlplane Ready control-plane 13m v1.32.4 worker1 Ready \u003cnone\u003e 4m28s v1.32.4 worker2 Ready \u003cnone\u003e 4m25s v1.32.4 ","categories":"","description":"Build a 3-node kubeadm cluster from scratch.","excerpt":"Build a 3-node kubeadm cluster from scratch.","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/creation/","tags":"","title":"Create a cluster"},{"body":"Using a local cluster is very handy to get started with Kubernetes, or to test things quickly. In this example, we’ll use K3s, a lightweight Kubernetes distribution (5 ’s’ fewer than k8s :) ). K3s is a certified distribution, well-suited for IoT, Edge computing, and which works well with huge servers.\nIn this section, we’ll use Multipass to create an Ubuntu virtual machine and install k3s on this one. Multipass is a convenient tool to launch Ubuntu VM on Mac/Linux/Windows; it can be installed on your environment following the documentation.\ninfo 🔗 We use Multipass as it is handy and lightweight, but feel free to use the tool you’re the most comfortable with. If you use another tool, please just make sure to adapt the commands below.\nWe’ll only create a single node Kubernetes cluster in this section.\nPre-requisite 🔗 On your local machine, install kubectl. It’s the essential tool for communicating with a Kubernetes cluster from the command line.\nCreating an Ubuntu VM 🔗 Once you’ve installed Multipass, create an Ubuntu 24.04 virtual machine named k3s with 2G of memory allocated. This process should take a few tens of seconds.\nmultipass launch --name k3s --memory 2G Then get the IP address of the newly created VM.\nIP=$(multipass info k3s | grep IP | awk '{print $2}') Installing k3s 🔗 Run the following command to install k3s the VM. This process should also take a few tens of seconds.\nmultipass exec k3s -- bash -c \"curl -sfL https://get.k3s.io | sh -\" info 🔗 The command curl -sfL https://get.k3s.io | sh -, used to install k3s comes from the official k3s documentation\nGetting the kubeconfig file 🔗 Retrieve the configuration file generated during Kubernetes installation on your local machine:\nmultipass exec k3s sudo cat /etc/rancher/k3s/k3s.yaml \u003e k3s.cfg.tmp In this file, replace the local IP address (127.0.0.1) with the IP address of the VM you created. This IP address should be in the $IP environment variable.\ncat k3s.cfg.tmp | sed \"s/127.0.0.1/$IP/\" \u003e k3s.cfg Then set the KUBECONFIG environment variable to point to the previously retrieved configuration file:\nexport KUBECONFIG=$PWD/k3s.cfg Info 🔗 This environment variable configures the kubectl binary so it can communicate with the cluster.\nYou can now communicate with the cluster’s API Server.\nList of the cluster’s node (only one in this example):\nkubectl get nodes List of the Pods running in the cluster:\nkubectl get pods -A You’ll use this local cluster to do the exercises of the next section.\n","categories":"","description":"Create Local kubernetes cluster","excerpt":"Create Local kubernetes cluster","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/local/","tags":"","title":"Creation of a local cluster"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/cs-and-cloud/","tags":"","title":"CS \u0026 Cloud"},{"body":" Explained 🔗 Incorporating cloud computing into the Corporate Sustainability Reporting Directive (CSRD) context represents a strategic alignment between technological innovation and sustainable business practices. The role of cloud computing in the context of the CSRD can be significant for several reasons.\nReasons 🔗 Data Management \u0026 Reporting Transparency \u0026 Enhanced Accessibility Compliance \u0026 Standardization Assurance \u0026 Verification Scalability \u0026 Efficiency Cloud Computing is not only a game-changer for operational efficiency and strategic innovation but also a powerful ally in the journey towards Corporate Sustainability and Responsibility.\nData Management \u0026 Reporting 🔗 Sustainability Reporting ESG Factors Cloud Computing The CSRD requires extensive sustainability reporting, which includes detailed information on environmental, social, and governance (ESG) factors. Cloud computing can offer robust solutions for managing, storing, and processing the vast amounts of data required for these reports. With cloud-based systems, companies can more easily aggregate, analyze, and report sustainability data, improving the accuracy and reliability of the information presented.\nEnhanced Accessibility \u0026 Transparency 🔗 Transparency Cloud services Global accessibility One of the CSRD’s goals is to increase transparency and make sustainability information more accessible to investors, regulators, and the public. Cloud services can facilitate this by hosting sustainability reports and relevant data on platforms easily accessible worldwide. This global accessibility enhances transparency and allows for broader scrutiny, aligning with the CSRD’s intentions to make sustainability information central to corporate reporting.\nStandardization \u0026 Compliance 🔗 Standardization Cloud-based reporting tools Compliance The CSRD mandates compliance with European sustainability reporting standards, aiming for standardization across reports. Cloud-based reporting tools can be designed or configured to adhere to these standards, streamlining company compliance processes. By utilizing cloud solutions, businesses can ensure their reporting meets the required specifications, reducing non-compliance risk.\nAssurance \u0026 Verification 🔗 Assurance requirements Cloud-based systems Auditable trails With the CSRD introducing mandatory assurance requirements for sustainability information, cloud computing can aid in the preparation and verification process. By leveraging cloud-based systems, companies can maintain clear, auditable trails of their data and reporting workflows, facilitating the assurance process. This can help ensure that reports meet the limited or reasonable assurance standards required over time.\nScalability \u0026 Efficiency 🔗 Scalability Data volumes Report generation As companies grow and their sustainability reporting requirements become more complex, cloud computing offers the scalability to accommodate increasing data volumes and computational demands. This scalability supports more efficient report generation, analysis, and dissemination processes, making it easier for companies to evolve their reporting practices in line with the expanding scope and depth of the CSRD.\n","categories":"","description":"","excerpt":" Explained 🔗 Incorporating cloud computing into the Corporate …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd-and-cloud/content/csrd-and-cloud/","tags":"","title":"CSRD \u0026 Cloud"},{"body":" EU CSRD 🔗 Sustainability Reporting Regulation Green Deal Expansion CSRD Rollout Complexity Driving Sustainable Economy Demystifying Corporate Sustainability Sustainability Reporting Regulation 🔗 The European Union’s Corporate Sustainability Reporting Directive (CSRD) is a significant regulatory development aimed at enhancing and standardizing the sustainability reporting by companies.\nGreen Deal Expansion 🔗 Building on the foundations of the Non-Financial Reporting Directive (NFRD), the CSRD was proposed by the European Commission in April 2021 as part of the EU’s Green Deal and Sustainable Finance Agenda.\nCSRD Rollout Complexity 🔗 The CSRD’s implementation originally set for 2024 (FY 2023) for NFRD companies, faces potential delays due to standard complexity and legal preparations, with new companies reporting from 2025 (FY 2024) onwards based on size and NFRD status.\nDriving Sustainable Economy 🔗 As a key component of the EU’s sustainable finance agenda, the CSRD aims to foster a more sustainable economy by enhancing transparency, accountability, and promoting sustainable corporate practices.\nDemystifying Corporate Sustainability 🔗 Its goal is to make sustainability information more accessible worldwide, helping investors, consumers, and policymakers understand companies’ green efforts. This enables smarter decisions and promotes eco-friendly practices.\n","categories":"","description":"","excerpt":" EU CSRD 🔗 Sustainability Reporting Regulation Green Deal Expansion …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd/content/definition/","tags":"","title":"Deinition"},{"body":" To play with Istio and demonstrate some of it’s capabilities, you will deploy the example BookInfo application, which is included the Istio package.\nWhat is the Bookinfo Application 🔗 This application is a polyglot composition of microservices are written in different languages and sample BookInfo application displays information about a book, similar to a single catalog entry of an online book store. Displayed on the page is a description of the book, book details (ISBN, number of pages, and so on), and a few book reviews.\nIt’s worth noting that these services have no dependencies on Istio, but make an interesting service mesh example, particularly because of the multitude of services, languages and versions for the reviews service.\nAs shown in the figure below, proxies are sidecarred to each of the application containers.\nFigure: BookInfo deployed on the mesh\nSidecars proxy can be either manually or automatically injected into the pods. Automatic sidecar injection requires that your Kubernetes api-server supports admissionregistration.k8s.io/v1 or admissionregistration.k8s.io/v1beta1 or admissionregistration.k8s.io/v1beta2 APIs. Verify whether your Kubernetes deployment supports these APIs by executing:\nkubectl api-versions | grep admissionregistration If your environment does NOT supports either of these two APIs, then you may use manual sidecar injection to deploy the sample app.\nAs part of Istio deployment in Previous chapter, you have deployed the sidecar injector.\nDeploying Sample App with Automatic sidecar injection Istio, deployed as part of this workshop, will also deploy the sidecar injector. Let us now verify sidecar injector deployment.\nkubectl -n istio-system get configmaps istio-sidecar-injector Output:\nNAME DATA AGE istio-sidecar-injector 2 9h NamespaceSelector decides whether to run the webhook on an object based on whether the namespace for that object matches the selector.\nkubectl get namespace -L istio-injection Output:\nNAME STATUS AGE ISTIO-INJECTION default Active 1h enabled istio-system Active 1h disabled kube-public Active 1h kube-system Active 1h Using Meshery, navigate to the Istio management page.\nEnter default in the Namespace field. Click the (+) icon on the Sample Application card and select BookInfo Application from the list. This will do 3 things:\nLabel default namespace for sidecar injection. Deploys all the BookInfo services in the default namespace. Deploys the virtual service and gateway needed to expose the BookInfo’s productpage application in the default namespace. Verify Bookinfo deployment 🔗 Verify that the deployments are all in a state of AVAILABLE before continuing. watch kubectl get deployment Choose a service, for instance productpage, and view it’s container configuration: kubectl get po kubectl describe pod productpage-v1-..... Examine details of the services: kubectl describe svc productpage Next, you will expose the BookInfo application to be accessed external from the cluster.\nAlternative: Manual installation 🔗 Follow this if the above steps did not work for you\nLabel namespace for injection 🔗 Label the default namespace with istio-injection=enabled\nkubectl label namespace default istio-injection=enabled kubectl get namespace -L istio-injection Output:\nNAME STATUS AGE ISTIO-INJECTION default Active 1h enabled istio-system Active 1h disabled kube-public Active 1h kube-system Active 1h Deploy BookInfo 🔗 Applying this yaml file included in the Istio package you collected in Getting Started will deploy the BookInfo app in you cluster.\nkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Deploy Gateway and Virtual Service for BookInfo app 🔗 kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml Manual Sidecar Injection Use this only when Automatic Sidecar injection doesn't work\nTo do a manual sidecar injection we will be using istioctl command:\ncurl https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - \u003e newBookInfo.yaml Observing the new yaml file reveals that additional container Istio Proxy has been added to the Pods with necessary configurations:\nimage: docker.io/istio/proxyv2:1.3.0 imagePullPolicy: IfNotPresent name: istio-proxy We need to now deploy the new yaml using kubectl\nkubectl apply -f newBookInfo.yaml To do both in a single command:\nkubectl apply -f \u003c(curl https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f -) Now continue to Verify Bookinfo deployment.\n","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/advance-concepts-of-service-meshes/deploy-an-application/","tags":"","title":"Deploy a sample application"},{"body":" To play with Istio and demonstrate some of it’s capabilities, you will deploy the example BookInfo application, which is included the Istio package.\nWhat is the Bookinfo Application 🔗 This application is a polyglot composition of microservices are written in different languages and sample BookInfo application displays information about a book, similar to a single catalog entry of an online book store. Displayed on the page is a description of the book, book details (ISBN, number of pages, and so on), and a few book reviews. The end-to-end architecture of the application is shown in the figure.\nIt’s worth noting that these services have no dependencies on Istio, but make an interesting service mesh example, particularly because of the multitude of services, languages and versions for the reviews service. As shown in the figure below, proxies are sidecarred to each of the application containers.\nSidecars proxy can be either manually or automatically injected into the pods. Automatic sidecar injection requires that your Kubernetes api-server supports `admissionregistration.k8s.io/v1` or `admissionregistration.k8s.io/v1beta1` or `admissionregistration.k8s.io/v1beta2` APIs. Verify whether your Kubernetes deployment supports these APIs by executing: kubectl api-versions | grep admissionregistration If your environment does NOT supports either of these two APIs, then you may use manual sidecar injection to deploy the sample app.\nAs part of Istio deployment in Previous chapter, you have deployed the sidecar injector.\n### **Deploying Sample App with Automatic sidecar injection** Istio, deployed as part of this workshop, will also deploy the sidecar injector. Let us now verify sidecar injector deployment. kubectl -n istio-system get configmaps istio-sidecar-injector Output:\nNAME DATA AGE istio-sidecar-injector 2 9h NamespaceSelector decides whether to run the webhook on an object based on whether the namespace for that object matches the selector.\nkubectl get namespace -L istio-injection Output:\nNAME STATUS AGE ISTIO-INJECTION default Active 1h enabled istio-system Active 1h disabled kube-public Active 1h kube-system Active 1h Using Meshery, navigate to the Istio management page.\nEnter default in the Namespace field. Click the (+) icon on the Sample Application card and select BookInfo Application from the list. This will do 3 things:\nLabel default namespace for sidecar injection. Deploys all the BookInfo services in the default namespace. Deploys the virtual service and gateway needed to expose the BookInfo’s productpage application in the default namespace. Verify Bookinfo deployment{\" \"} Verify that the deployments are all in a state of AVAILABLE before continuing. watch kubectl get deployment Choose a service, for instance productpage, and view it’s container configuration: kubectl get po kubectl describe pod productpage-v1-..... Examine details of the services: kubectl describe svc productpage Next, you will expose the BookInfo application to be accessed external from the cluster.\n#### **Alternative: Manual installation** Follow this if the above steps did not work for you Label namespace for injection 🔗 Label the default namespace with istio-injection=enabled\nkubectl label namespace default istio-injection=enabled kubectl get namespace -L istio-injection Output:\nNAME STATUS AGE ISTIO-INJECTION default Active 1h enabled istio-system Active 1h disabled kube-public Active 1h kube-system Active 1h Deploy BookInfo 🔗 Applying this yaml file included in the Istio package you collected in Getting Started will deploy the BookInfo app in you cluster.\nkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml Deploy Gateway and Virtual Service for BookInfo app 🔗 kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml Manual Sidecar Injection 🔗 Use this only when Automatic Sidecar injection doesn’t work\nTo do a manual sidecar injection we will be using istioctl command:\ncurl https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - \u003e newBookInfo.yaml Observing the new yaml file reveals that additional container Istio Proxy has been added to the Pods with necessary configurations:\nimage: docker.io/istio/proxyv2:1.3.0 imagePullPolicy: IfNotPresent name: istio-proxy We need to now deploy the new yaml using kubectl\nkubectl apply -f newBookInfo.yaml To do both in a single command:\nkubectl apply -f \u003c(curl https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f -) Now continue to Verify Bookinfo deployment.\n","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/introduction-to-service-meshes/deploy-an-application/","tags":"","title":"Deploy a Sample Application"},{"body":"The Dapr control plane is a set of services responsible for managing and orchestrating the runtime and configuration of Dapr. The control plane ensures the smooth operation, deployment, and management of Dapr sidecars and their interactions within a microservices architecture.\nSteps 🔗 Import Dapr Helm Chart 🔗 In the left sidebar, click the upward arrow symbol (import icon) to import the file into Meshery. In the modal that appears: Enter a name for your design in the Design File Name field (e.g. DAPR). Select Helm Chart from the Design Type dropdown menu. Navigate to the Dapr Helm Chart page on Artifact Hub. Right-click this_link at the bottom of the installation modal to copy the link address. Paste the copied link in the URL field. Then, click Import. On the Designs tab on the left, click on the DAPR design you just imported. This will display the various Kubernetes resource components required for deploying the Dapr control plane on the canvas. Design Interpretation 🔗 From the design above, several key components that constitute the Dapr control plane can be observed, including:\nDapr Sidecar Injector: Automatically injects Dapr sidecars into the application pods. Dapr Operator: Manages the lifecycle of Dapr components and configurations within the Kubernetes cluster. Dapr Placement Service: Provides service discovery and routing for stateful applications, ensuring that stateful actors are placed correctly. Dapr Sentry: Handles certificate authority and manages secure communication between Dapr instances. Let’s take a closer look at some of the relationships between these components.\nStatefulsets, Deployments, and Services\nThe triangles represent services, the rectangles represent deployments, and the cylinder icons represent statefulsets. Arrows indicate that a service is associated with a deployment/statefulset, exposing network access to it, indicative of an Edge-Network relationship. The ports exposed by each service are shown at the ends of the arrows. To learn more about how to identify these components, see Component Shape Guide.\nTo briefly take a look at one of these network relationships, click on the dapr-operator container in the deployment and its service to open their configuration tabs.\nFrom the diagram above, the container within the dapr-operator deployment is configured to expose Container port 6500. The dapr-api service listens on port 443 and forwards incoming traffic to Target Port 6500. This is why the arrow is labeled with 443/TCP.\nThis illustrates how exposed ports are mapped to their corresponding services within a deployment configuration.\nRoles, ClusterRoles, and ServiceAccounts\nThese resources represent Roles, ClusterRoles, ServiceAccounts, and other necessary RBAC configurations that the Dapr control plane requires within the cluster. An Edge-permission relationship exists between some of these components, which are bound through RoleBindings and ClusterRoleBindings.\nCustom Resource and Custom Resource Definitions\nDapr leverages several Custom Resource Definitions (CRDs) to allow users manage and define its configuration and operation. The key custom resources are Configurations, Components, Resiliency, Subscriptions, HTTPEndpoints.\nThe daprsystem icon above is a custom resource of kind Configuration. It is used to configure global settings for Dapr components within a Kubernetes cluster.\nThe Dapr State Store component that manages Redis, as we mentioned before in the introduction, is a custom resource of kind Component. The Component CRD defines the configuration for external resources such as state stores, pub/sub brokers, and secret stores.\nWe can also explore relationships through grouped components.\nGrouped Components\nClick on the Group Components icon on the dock at the bottom of the canvas to group resources based on shared labels or annotations.\nUsing Meshery’s grouping feature, we can organize related components, enhancing our understanding of the Dapr control plane within the Kubernetes environment. Let’s take a detailed look at the first group in the image above. It is the Dapr-sidecar-injector group consisting of these components:\nDeployment Service, and Mutating Webhook Configuration When a new application pod is deployed, the dapr-sidecar-injector deployment intercepts the pod creation request using the Mutating Webhook Configuration and programmatically attaches the Dapr sidecar to the pod based on configured annotations.\nThe grouping of these components illustrates their interconnections, enhancing our ability to visualize and understand how various parts function together within the cluster.\nDeploy Dapr 🔗 The Dapr control plane has to be deployed in the dapr-system namespace. Click on the namespaced components (deployments, services, etc.) to open up the configuration tab. Then change the namespace from default to dapr-system. To deploy, click Actions in the top right corner and click on Deploy (double tick). To learn about deploying Designs in Meshery, see Chapter Deploying Meshery Designs. After deployment, Click on Open In Visualizer to see the pre-filtered view of the deployed resources in the cluster. In Visualizer mode, use the filter to adjust the views of the resources in the cluster. For View Selector select Single Node. For Kinds select the resources you want to see including Deployments, Pods, Services, Statefulset, Secret, Replicaset, Endpoints and Endpoint slices. Using Meshery, we have visualized the components of the Dapr control plane, explored their relationships, and successfully deployed Dapr. Our next step is to integrate the Redis store into our architecture.\n","categories":"","description":"This chapter takes you through the import, exploration and deployment of Dapr control plane components","excerpt":"This chapter takes you through the import, exploration and deployment …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/explore-dapr-with-meshery/dapr/deploy-dapr-control-plane/","tags":"","title":"Deploy Dapr Control Plane"},{"body":" Introduction 🔗 Deploying Meshery designs is a common and essential task for managing your infrastructure and workloads. Having a thorough understanding of this procedure and Meshery’s design deployment behavior is key to ensuring that your component configurations work as intended in a live Kubernetes environment. Meshery’s design deployment process not only attests to the validity of your configuration, but also offers a seamless transition from Designer mode to Visualizer mode, where you can visually inspect your deployment, and begin to manage live-running deployments in your environment.\nMeshery’s deployment process encompasses four steps: validation, dry-run, environment selection, and finally, executing the deployment itself.\nThis chapter guides you through each of these four steps. Along the way, you will learn what a successful step looks like, understand common failure scenarios, and go through strategies for troubleshooting and resolving any issues that might arise. Additionally, you will learn about Meshery’s Notification Center and how to best use it to your advantage.\nPrerequisites\nEnsure you have one or more Meshery designs available.\nAccess to Meshery (Self-Hosted or Meshery Playground).\nAction Button 🔗 There are two modes on the Actions button:\nClicking on the word Actions presents a set of choices, each labeled with its respective action. Selecting an option opens a modal window, providing detailed guidance.\nClicking on the Action drop-down icon reveals a list of icons. Selecting an icon allows seasoned users to perform actions with a single click, bypassing the modal interaction.\nIn this chapter, we focus on utilizing the first option to provide a comprehensive view of the deployment process.\nDesign Validation 🔗 Validation is the first step in deploying designs with Meshery. While optional, it is highly recommended as it helps identify and resolve possible misconfigurations upfront.\nHow Validation Works 🔗 Meshery uses static analysis to verify your design. It checks all components within your design and all configured and unconfigured properties of your components against well-defined schemas based on Meshery Models.\nThis comprehensive validation ensures that:\nThe design adheres to the expected structure and format. All components are valid and recognized by Meshery. There are no missing required configurations. Performing Validation 🔗 To validate your design, navigate to the Actions button at the top of the Design canvas.\nClick on the Validate Icon.\nIf the validation is successful, you will see a modal displaying the number of components validated and the number of annotations, similar to the one shown below:\n{\" “}\nHandling Validation Errors 🔗 If your design fails validation checks, the modal will indicate the number of errors detected. Each error will provide specific information about the component or annotation that caused the failure. Use this detailed feedback to identify and correct the issues in your design before proceeding with the deployment.\nValidation Errors 🔗 A common validation error is:\nMissing Required Field: This happens when a required field is not provided. For example, in the image below, the first error indicates that the field “.spec.template.spec.containers.0.env.0” must have a value.\nTo troubleshoot and remediate validation issues:\nClick on the error on the modal. This will open the configuration tab of the component that has that error. Fix the configuration using the error details provided. Re-run validation after any significant changes to your design to ensure all issues are resolved. The validation process is confined to schema-based checks within Meshery and does not require communication with your target environment. The Dry Run step, however, does involve this communication. Let’s explore the Dry Run step next.\nDesign Dry Run 🔗 A dry run in Meshery simulates the deployment of your design in the selected target environment without making any actual changes. This step is highly beneficial as it helps identify potential issues before they occur, ensuring a smoother and more reliable deployment process.\nPerforming Dry Run 🔗 Navigate to the Actions button at the top of the Design canvas.\nClick on the Dry Run icon.\nReview the results to identify any potential issues.\nMake necessary adjustments to your configuration based on the feedback provided by the dry run.\nRe-run the dry run to ensure all issues have been resolved.\nExamples of Dry Run Errors 🔗 Some examples of dry run errors are:\nInvalid Field Value: The error message indicates that a field has an invalid value. For instance, in the image below, the fields “spec \u003e ports[0] \u003e port” and “spec \u003e ports[0] \u003e targetPort” have invalid values of 0. These values must be between 1 and 65535, inclusive.\nMissing Required Field\nMissing Dependencies: In this case, the error occurs because a Kubernetes Custom Resource Definition (CRD) should have been deployed first before attempting to deploy this component.\nTo resolve this, ensure that all necessary dependencies, such as CRDs, are deployed before deploying the components that rely on them.\nEnvironment Selection 🔗 Meshery Environments are logical groups of Connections which include GitHub integrations, Prometheus connections, and Kubernetes Clusters and so on managed by Meshery. To make your Kubernetes connections available for selection, you need to assign them to an environment and link that environment to your current workspace. Note: Environments can be assigned to one or more Workspaces.\nAdding an Environment During Deployment 🔗 During deployment, you will be required to select an environment.\nYou can add an environment through the deployment modal by following these steps:\nClick on Add Environments\nSelect Create and put in a name for the environment, e.g.Development, and Save.\nAfter creating the environment you add connections to the environment, here we want to add a Kubernetes cluster. Click on the arrows icon to open the Development Resources modal.\nAvailable Connections on the left side shows a list of Kubernetes clusters that are currently managed by Meshery.\nAdd Kubernetes Connections If you do not have any Kubernetes connections available, refer to this documentation on how to manage your clusters with Meshery: Managing Kubernetes Clusters with Meshery. Select a cluster, and use the arrow to assign just the selected connections and save. Missing Connections 🔗 During the deployment, if a connection has not yet been added to your environment, it will appear as shown below.\nIn this scenario, to add a new environment.\nNavigate to the left sidebar of Meshery Click on the Lifecycle dropdown and you will see the Environment Icon under it Click on it and you can follow the same steps we mentioned above to add a connection to your environment. Selecting an Environment for an Existing Connection 🔗 Another way to add a connection to an Environment is by selecting an environment for an existing connection. To do this:\nClick on the Lifecycle icon to view a list of connections with their attributes, including environments, kind (type of connection), and connection status.\nIdentify the desired connection. Under the Environment section, click the dropdown menu to add and select the environment you want to associate with your connection.\nVerifying Kubernetes Connections 🔗 The Kubernetes connection icon at the top right corner of the screen shows the list of connected Kubernetes clusters. Clicking on the icon will invoke an ad hoc connectivity test between your Meshery Server and the specific Kubernetes cluster. Should this check fail, verify the health of your Meshery Operator deployment within that cluster.\nEnvironment Error 🔗 If your environment is not properly set up before deployment, you may encounter the error below.\nTo handle this error follow the suitable steps for adding a connection to your environment as previously discussed.\nDeployment 🔗 At this stage, you deploy your resources to your available Kubernetes cluster connection(s) managed by Meshery. First, ensure the connections to your clusters are established and configured correctly, placing them in the appropriate environments. This ensures you have control over your deployment strategy.\nKubernetes Cluster Management using Meshery 🔗 Meshery Server deploys as a single container. Whether the Meshery Server container is deployed in a stand-alone Docker host or inside a Kubernetes cluster, Meshery Server is fully capable of managing multiple Kubernetes clusters. For more information on how Meshery Server connects to and continually synchronizes with your Kubernetes cluster(s), see Meshery Operator and MeshSync.\nUnderstanding Meshery Operator and Kubernetes Cluster Relationships One-to-One Relationship: There is a one-to-one relationship between a Meshery Operator and a Kubernetes cluster. This means each Meshery Operator is associated with exactly one Kubernetes cluster. This applies whether the cluster is a managed cluster (like the one you’re adding as a connection) or the cluster where the Meshery Server is deployed.\nMany-to-One Relationship: There is a many-to-one relationship between Meshery Operator and Meshery Server. Multiple Meshery Operators can be associated with a single Meshery Server. This means a single Meshery Server can manage several Kubernetes clusters through different Meshery Operators.\nIn summary, while each Kubernetes cluster has its own Meshery Operator, a single Meshery Server can interact with multiple Kubernetes clusters through these Operators.\nAvailable Clusters 🔗 Playground Users Users of Meshery Playground should see a pre-registered Kubernetes connection, representing the sandbox cluster available with the playground environment. You have the option of using the live cluster provided by Meshery Playground or connecting your own Kubernetes cluster using your kubeconfig file. During deployment, these clusters will be available as connections for you to select.\nDeploying Designs 🔗 To deploy a design, navigate to the Actions button at the top of the Design canvas.\nClick on the Deploy icon.\nThis opens a modal that will take you through all the steps before the final deployment.\nClick on Open In Visualizer to see the pre-filtered view of the deployed resources in the cluster.\nClick Finish.\nDeployment Errors 🔗 Missing Namespace: This error occurs when you attempt to create a Kubernetes resource without specifying a namespace. Kubernetes requires that all resources have an associated namespace.\nEmpty Label Selector: This error indicates an empty label selector.\nTroubleshooting Errors 🔗 When reviewing validation, dry run, or deployment issues, you’ll notice specific error codes denoted from time to time.\nAs a system, Meshery itemizes different errors that occur and assigns a unique error code to each along with details on how to remediate the issue at hand.\nFor the comprehensive list of error codes refer to Error Code Reference in the Meshery documentation.\nIf you encounter persistent issues, consider consulting the Meshery Community forum.\nUsing the Notification Center for Troubleshooting 🔗 The Notification Center in Meshery helps manage events during the deployment process. It provides real-time updates and alerts on the status of the deployment. This feature can be particularly useful for troubleshooting, as it:\nDisplays immediate feedback on the success or failure of each deployment step.\nHighlights specific error messages and codes, helping you quickly identify and understand issues.\nOffers links to detailed documentation and guides for resolving common problems.\nKeeps a log of past notifications, allowing you to track and review previous errors and their resolutions.\nBy actively monitoring the Notification Center, you can promptly address issues as they arise, ensuring a smoother deployment process. Learn more about Managing Events with the Notification Center.\nUndeploy 🔗 To undeploy the resources\nNavigate to the Actions button at the top of the Meshery Design canvas.\nClick on the Undeploy icon.\nConclusion 🔗 In this chapter, you learned how to effectively deploy Meshery designs, ensuring your resources are deployed in the correct environment. You started by validating your designs to ensure configurations adhere to Kubernetes API specifications and best practices. Next, you did a dry run to simulate deployments without making actual changes. You also explored how to select and configure environments to manage Kubernetes connections seamlessly, and finally, how to deploy resources to your preferred cluster.\nThroughout these steps, you encountered common errors and learned how to address them. During these steps, you learned how to use Meshery’s Notification Center to troubleshoot issues, helping you through the deployment process.\n","categories":"","description":"This chapter covers the steps for deploying Meshery Designs, including validation, dry-run, environment selection, and deployment. It explains how to carry out these actions, addresses possible errors, and provides remediation strategies. Additionally, it covers how to use the Notification Center for troubleshooting errors and viewing the deployment status.","excerpt":"This chapter covers the steps for deploying Meshery Designs, including …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-meshery/introduction-to-meshery/deploying-meshery-designs/","tags":"","title":"Deploying Meshery Designs"},{"body":"","categories":"","description":"Manage Pods lifecycle","excerpt":"Manage Pods lifecycle","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/deployment/","tags":"","title":"Deployment"},{"body":" Explained 🔗 DevOps is a software development methodology emphasizing collaboration and communication between development and operations teams to streamline the software development lifecycle. It involves automating and integrating processes across the entire software delivery pipeline, from development and testing to deployment and monitoring. DevOps aims to deliver software faster, with higher quality and reliability, by breaking down silos between teams and fostering a culture of continuous improvement and feedback.\nDevOps is a powerful approach to software development that can help businesses achieve greater agility, efficiency, and quality in their software delivery process. By breaking down silos between development and operations teams, DevOps can foster greater collaboration and communication, leading to faster and more reliable software delivery. Additionally, DevOps emphasizes automation and integration across the entire software delivery pipeline, allowing businesses to streamline their development processes and reduce the risk of errors and delays.\nHowever, some potential challenges to implementing DevOps exist, mainly when working with microservices or distributed systems. For example, managing many microservices can require additional tools and expertise. Additionally, DevOps can introduce other security risks if not properly configured and managed. Despite these challenges, DevOps is becoming increasingly popular in modern software development. It allows businesses to deliver software faster, with higher quality and reliability, while improving team collaboration and communication.\n","categories":"","description":"","excerpt":" Explained 🔗 DevOps is a software development methodology emphasizing …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud-native/content/devops/","tags":"","title":"DevOps"},{"body":"In this chapter, you will import and deploy Edge Stack Custom Resource Definitions (CRDs) and YAML files. You’ll learn how to configure the necessary Ambassador Cloud license and set up essential components like Listener and Mapping resources. Finally, you’ll deploy a sample Quote service application to route traffic through Edge Stack.\nSteps 🔗 Import Edge Stack CRD YAML and Deploy 🔗 The Edge Stack CRD YAML file typically contains the definitions for custom resources used by Edge Stack. These definitions include the schemas and validation rules for resources like Mappings, Hosts, TLSContexts, RateLimits, Filters, and more. These custom resources allow you to define and manage the various aspects of your API gateway configuration, such as routing, authentication, rate limiting, and TLS settings, directly within your Kubernetes cluster.\nIn the left sidebar, click on the upward arrow symbol (import icon) to import the file into Meshery.\nIn the modal that appears:\nEnter a name for your design in the Design File Name field (e.g. Edge-stack-crd). Select Kubernetes Manifest from the Design Type dropdown menu. Choose Url upload for the upload method, and paste in the Edge-Stack-crd YAML link. Then, click on Import. Click on the name of the design on the Designs tab to display the visual representations of the various Kubernetes resources and their relationships on the canvas.\nClick Actions in the top right corner and click on Deploy (double tick).\nTo check the status of your deployment, click on the notification icon on the top right corner.\nClick on Open In visualizer to navigate to the Visualize section and see a pre-filtered view of the deployed resources in the cluster. Import the Edge Stack YAML 🔗 Now that the CRDs have been deployed, go ahead to deploy the main Edge Stack Configuration.\nFollow the previous Steps to Import the Edge Stack YAML into Kanvas with the name Edge-Stack. You can use the Group Components icon on the dock below to group resources based on shared labels or annotations. Configure Edge Stack License 🔗 Ambassador Edge Stack requires a valid license to operate. Generate your license token to establish a secure connection between Edge Stack and Ambassador Cloud.\nComplete the steps on the Ambassador Cloud to generate your license token, then copy the token. Be sure to convert it into base64 format. You can use an online tool for this conversion.\nClick on the Secret component on the design canvas and input the token.\nDeploy Edge Stack 🔗 Click Actions in the top right corner and click on Deploy (double tick).\nClick on Open In visualizer to navigate to the Visualize section and see a pre-filtered view of the deployed resources in the cluster.\nListener Custom Resource 🔗 The Listener Custom Resource tells Ambassador Edge Stack what port to listen on.\nCopy the following the YAML and save it to a file called listener.yaml , then import it into Kanvas. --- apiVersion: getambassador.io/v3alpha1 kind: Listener metadata: name: edge-stack-listener-8080 namespace: ambassador spec: port: 8080 protocol: HTTP securityModel: XFP hostBinding: namespace: from: ALL --- apiVersion: getambassador.io/v3alpha1 kind: Listener metadata: name: edge-stack-listener-8443 namespace: ambassador spec: port: 8443 protocol: HTTPS securityModel: XFP hostBinding: namespace: from: ALL Deploy the resource on Kanvas. Mapping Resource 🔗 Create a Mapping configuration that instructs Edge Stack on how and where to route traffic. In the YAML file below, any request coming to the specified hostname with the prefix /backend/ will be directed to the quote service.\nCopy the following the YAML and save it to a file called mapping.yaml , then import it into Kanvas.\napiVersion: getambassador.io/v3alpha1 kind: Mapping metadata: name: quote-backend spec: hostname: \"*\" prefix: /backend/ service: quote docs: path: \"/.ambassador-internal/openapi-docs\" Deploy Quote Service 🔗 Next, import the Quote Service YAML and deploy it on Kanvas. This step will create the necessary deployment and service resources for the Quote service within your Kubernetes cluster, allowing you to see how Edge Stack manages and routes traffic to this backend service.\n","categories":"","description":"This chapter covers the configuration of the Edge Stack, including importing and deploying Edge Stack Custom Resource Definitions (CRDs) and YAML files. You will learn how to configure the necessary Ambassador Cloud license and set up essential components like Listener and Mapping resources. Finally, you will deploy a sample Quote service application to route traffic through Edge Stack.","excerpt":"This chapter covers the configuration of the Edge Stack, including …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/ambassador-edge-stack-api-gateway-with-meshery/edge/edge-stack-configuration/","tags":"","title":"Edge Stack Configuration"},{"body":" Overview 🔗 For businesses looking to enhance their corporate sustainability, selecting cloud providers that prioritize environmental sustainability, use renewable energy, and have a transparent sustainability reporting process is essential. This ensures their move to the cloud aligns with broader sustainability goals and contributes positively to their overall environmental impact.\nEnergy Efficiency Resource Optimization Supporting Remote Work Green Cloud Solutions Energy Efficiency 🔗 Data Center Efficiency 🔗 Cloud Providers Economies of Scale Reduced Emissions Cloud providers invest in state-of-the-art data center technologies that are significantly more energy-efficient than traditional on-site data centers. Thanks to economies of scale, cloud data centers maximize energy efficiency, leading to reduced greenhouse gas emissions.\nUtilization and Scale 🔗 Cloud Services Scaling to Demand Optimized Energy Consumption Cloud services operate at higher utilization rates and scale according to demand, ensuring energy consumption aligns more closely with actual usage. This avoids the wasteful energy consumption typical of underutilized servers. Resource Optimization\nInfrastructure Consolidation 🔗 Consolidate Digital Infrastructure Migration to the Cloud Minimizing Data Center Footprint Organizations can consolidate their digital infrastructure by migrating to the cloud, minimizing the physical footprint and associated energy usage of maintaining multiple data\nShared Resources 🔗 Efficient Use Pooling Resources Reducing Material Waste By pooling resources, cloud computing enables more efficient use of physical hardware, reducing the need for physical devices and their material waste. centers. Supporting Remote Work\nReduced Commuting 🔗 Facilitate Remote Working Reduces Commuting Lowering Carbon Emissions Cloud computing facilitates remote work by providing employees access to data and applications from anywhere. This reduces the need for commuting, lowering carbon emissions related to transport.\nVirtual Collaboration 🔗 Virtual Collaboration \u0026 Meetings Reducing Business Travel Positive Environmental Impact Virtual meetings and collaboration, supported by tools and platforms hosted in the cloud, help reduce the need for business travel and its environmental impact. Green Cloud Solutions\nRenewable Energy 🔗 Cloud Data Centers Greener Alternatives Wind/Solar Powered Leading cloud providers increasingly power their data centers with renewable energy sources, such as wind and solar power, making cloud computing a greener alternative.\nCarbon Neutral Commitments 🔗 Committed Cloud Providers Renewable Energy Projects Sustainability Initiatives Many cloud providers have committed to achieving carbon neutrality through various sustainability initiatives, including purchasing carbon offsets and investing in renewable energy projects.\n","categories":"","description":"","excerpt":" Overview 🔗 For businesses looking to enhance their corporate …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/cs-and-cloud/content/enhancing-sustainability/","tags":"","title":"Enhancing Sustainability"},{"body":" Errors, Debugging … 🔗 The art of identifying and finding errors in IT systems, if it’s code or if it’s infrastructure configurations, today they are almost the same. However, both sides of IT systems challenge us with increased complexity, the cost for more flexibility, features, and performance.\nHence, the toolbox we must have to conquer those challenges needs to be fine-honed to do the trick. The preferences in tools are vast, and we look at both diametral ends of tools sets, graphical user interface, and command line.\nErrors, Debugging … 🔗 Video: Errors, Debugging … Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Errors, Debugging … 🔗 The art of identifying and finding errors in IT …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/troubleshooting/content/errors-debugging/","tags":"","title":"Errors, Debugging ..."},{"body":"","categories":"","description":"Learn about Exoscale, its services, and how to use them effectively.","excerpt":"Learn about Exoscale, its services, and how to use them effectively.","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/exoscale/","tags":"","title":"Exoscale"},{"body":" Exoscale DBaaS Features 🔗 Daily Backups included 🔗 Backups are done on a daily basis and are included with every DBaaS offering.\nCompletely Integrated 🔗 Integrated DBaaS for your instances. Easily manage your database, instance, or storage from the same interface.\nNo Vendor Lock-In 🔗 Keep your cloud infrastructure independent and flexible with our offering of open source databases.\nYour Data Stays In Europe 🔗 All data is stored in the country of your chosen zone, fully GDPR-compliant. DBaaS is available across European zones.\nAutomate Everything 🔗 Easily automate everything with our simple web portal, CLI, API or tools like Terraform.\n99.99% Uptime SLA 🔗 All DBaaS (cluster) offerings come with an uptime SLA of 99.99%.\n","categories":"","description":"","excerpt":" Exoscale DBaaS Features 🔗 Daily Backups included 🔗 Backups are done …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/exoscale-dbaas/content/features/","tags":"","title":"Exoscale DBaaS Features"},{"body":" Configuration Differences 🔗 What a difference a configuration make :).\nThe significant change in this new IT world is that well-trained and practiced processes are outdated and useless. The imperative was the old world, you defined step-by-step guidance, and you also executed or overseen the execution of those configuration steps. The declarative way is to determine the desired state and an intelligent system conducts and supervises the configuration and operation steps.\n","categories":"","description":"","excerpt":" Configuration Differences 🔗 What a difference a configuration make …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-details/content/imperative-vs-declarative/","tags":"","title":"Imperative vs Declarative"},{"body":" In the left sidebar, click on the upward arrow symbol (import icon) to import the designs into Meshery. In the modal that appears: Enter a name for your design in the “Design File Name” field (e.g. mysql-deployment). Select Kubernetes Manifest from the “Design Type” dropdown menu. Figure: Import modal\nChoose File Upload for the upload method, and select the file you just downloaded. Then, click on Import. Figure: Import mysql-deployment\nUnder the “Designs” tab, you will see the successfully imported mysql-deployment design.\nClicking on the names of the designs on the Designs tab displays the visual representations of the various Kubernetes resources and their relationships on the canvas.\nFigure: Imported designs on canvas\nNow, follow the same steps to import the wordpress-deployment file. Figure: wordpress-deployment\nMerging the Designs 🔗 Next, you will combine the WordPress and MySQL designs into a single design file. By merging these designs, you can manage and deploy both resources together.\nTo merge the MySQL deployment design with the WordPress deployment design:\nClick and drag the mysql-deployment design from the left panel and drop it onto the design canvas of the wordpress-deployment. Figure: drag and drop design\nThis action will open a merge modal asking if you want to merge the design, Click on Merge. Figure: merge modal\nClick on Save As and enter wordpress-mysql-deployment as the new file name. Figure: save design\n","categories":"","description":"Here we will import the design files in Meshery Playground and learn how to merge designs.","excerpt":"Here we will import the design files in Meshery Playground and learn …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/deploying-wordpress-and-mysql-with-persistent-volumes-with-meshery/sql/import-designs/","tags":"","title":"Import Design Files and Merge"},{"body":" Ingress 🔗 We already know the LoadBalancer service in Kubernetes from the SKS starter course. However, LoadBalancer works only on network layer 4; it can not distinguish between different hostnames and paths and cannot terminate SSL traffic. These additional capabilities are provided in Kubernetes by the Ingress service.\nThe Ingress uses a reverse proxy, like Nginx, to handle network layer 7 balancing in the Kubernetes cluster and the often needed additional, multiple routing paths. If there are advanced routing demands, the Ingress service can provide and handle them.\nIngress 🔗 Video: Ingress Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Ingress 🔗 We already know the LoadBalancer service in Kubernetes from …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/ingress/","tags":"","title":"Ingress"},{"body":"","categories":"","description":"Welcome to our introductory course on Sustainability at Exoscale. Delving into the core of sustainability, this course navigates the essentials required to comprehend how sustainability initiatives align with broader corporate sustainability goals, particularly within the cloud computing sphere. By understanding the critical aspects of environmental responsibility, social equity, and economic viability, participants will gain a solid foundation in corporate sustainability practices and the impact of regulations such as the EU Corporate Sustainability Reporting Directive (CSRD). This course is designed to equip you with knowledge and understanding with no prerequisites, making it suitable for anyone keen to explore the intersection of sustainability and technology.","excerpt":"Welcome to our introductory course on Sustainability at Exoscale. …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/","tags":"","title":"Intro Sustainability"},{"body":" Overview 🔗 Stakeholder Engagement Environmental Responsibility Social Equity and Ethics Economic Viability Stakeholder Engagement 🔗 Sustainable Businesses Stakeholder Involvement Stakeholder Value A sustainable business doesn’t just involve stakeholders, it values them. It actively engages them in dialogue and decision-making processes related to sustainability issues that affect them. This includes investors, employees, customers, suppliers, communities, and regulators, recognizing their integral role in the sustainability journey.\nEnvironmental Responsibility 🔗 Energy Efficiency Resource Conservation Environmental Impact Companies adopt practices that promote environmental stewardship, conservation, and sustainability, aiming to reduce their carbon footprint, enhance energy efficiency, minimize waste, and conserve resources.\nSocial Equity and Ethics 🔗 Diversity and Inclusion Community Engagement Ethical Treatment Fostering fair treatment, respect, and ethical practices for all stakeholders, including employees, customers, communities, and suppliers. This involves addressing issues like labor rights, fair trade, diversity, inclusion, and community engagement.\nEconomic Viability 🔗 Sustainability Long-term Value Economic Health While sustainability often emphasizes environmental and social components, economic viability remains crucial. Businesses need to be profitable to sustain their operations and implement sustainable practices. However, the focus is on long-term value creation rather than short-term gains.\n","categories":"","description":"","excerpt":" Overview 🔗 Stakeholder Engagement Environmental Responsibility Social …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/corporate-sustainability/content/key-principles/","tags":"","title":"Key Principles"},{"body":" Kubernetes in Plain English 🔗 Containerization is this trend that’s taking over the world to allow people to run all kinds of different applications in a variety of different environments. When they do that, they need an orchestration solution in order to keep track of all of those containers and schedule them and orchestrate them. Kubernetes is an increasingly popular way to do that.\nDan Kohn, former executive director of the CNCF\n","categories":"","description":"","excerpt":" Kubernetes in Plain English 🔗 Containerization is this trend that’s …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-kubernetes/content/kubernetes-plain-english/","tags":"","title":"Kubernetes in Plain English"},{"body":" Configuration resources 🔗 There are 2 type of configuration resources:\na ConfigMap is used to manage configuration data a Secret is used to manage sensitive data ConfigMap and Secret contain one or more key / value pairs. Each resource can be consumed in a Pod\nvia a mount point in a container’s filesystem via environment variables ConfigMap 🔗 Mounting a ConfigMap into a container’s filesystem 🔗 The following nginx configuration file defines a server which listens on port 80 and forwards all the requests to the port 5000 of the api service.\nuser www-data; worker_processes 4; pid /run/nginx.pid; events { worker_connections 768; } http { server { listen *:80; location / { proxy_pass http://api:5000; } } } The following specification defines ConfigMap which provides the nginx configuration file within the cluster, allowing Pods to use it:\napiVersion: v1 kind: ConfigMap metadata: name: nginx-config data: nginx.conf: | user www-data; worker_processes 4; pid /run/nginx.pid; events { worker_connections 768; } http { server { listen *:80; location / { proxy_pass http://api:5000; } } } As this ConfigMap contains a file, we can use it in a Pod mounting it into the container’s filesystem. The following specification defines a Pod with a single container running the nginx:1.20 image.\napiVersion: v1 kind: Pod metadata: name: proxy spec: containers: - name: proxy image: nginx:1.20 ports: - containerPort: 80 To give access to the nginx configuration file existing in the nginx-config ConfigMap:\nwe define a volume based on the ConfigMap we mount this volume in the container’s filesystem The new version of the Pod specification is as follows:\napiVersion: v1 kind: Pod metadata: name: proxy spec: containers: - name: proxy image: nginx:1.20 ports: - containerPort: 80 volumeMounts: # (1) - name: config mountPath: \"/etc/nginx/\" volumes: # (2) - name: config configMap: name: nginx-config Volume mounted in the container’s filesystem\nDefinition of the volume allowing to consume the content of the ConfigMap\nInjecting a ConfigMap via environment variables 🔗 The following specification defines a ConfigMap containing 2 key / value pairs:\nenv: production log_level: warning apiVersion: v1 kind: ConfigMap metadata: name: app-cfg namespace: default data: env: production log_level: WARNING The content of this ConfigMap can be injected in the container’s environment variables as follows:\napiVersion: v1 kind: Pod metadata: name: api spec: containers: - name: api image: org/api:1.2 env: - name: LOG_LEVEL # (1) valueFrom: configMapKeyRef: name: app-cfg key: log_level - name: ENV # (2) valueFrom: configMapKeyRef: name: app-cfg key: env Container’s LOG_LEVEL env var with value retrieved from the ConfigMap’s log_level key\nContainer’s ENV env var with value retrieved from the ConfigMap’s env key\nSecret 🔗 Role and usage 🔗 A Secret is used to manage sensitive data.\nThere are different types of Secret including:\nGeneric Docker-registry TLS The content of a Secret is not encrypted in etcd by default but can be encrypted using an EncryptionConfiguration resource\nSecret of type generic 🔗 Let’s consider a dummy connection string to connect to a MongoDB database: mongodb://admin:45fe3efa@mgserv1.org/mgmt. For this sensitive piece of information to be used in a Pod we first encode it in base64:\n$ echo -n \"mongodb://admin:45fe3efa@mgserv1.org/mgmt\" | base64 bW9uZ29kYjovL2FkbWluOjQ1ZmUzZWZhQG1nc2VydjEub3JnL21nbXQ= Then we create a Secret defining a key / value pair using the resulting encoded value:\napiVersion: v1 kind: Secret metadata: name: mongo data: mongo: bW9uZ29kYjovL2FkbWluOjQ1ZmUzZWZhQG1nc2VydjEub3JnL21nbXQ= To consume the content of the Secret in a Pod, we can either mount it in the container’s filesystem as a volume, or we can inject it in environment variables:\nMounting the Secret’s content as a Volume apiVersion: v1 kind: Pod metadata: name: api spec: containers: - name: api image: api:1.2 volumeMounts: - name: creds mountPath: /etc/creds volumes: - name: creds secret: secretName: mongo Injecting the Secret’s content in environment variables apiVersion: v1 kind: Pod metadata: name: api spec: containers: - name: api image: api:1.2 env: - name: MONGO_URL valueFrom: secretKeyRef: name: mongo key: mongo Using a Secret in the VotingApp 🔗 The previous exercise left the app with the following components:\nWe will now add a Secret to store the password required to connect to the PostgreSQL database.\n","categories":"","description":"","excerpt":" Configuration resources 🔗 There are 2 type of configuration …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/configuration/content/learn/","tags":"","title":"Learn"},{"body":" What is a Deployment ? 🔗 A Deployment is used to manage one or more Pods with the same specification. It manages the Pod lifecycle:\ncreation and deletion scaling progressive deployment (Rollout) and rollback There are an additional level of abstraction:\nDeployment: creates and updates applications ReplicaSet: ensures the number of Pod replicas Pod: smallest workload unit containing one or more containers From Pod to Deployment 🔗 Pod are rarely created directly, they are usually managed by higher level resources such as:\nDeployment DaemonSet Job CronJob StatefulSet The choice of resource depends on the application’s use case. For example, a Deployment manages the lifecycle of multiple identical Pods and ensures that a specified number of Pods are always running.\nThe following specification defines a Pod with a unique container running the nginx:1.20 image:\napiVersion: v1 kind: Pod metadata: name: www labels: app: www spec: containers: - name: www image: nginx:1.20 ports: - containerPort: 80 If we want to manage 3 identical Pods having the previous specification, we can specify a Deployment as follows:\napiVersion: apps/v1 kind: Deployment metadata: name: www spec: replicas: 3 selector: matchLabels: app: www template: metadata: labels: app: www spec: containers: - name: www image: nginx:1.20 ports: - containerPort: 80 We can create this Deployment:\nkubectl apply -f deploy.yaml If a Pod is deleted, a new one is automatically created to replace it, ensuring that 3 Pods (replicas) are always running as specified\nDeployment\nIf we need to update an application, in order to use a newer image for the containers, we can easily do so modifying the Deployment specification and applying the new version.\nBelow is the updated deploy.yaml specification, the nginx image was changed from nginx:1.20 to nginx:1.22:\napiVersion: apps/v1 kind: Deployment metadata: name: www spec: replicas: 3 selector: matchLabels: app: www template: metadata: labels: app: www spec: containers: - name: www image: nginx:1.22 ports: - containerPort: 80 We can send this new version of the specification to Kubernetes:\nkubectl apply -f deploy.yaml The Pods running nginx:1.20 will gradually be replaced with Pods running nginx:1.22\nBase commands 🔗 Creating a Deployment from a YAML file kubectl apply -f deploy.yaml Creating a Deployment using an imperative command kubectl create deploy www --image=nginx:1.22 --replicas=3 Listing existing Deployments kubectl get deploy Getting the Deployment’s properties kubectl get deploy www -o yaml Getting the Deployment’s main properties kubectl describe deploy www Changing the number of Pods managed by a Deployment (scaling operation) kubectl scale deploy www --replicas=5 Removing a Deployment kubectl delete deploy www Using Deployments in the VotingApp 🔗 The previous exercise left the app with the following components:\nWe will now use Deployments to manage Pods instead of running the Pods directly. The Pods created by these Deployments will be exposed by the same Service.\n","categories":"","description":"","excerpt":" What is a Deployment ? 🔗 A Deployment is used to manage one or more …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/deployment/content/learn/","tags":"","title":"Learn"},{"body":" What is Helm ? 🔗 Helm is a Package Manager for Kubernetes, it allows defining, installing, upgrading and deleting applications.\nA Helm Chart is an application packaged with Helm, it has a script file/folder structure and uses a templating language.\nThe Helm Client is a binary used to manipulate application packaged with Helm.\nThe complete documentation can be found at https://helm.sh\nArtifactHub 🔗 The ArtifactHub is the place where many applications are distributed and ready to be deployed in a Kubernetes cluster.\nInstalling a chart 🔗 An application installed from a Chart is called a release.\nInstalling a chart from a helm repository # Adding a repository helm repo add grafana https://grafana.github.io/helm-charts # Installing a chart (= creating a release from this chart) helm install my-grafana grafana/grafana --version 7.3.8 Installing a chart from an OCI registry helm install my-redis oci://registry-1.docker.io/bitnamicharts/redis Base commands 🔗 Creating / upgrading a release helm install my-release CHART_URL -f values.test.yaml or helm upgrade --install my-release CHART_URL -f values.test.yaml Getting information about a given release helm get all/hooks/manifest/notes/values my-release Listing existing releases helm list Removing a release helm delete my-release Creating a chart to package an application 🔗 The following command creates a sample Chart containing resources to deploy a NGinx server:\nhelm create my-app The folder structure created is as follows:\n$ tree my-app my-app ├── Chart.yaml ├── charts ├── templates │ ├── NOTES.txt │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── ingress.yaml │ ├── service.yaml │ ├── serviceaccount.yaml │ └── tests │ └── test-connection.yaml └── values.yaml Chart.yaml contains the application metadata and the dependencies list charts is a folder used to store the dependent charts NOTES.txt provides the post install instructions to the user _helpers.tpl contains functions and variables to simplify the templating the YAML files in the templates’ folder contain specifications with templating code values.yaml defines the configuration values for the application Packaging the VotingApp with Helm 🔗 In the exercises we created many resources and put them all in a single folder. We will now use Helm to package the VotingApp to ease the distribution and deployment of this application.\n","categories":"","description":"","excerpt":" What is Helm ? 🔗 Helm is a Package Manager for Kubernetes, it allows …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/helm/content/learn/","tags":"","title":"Learn"},{"body":" What is an Ingress ? 🔗 An Ingress resource allows exposing Services outside the cluster. There are other solutions to do so, such as:\na NodePort Service a Load Balancer Service Use case examples:\nHTTP routing via domain name or query content TLS termination An Ingress resource is used by an Ingress Controller\nIngress controller 🔗 An Ingress controller is a reverse proxy configured with Ingress resources.\nAmong the existing solutions:\nNginx Traefik HAProxy Kong Contour Gloo Routing using domain name 🔗 The following specification defines an Ingress resource which ensures the traffic arriving at api.example.com is forwarded to the api Service on port 80:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: myapp spec: ingressClassName: nginx rules: - host: api.example.com http: paths: - path: / pathType: Prefix backend: service: name: api port: number: 80 Exposing the VotingApp with an Ingress resource 🔗 The previous exercise left the app with the following components:\nWe will now use an Ingress Controller (based on Traefik) and an Ingress resource to make the application available to the outside world:\n","categories":"","description":"","excerpt":" What is an Ingress ? 🔗 An Ingress resource allows exposing Services …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/ingress/content/learn/","tags":"","title":"Learn"},{"body":" What is a Job ? 🔗 A Job is used to launch batch-jobs. It can either:\nlaunch a single Pod launch several Pods in parallel or in sequence Sample specification 🔗 The following specification defines a simple Job which runs 3 Pods sequentially, each Pod waits 10 seconds and displays a simple text on its standard output:\napiVersion: batch/v1 kind: Job metadata: name: hello spec: completions: 3 # (1) parallelism: 1 # (2) template: spec: restartPolicy: OnFailure containers: - name: hello image: alpine args: - /bin/sh - -c - sleep 10; echo Hello from kube The job will terminate when 3 Pods have reached the “Completed” status\nPods run one after the other\nWhat is a CronJob ? 🔗 A CronJob is used to run Jobs at regular intervals. It is kind of similar to Linux crontab.\nSeveral use case examples:\ndatabase backups data processing sending marketing emails Sample specification 🔗 The following specification defines a CronJob which create a Job (based on the previous spec) every minute:\napiVersion: batch/v1 kind: CronJob metadata: name: hello spec: schedule: \"* * * * *\" jobTemplate: spec: template: spec: restartPolicy: OnFailure containers: - name: hello image: alpine args: - /bin/sh - -c - sleep 10; echo Hello from kube Using a CronJob with the VotingApp 🔗 The previous exercise left the app with the following components:\nWe will now add a CronJob in charge of creating a couple of dummy votes every minute:\n","categories":"","description":"","excerpt":" What is a Job ? 🔗 A Job is used to launch batch-jobs. It can either: …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/job/content/learn/","tags":"","title":"Learn"},{"body":" What is a Pod ? 🔗 A Pod is the smallest unit allowing to run workload. It is a group of containers sharing a network stack and storage. The specification of a Pod is defined in an YAML file.\nExample of a simple Pod’s specification: 🔗 The following specification defines a Pod which running a simple container based on the stefanprodan/podinfo image.\napiVersion: v1 kind: Pod metadata: name: podinfo spec: containers: - name: podinfo image: stefanprodan/podinfo Many additional properties are needed for this Pod to run securely in a production environment, such as:\nresource definition to control the amount of RAM / CPU the containers can use readinessProbe to know when a container is ready to receive traffic livenessProbe to check the health of a container securityContext to control what the container’s process can and cannot do from a security perspective Below is the example of a more production ready Pod’s specification:\napiVersion: v1 kind: Pod metadata: name: podinfo labels: app: podinfo spec: containers: - image: stefanprodan/podinfo:6.1.0 name: podinfo resources: requests: cpu: 50m memory: 64Mi limits: cpu: 50m memory: 64Mi livenessProbe: httpGet: path: /healthz port: 9898 initialDelaySeconds: 3 periodSeconds: 3 readinessProbe: httpGet: path: /readyz port: 9898 initialDelaySeconds: 3 periodSeconds: 3 securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 10000 runAsNonRoot: true seccompProfile: type: RuntimeDefault capabilities: drop: - ALL … Base commands 🔗 Creating a Pod using a specification kubectl apply -f pod.yaml Creating a Pod using an imperative command kubectl run podinfo --image=stefanprodan/podinfo Listing the existing Pods kubectl get pods Getting a Pod’s details kubectl get pods podinfo -o yaml Getting the main properties of a Pod kubectl describe pod podinfo Getting the logs of a Pod’s container kubectl logs podinfo Running an Interactive shell in a Pod’s container kubectl exec -ti podinfo -- /bin/sh Deleting a Pod kubectl delete pod podinfo Running the VotingApp inside Pods 🔗 As presented above, the VotingApp is a microservice application with the following architecture\nYou will now run a Pod for each of the microservice\n","categories":"","description":"","excerpt":" What is a Pod ? 🔗 A Pod is the smallest unit allowing to run …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/pod/content/learn/","tags":"","title":"Learn"},{"body":" Storage in Kubernetes 🔗 The following items are related to data storage in Kubernetes:\nVolume : shares data between containers in a Pod PersistentVolume (PV) : storage resource PersistentVolumeClaim (PVC) : request for storage StorageClass : dynamique storage provisioning StatefulSet : used to manage stateful applications PersistentVolume / PersistentVolumeClaim 🔗 A PersistentVolume provides the storage, it can be provisioned either statically (by an administrator) or dynamically (by a StorageClass).\nA PersistentVolumeClaim is a request for storage, it decouples an application from the storage solution and allows storage to be consumed by a Pod\nStorageClass 🔗 A StorageClass is a resource which allows PV to be dynamically created. It is referenced within the specification of a PVC.\nProvisioning with a StorageClass 🔗 The local cluster, based on k3s, used in this section has a default storage class based on the local-path provisioner. It allows to create storage on the VM filesystem. The specification of this StorageClass is the following one:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-path provisioner: rancher.io/local-path reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer ... This StorageClass should not be used in a production environment.\nIn order to use this StorageClass, we define a PVC (PersistentVolumeClaim) referencing the StorageClass:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim spec: accessModes: - ReadWriteOnce storageClassName: local-path resources: requests: storage: 1Gi Next we reference this PVC in our application:\napiVersion: apps/v1 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password volumeMounts: - name: mysql-storage mountPath: /var/lib/mysql volumes: - name: mysql-storage persistentVolumeClaim: claimName: mysql-pv-claim Creating the Deployment and the PVC triggers the creation of a PV of 1G in size.\nUsing Storage in the VotingApp 🔗 The previous exercise left the app with the following components:\nWe will now use the existing StorageClass and define PVCs to request storage for both databases used in the VotingApp:\n","categories":"","description":"","excerpt":" Storage in Kubernetes 🔗 The following items are related to data …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/storage/content/learn/","tags":"","title":"Learn"},{"body":" Managed Databases 🔗 Looking at managed databases specifically, we see the benefits of simplifying the tasks associated with provisioning and maintaining a database. However, it is still likely that you need some level of experience working with databases to interact with them as you build and scale your app.\nSuppose we do such evaluations in the context of IT services and factor in moving traditional data center services to the cloud. In that case, two trendy acronyms pop up immediately, CAPEX and OPEX.\nPlease don’t put your data center glasses away too quickly. Instead, let us dig into more details about infrastructure and operations costs and look at the table below.\nRunning modern applications is a demanding business. Besides the always needed machine and storage components, more sophisticated elements like Kubernetes (k8s) clusters/nodes, load balancers, software licenses, and last but not least, required connectivity to those applications in the form of ingress and egress.\nOwning all of it is a CAPEX game; taking care of it is an OPEX game, very demanding businesses, both regarding time and expertise. The time you miss following up on business opportunities is potentially lost business, valid for all who are not in the business of running data centers.\n","categories":"","description":"","excerpt":" Managed Databases 🔗 Looking at managed databases specifically, we see …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/managed-databases/content/managed-db/","tags":"","title":"Managed Databases"},{"body":" Container Orchestration Ecosystem 🔗 While Kubernetes solves the orchestration part, additional components are needed to deploy and run containers smoothly in production.\nContainer lifecycle management as well as monitoring, backup, CI/CD, security, logging and registration are all activities needed in container orchestration.\nK8s comes with multiple options and plugins for each layer. Each team has its preferences for ecosystem decisions.\nManaged Container Orchestration Ecosystems 🔗 SKS\nExoscale’s Scalable Kubernetes Service (SKS) built into the platform’s core based on vanilla Kubernetes. Easily upgradeable to go along with the progress of the Kubernetes project. A fully open ecosystem with the flexibility to choose all additional pluggable solutions based on your needs. Start a production Kubernetes cluster in seconds. Run from the portal, CLI, API and configuration management tools. The managed service scope is the Kubernetes Cluster, precisely the total lifecycle management of the control plane and the nodes.\nAPPUiO\nVSHN’s APPUiO is the leading Kubernetes based Container Platform for the design, development and operation of applications. Based on reliable Open Source concepts, such as Docker and Kubernetes, APPUiO supports the DevOps approach. Development, deployment and operation processes are accelerated through automation and self-service. The cooperation between software developers and business organization is also improved. The managed service scope extends into parts of the ecosystem of pluggable Kubernetes solutions, increasing user comfort and reducing user flexibility in individual tool selection.\nCK8s\nElastisys’ Compliant Kubernetes (CK8s) is a security-hardened, CNCF certified Kubernetes distribution that allows organizations to enjoy the benefits of Kubernetes while fulfilling regulatory requirements – all the way from development to deployment to operations and audits. Compliant Kubernetes comes pre-configured with CNCF approved open source projects that make life easier at audits and enforces compliance policies for your workloads, helping fulfil regulatory standards. You can rely on pre-configured templates and best practices and define your policies to achieve your regulatory goals. The managed service scope is fully managed.\n","categories":"","description":"","excerpt":" Container Orchestration Ecosystem 🔗 While Kubernetes solves the …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-managed-kubernetes/content/managed-ecosystem/","tags":"","title":"Managed Ecosystems"},{"body":" ","categories":"","description":"Learn how to configure your Kubernetes clusters and manage the lifecycle of your workloads","excerpt":"Learn how to configure your Kubernetes clusters and manage the …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/","tags":"","title":"Mastering Kubernetes for Engineers"},{"body":" New Ideas \u0026 Concepts 🔗 As often in IT, great “new” ideas and concepts are recycled or borrowed from others. So it happened that the shipping industry was a big inspiration for optimizing IT infrastructure operations more than two decades ago.\nThe concept of a container to standardize the packing of goods, make them universal to handle and transport on different means of transportation to improve efficiency and reduce the transportation costs was a real success story for the transportation industry.\nReplication of success is always desirable and sparked a new way of operating IT infrastructures indifferent of application on development, testing, or production.\nContainer Technology in the shipping industry Instead of thinking up a separate way of shipping for each product, placing the goods in steel containers designed for pickup by the crane on the dock and fit into the ship is a more efficient way to do transport at scale.\n","categories":"","description":"","excerpt":" New Ideas \u0026 Concepts 🔗 As often in IT, great “new” ideas and concepts …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-containers/content/new-idea-concepts/","tags":"","title":"New Ideas \u0026 Concepts"},{"body":" Packaging \u0026 Pricing 🔗 ","categories":"","description":"","excerpt":" Packaging \u0026 Pricing 🔗 ","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-exoscale-sks/content/packaging-and-pricing/","tags":"","title":"Packaging \u0026 Pricing"},{"body":" PODs 🔗 Pods are the basic building blocks to run containers inside of Kubernetes. Every Pod holds at least one container and controls the execution of that container. If all containers terminate, the Pod terminates too. Mounting storage, setting environment variables, and feed information into the container are all functions provided by the Pod.\nPods are the smallest deployable units of computing that you can create and manage in Kubernetes. Pods in a Kubernetes cluster are used in two main ways:\nPods that run a single container The “one-container-per-Pod” model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers directly.\nPods that run multiple containers that need to work together A Pod can encapsulate an application composed of multiple co-located containers tightly coupled and need to share resources. These co-located containers form a single cohesive unit of service—for example, one container serving data stored in a shared volume to the public. In contrast, a separate sidecar container refreshes or updates those files. The Pod wraps these containers, storage resources, and an ephemeral network identity together as a single unit.\n","categories":"","description":"","excerpt":" PODs 🔗 Pods are the basic building blocks to run containers inside of …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-building-blocks/content/pod/","tags":"","title":"PODs"},{"body":" Use Deployments in the VotingApp 🔗 In the votingapp directory, replace each Pod specification with a Deployment specification with a single replica. Name these Deployment files deploy-XXX.yaml where XXX is the name of the microservice (voteui, vote, …)\nDeploy the application defined by these specifications\nAccess the vote and result interfaces via NodePort Services\nDelete a Pod. What happens ?\nDelete the application\nSolution apiVersion: apps/v1 kind: Deployment metadata: labels: app: vote-ui name: vote-ui spec: replicas: 1 selector: matchLabels: app: vote-ui template: metadata: labels: app: vote-ui spec: containers: - image: voting/vote-ui:latest name: vote-ui apiVersion: apps/v1 kind: Deployment metadata: labels: app: vote name: vote spec: replicas: 1 selector: matchLabels: app: vote template: metadata: labels: app: vote spec: containers: - image: voting/vote:latest name: vote apiVersion: apps/v1 kind: Deployment metadata: labels: app: redis name: redis spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - image: redis:7.0.8-alpine3.17 name: redis apiVersion: apps/v1 kind: Deployment metadata: labels: app: worker name: worker spec: replicas: 1 selector: matchLabels: app: worker template: metadata: labels: app: worker spec: containers: - image: voting/worker:latest name: worker apiVersion: apps/v1 kind: Deployment metadata: labels: app: db name: db spec: replicas: 1 selector: matchLabels: app: db template: metadata: labels: app: db spec: containers: - image: postgres:15.1-alpine3.17 name: postgres env: - name: POSTGRES_PASSWORD value: postgres ports: - containerPort: 5432 name: postgres apiVersion: apps/v1 kind: Deployment metadata: labels: app: result name: result spec: replicas: 1 selector: matchLabels: app: result template: metadata: labels: app: result spec: containers: - image: voting/result:latest name: result apiVersion: apps/v1 kind: Deployment metadata: labels: app: result-ui name: result-ui spec: replicas: 1 selector: matchLabels: app: result-ui template: metadata: labels: app: result-ui spec: containers: - image: voting/result-ui:latest name: result-ui Deploy the application with the following command from the votingapp directory: kubectl apply -f . As before, using the IP address of one of the cluster nodes, you can access the vote and result interfaces via ports 31000 and 31001 respectively. Each Pod is now managed by a Deployment. If a Pod is deleted, another Pod is automatically created to replace it. List of Pods:\n$ kubectl get po NAME READY STATUS RESTARTS AGE db-647c8f548b-j7z79 1/1 Running 0 3m35s redis-6f95f75d56-7gwjz 1/1 Running 0 3m35s result-7f897b4d58-qqtt4 1/1 Running 0 3m35s result-ui-5cdd74d999-q5tx7 1/1 Running 0 3m34s vote-6c847fd45-fpprh 1/1 Running 0 3m35s vote-ui-74849dd9b4-gwcq9 1/1 Running 0 3m35s worker-8655654586-k44vw 1/1 Running 0 3m35s Deleting a Pod (e.g., worker):\n$ kubectl delete po worker-8655654586-k44vw pod \"worker-8655654586-k44vw\" deleted A new Pod is automatically launched to replace the one that was deleted.\n$ kubectl get po NAME READY STATUS RESTARTS AGE db-647c8f548b-j7z79 1/1 Running 0 5m15s redis-6f95f75d56-7gwjz 1/1 Running 0 5m15s result-7f897b4d58-qqtt4 1/1 Running 0 5m15s result-ui-5cdd74d999-q5tx7 1/1 Running 0 5m14s vote-6c847fd45-fpprh 1/1 Running 0 5m15s vote-ui-74849dd9b4-gwcq9 1/1 Running 0 5m15s worker-8655654586-mmzgh 1/1 Running 0 4s A Deployment ensures that Pods are always present. If we had deleted a Pod that was not managed by a Deployment (a Naked Pod), no new Pod would be automatically created to replace it.\nWe delete the application with the following command: kubectl delete -f . ","categories":"","description":"","excerpt":" Use Deployments in the VotingApp 🔗 In the votingapp directory, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/deployment/content/practice/","tags":"","title":"Practice"},{"body":" Package and distribute the VotingApp with Helm 🔗 In this exercise, you will package the VotingApp application into a Helm Chart.\nInstall the Helm Client If you do not already have the Helm client installed, you can do so using the following command:\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Creating a Helm Chart Use the following command to create a Chart named vote.\nhelm create vote By default, it includes the following elements:\na Chart.yaml file that defines the project metadata a template for creating a Deployment that manages a single Pod (based on nginx) a template for creating a Service to expose this Pod within the cluster a template for creating an Ingress resource to expose the service externally a values.yaml file used to substitute placeholders in the templates with dynamic values a NOTES.txt file that provides information on creating the release and during updates You can view its contents using the tree command (install it first with “sudo apt install tree”):\n$ tree vote/ vote/ ├── Chart.yaml ├── charts ├── templates │ ├── NOTES.txt │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── hpa.yaml │ ├── ingress.yaml │ ├── service.yaml │ ├── serviceaccount.yaml │ └── tests │ └── test-connection.yaml └── values.yaml First, in the templates directory, delete the yaml files, the NOTES.txt file, and the test directory.\nNext, copy all the manifests from VotingApp into the templates directory.\nAlso, delete the contents of the values.yaml file but do not delete the file itself.\nThe vote directory will then have the following contents:\n$ tree vote/ vote/ ├── Chart.yaml ├── charts ├── templates │ ├── _helpers.tpl │ ├── cronjob.yaml │ ├── deploy-db.yaml │ ├── deploy-redis.yaml │ ├── deploy-result.yaml │ ├── deploy-resultui.yaml │ ├── deploy-vote.yaml │ ├── deploy-voteui.yaml │ ├── deploy-worker.yaml │ ├── ingress.yaml │ ├── pvc-db.yaml │ ├── pvc-redis.yaml │ ├── secret-db.yaml │ ├── svc-db.yaml │ ├── svc-redis.yaml │ ├── svc-result.yaml │ ├── svc-resultui.yaml │ ├── svc-vote.yaml │ └── svc-voteui.yaml └── values.yaml Deploying the Application From the vote directory, deploy the application using the following command:\nhelm upgrade --install vote . You should see a result similar to the one below:\nRelease \"vote\" does not exist. Installing it now. NAME: vote LAST DEPLOYED: Tue Feb 13 10:27:48 2024 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None Then, check that all the Pods of the VotingApp are in the Running status:\n$ kubectl get po NAME READY STATUS RESTARTS AGE worker-56b544777-nt6wf 1/1 Running 0 45s result-ui-5cdd74d999-bpwkj 1/1 Running 0 45s vote-6c847fd45-jqvhr 1/1 Running 0 45s vote-ui-74849dd9b4-gbmbp 1/1 Running 0 45s redis-5c4f4598f5-n8kxf 1/1 Running 0 45s result-854cd4779c-w4jhm 1/1 Running 0 45s db-54d89ccb97-nxz7v 1/1 Running 0 45s seed-28463608-dwgst 0/1 Completed 0 34s Ensure that the application is accessible at http://vote.votingapp.com / http://result.votingapp.com (or at http://vote.IP_OF_YOUR_VM.nip.io / http://result.IP_OF_YOUR_VM.nip.io if you are using the nip.io approach)\nUsing the Helm templating Image Tags You will ensure that image tags are no longer based on latest (which is actually a very bad practice) but on a value specified in the values.yaml file. First, add the following content to values.yaml:\nregistry: voting voteui: tag: v1.0.19 vote: tag: v1.0.13 worker: tag: v1.0.15 result: tag: v1.0.16 resultui: tag: v1.0.15 tools: tag: v1.0.4 Then, modify the specifications of the Deployments voteui, vote, worker, result, and resultui so that the image name is generated from the values of the registry property and the corresponding tag for the microservice. For example, the specification for the voteui Deployment will be modified as follows:\napiVersion: apps/v1 kind: Deployment metadata: labels: app: vote-ui name: vote-ui spec: replicas: 1 selector: matchLabels: app: vote-ui template: metadata: labels: app: vote-ui spec: containers: - image: {{ .Values.registry }}/vote-ui:{{ .Values.voteui.tag }} name: vote-ui Regular Vote Generation You will now configure the creation of 5 dummy votes every 2 minutes. Add the following content to the end of the values.yaml file:\n# Add dummy votes on a regular basis seed: enabled schedule: \"*/2 * * * *\" number_of_votes: 5 Then, modify the cronjob.yaml specification as follows:\n{{ if .Values.seed }}{{ if eq .Values.seed \"enabled\" }} apiVersion: batch/v1 kind: CronJob metadata: name: seed spec: schedule: \"{{ .Values.schedule }}\" jobTemplate: metadata: name: seed spec: template: spec: containers: - image: voting/tools:{{ .Values.tools.tag }} name: seed env: - name: NUMBER_OF_VOTES value: \"{{ .Values.number_of_votes }}\" imagePullPolicy: Always restartPolicy: OnFailure {{ end }}{{ end }} This specification uses Helm templating language:\nThe cronjob is created only if the seed property exists and is equal to “enabled” in the values file The schedule for the cronjob is retrieved from the schedule property in the values file The number of votes is defined in the NUMBER_OF_VOTES environment variable, with the value also retrieved from the values file Update the application using the same command used for creation:\nhelm upgrade --install vote . The values.yaml file is used by default\nThe second revision of the application will then be created:\nRelease \"vote\" has been upgraded. Happy Helming! NAME: votingapp LAST DEPLOYED: Tue Feb 13 11:05:21 2024 NAMESPACE: default STATUS: deployed REVISION: 2 TEST SUITE: None You will also see that the different Pods have been recreated to account for the changes made in their specifications:\n$ kubectl get po NAME READY STATUS RESTARTS AGE db-54d89ccb97-mp28m 1/1 Running 0 31m redis-5c4f4598f5-wgw2r 1/1 Running 0 31m seed-28463643-s2kzd 0/1 Completed 0 2m34s seed-28463644-n5b4w 0/1 Completed 0 94s seed-28463645-4kmqc 0/1 Completed 0 34s result-ui-76bdf59f99-qks6w 1/1 Running 0 13s vote-779875ff5b-t7drr 1/1 Running 0 13s vote-ui-695759b448-c7jgw 1/1 Running 0 13s result-66f5856c8b-xp9d6 1/1 Running 0 13s worker-6c5d7ddcf5-hprn7 1/1 Running 0 12s From the result interface, verify that 5 new votes are being created every 2 minutes.\nUninstalling the Application Use the following command to uninstall the application:\nhelm uninstall vote Distributing the Application A Helm Chart can easily be packaged and distributed to an OCI registry like DockerHub.\nFirst, use the following command to log in to Docker Hub (if you don’t have a Docker Hub account, you can create one for free in just a few minutes):\necho $DOCKERHUB_PASSWORD | helm registry login registry-1.docker.io -u $DOCKERHUB_USERNAME --password-stdin Then, create a package of the application (specifying version v0.0.1 for the application):\nhelm package --version v0.0.1 . You can then push this package to DockerHub:\nhelm push vote-v0.0.1.tgz oci://registry-1.docker.io/$DOCKERHUB_USERNAME The application is now available and ready to be retrieved from Docker Hub.\n","categories":"","description":"","excerpt":" Package and distribute the VotingApp with Helm 🔗 In this exercise, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/helm/content/practice/","tags":"","title":"Practice"},{"body":" Expose the VotingApp with an Ingress resource 🔗 Traefik Ingress Controller Since the cluster used in this section is based on k3s, it comes with Traefik Ingress Controller by default. This can be verified using the following command.\n$ kubectl get po -n kube-system | grep traefik helm-install-traefik-crd-vj4hz 0/1 Completed 0 56m helm-install-traefik-qg2g5 0/1 Completed 1 56m svclb-traefik-de21cf3d-vctd2 2/2 Running 0 56m traefik-57b79cf995-vf6qh 1/1 Running 0 56m Traefik is one implementation of the Kubernetes Ingress Controller. Other implementations exist including nginx, HAProxy, …\nGet the external IP of this load-balancer as you will need it in the next part.\n$ kubectl get service -n kube-system | grep traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik LoadBalancer 10.43.2.146 192.168.64.8 80:32667/TCP,443:32545/TCP 57m In this example, the external IP address is 192.168.64.8. This is the IP address of the Ubuntu VM you created using Multipass, in a previous step.\nIn a file named ingress.yaml, define the specification for an Ingress resource with the following characteristics: Name: vote ingressClassName: traefik All requests to the URL vote.votingapp.com should be redirected to port 80 of the vote-ui service All requests to the URL result.votingapp.com should be redirected to port 80 of the result-ui service Update your /etc/hosts file so that the URLs vote.votingapp.com and result.votingapp.com resolve to the IP address of the Ingress Controller (the one you get before)\nThen you should modify your /etc/hosts file to add the following entry:\n... IP_OF_YOUR_VM vote.votingapp.com result.votingapp.com If you do not have the necessary permissions to modify your /etc/hosts file, you can use the nip.io service and set the domain names vote.IP_OF_YOUR_VM.nip.io / result.IP_OF_YOUR_VM.nip.io instead of vote.votingapp.com / result.votingapp.com in your Ingress resource definition.\nDeploy the application and verify that the vote interface is available at http://vote.votingapp.com and the result interface is available at http://result.votingapp.com (or at http://vote.IP_OF_YOUR_VM.nip.io / http://result.IP_OF_YOUR_VM.nip.io if you are using the nip.io approach).\nDelete the application.\nSolution The specification for the Ingress resource is as follows: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: vote spec: ingressClassName: traefik rules: - host: vote.votingapp.com http: paths: - path: / pathType: Prefix backend: service: name: vote-ui port: number: 80 - host: result.votingapp.com http: paths: - path: / pathType: Prefix backend: service: name: result-ui port: number: 80 Deploy the application with the following command from the manifests directory: kubectl apply -f . You can then access the different interfaces using real domain names instead of a port number.\nDelete the application with the following command from the manifests directory: kubectl delete -f . ","categories":"","description":"","excerpt":" Expose the VotingApp with an Ingress resource 🔗 Traefik Ingress …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/ingress/content/practice/","tags":"","title":"Practice"},{"body":" Use a CronJob to create dummy votes 🔗 In a file named cronjob.yaml, define the specification for a CronJob resource with the following characteristics: Name: seed Schedule: “* * * * *” Contains a single container with the image voting/tools:latest and an environment variable NUMBER_OF_VOTES set to 10 Deploy the application and verify, from the resultui interface, that 10 new votes are created every minute.\nDelete the application.\nSolution The specification to define the seed CronJob is as follows:\napiVersion: batch/v1 kind: CronJob metadata: name: seed spec: schedule: \"* * * * *\" jobTemplate: metadata: name: seed spec: template: spec: containers: - image: voting/tools:latest name: seed env: - name: NUMBER_OF_VOTES value: \"10\" imagePullPolicy: Always restartPolicy: OnFailure Deploy the application with the following command from the manifests directory:\nkubectl apply -f . Using the IP address of one of the cluster nodes, you can access the vote and result interfaces via ports 31000 and 31001 respectively. If you observe the result interface for a few minutes, you will see that 10 new votes are created every minute.\nDelete the application with the following command from the manifests directory: kubectl delete -f . ","categories":"","description":"","excerpt":" Use a CronJob to create dummy votes 🔗 In a file named cronjob.yaml, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/job/content/practice/","tags":"","title":"Practice"},{"body":" Overview 🔗 Currently, 90% of our electricity comes from renewable energy sources. This is a significant accomplishment demonstrating our commitment to reducing our carbon footprint and sourcing energy from environmentally responsible sources.\nBenefits 🔗 Renewable energy refers to the energy generated from natural resources that can be replenished relatively quickly. These resources include sunlight, wind, water, geothermal heat, and biomass. Unlike finite fossil fuels, which will eventually run out, renewable energy sources are sustainable and can be used indefinitely without depleting the resource.\nRenewable energy technologies are becoming increasingly popular as the world seeks to reduce its reliance on fossil fuels and mitigate the effects of climate change. They are also becoming more affordable and efficient, making them a viable alternative to traditional energy sources.\nExamples of renewable energy technologies include solar panels, wind turbines, hydroelectric power plants, geothermal power plants, and biomass energy systems. These technologies generate electricity and heat without producing greenhouse gas emissions, which makes them a cleaner and more sustainable option for powering homes, businesses, and communities.\n","categories":"","description":"","excerpt":" Overview 🔗 Currently, 90% of our electricity comes from renewable …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/sustainable-cloud/content/renewable-energy/","tags":"","title":"Renewable Energy"},{"body":" Sample Application using Node.js 🔗 This sample app demonstration shows how traditional software deployment looks. A little JavaScript spins up a web server and runs the sample app on the server. It demonstrates how easy it is to run web-based applications, but still, you need to install and run Node.js beforehand. In addition, you have to use the proper versions of the software components in place and take care of all dependencies (runtime environments, libraries, …). Otherwise, the app will not run or will not run properly.\nThis is the source code of our sample app:\nconst express = require(\"express\") const app = express() const port = 3000 app.get('/', (req, res) =\u003e { res.send(\"Hello World!\") }) app.listen(port, () =\u003e { console.log(\"Example app listening at http://localhost:3000\") }) Take the code into action:\nnode app.js The application at work, displaying Hello World!:\n","categories":"","description":"","excerpt":" Sample Application using Node.js 🔗 This sample app demonstration …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/containers/content/sample-application/","tags":"","title":"Sample Application"},{"body":" Explained 🔗 Cloud services models refer to how cloud computing services are delivered to users. The three main cloud services models are:\nInfrastructure as a Service (IaaS):\nUsers access virtualized computing, networking, and storage resources as services, which are owned, managed, and maintained by the cloud provider. Users have complete control over the configuration and management of the virtualized resources.\nPlatform as a Service (PaaS):\nDevelopers access a complete development and deployment environment in the cloud, including application development frameworks, databases, middleware, and other tools needed to build, test, and deploy applications.\nSoftware as a Service (SaaS):\nUsers access software applications over the internet without the need for installation or maintenance. The application is hosted on the cloud provider’s servers and delivered to users via a web browser or mobile app.\nEach cloud services model offers different levels of control and flexibility, allowing organizations to choose the model that best suits their needs and requirements.\nService Model - SaaS 🔗 SaaS applications are typically hosted on the cloud provider’s servers and delivered to users via a web browser or mobile app. This allows organizations to quickly and easily deploy software solutions without costly hardware and IT support. SaaS applications are also highly scalable, allowing users to add or remove features as needed and pay only for what they use.\nOverall, SaaS is a cost-effective and flexible solution for organizations of all sizes, providing easy access to powerful software applications without complex installation, maintenance, or support.\nService Model - PaaS 🔗 With PaaS, developers can access various services, including application development frameworks, databases, middleware, and other tools needed to build, test, and deploy applications. PaaS also provides a scalable and flexible infrastructure that can easily accommodate changes in application demand without additional hardware or IT support.\nPaaS benefits organizations with limited IT resources, allowing developers to focus on application development without worrying about the underlying infrastructure. In addition, PaaS enables developers to collaborate more efficiently, as they can work on the same codebase and share resources in real-time.\nPaaS is a powerful and flexible application development and deployment solution, providing developers with the resources and tools they need to create and deliver applications quickly and efficiently.\nService Model - IaaS 🔗 At the same time, users have complete control over the configuration and management of the virtualized resources. With IaaS, users can quickly and easily provision computing resources, such as virtual machines, storage, and networking, without expensive hardware or IT support. In addition, IaaS is highly scalable, allowing users to add or remove resources as needed and pay only for what they use.\nIaaS is particularly useful for organizations that need to scale their infrastructure to quickly accommodate changes in demand or those that require a flexible and customizable infrastructure to support their unique business needs. IaaS enables users to focus on their core business rather than worrying about managing hardware and infrastructure.\n","categories":"","description":"","excerpt":" Explained 🔗 Cloud services models refer to how cloud computing …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud/content/service-models/","tags":"","title":"Service Models"},{"body":"","categories":"","description":"This SKS STARTER - Learning Path covers the entry topics of (managed) Kubernetes for a technical audience and conveys the benefits of containers and container orchestration for modern IT scenarios. It will help you learn how to begin with this new technology, use the associated terminology, understand the components and functions, and why these new technologies are so important.","excerpt":"This SKS STARTER - Learning Path covers the entry topics of (managed) …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/","tags":"","title":"SKS STARTER"},{"body":" Software Needed 🔗 Kubernetes 🔗 kubectl – for accessing clusters\nhttps://kubernetes.io/de/docs/tasks/tools/install-kubectl/\nLocal Test Cluster 🔗 minikube – tool to create a cluster on your local computer\nhttps://kubernetes.io/de/docs/setup/minikube/\nExoscale Cluster 🔗 exoscale-CLI or exoscale-UI – used to create HA-Clusters on Exoscale\nhttps://community.exoscale.com/documentation/compute/quick-start/\nK8s Software 🔗 Video: Kubernetes Software Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Software Needed 🔗 Kubernetes 🔗 kubectl – for accessing clusters …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/kubernetes/content/software-needed/","tags":"","title":"Software Needed"},{"body":" Framework 🔗 ISO 27001 🔗 Information Security Management specifies a set of requirements that helps organizations manage their assets’ security.\nISO 27001 is a globally recognized standard for information security management. It provides a framework for managing and protecting sensitive information assets like customer data, financial information, and intellectual property. The standard outlines a systematic approach to managing information security risks, including risk assessment, risk treatment, and continuous monitoring and improvement. Organizations that implement ISO 27001 can demonstrate to customers, partners, and regulators that they have a robust information security management system in place. The standard covers many security controls, including physical security, access controls, network security, and incident management.\nISO 27017 🔗 Information Security Controls for cloud services is a code of practice that provides guidelines on managing information security controls.\nISO 27017 is a standard that provides guidelines for cloud service providers on implementing adequate information security controls in their cloud environments. The standard is based on ISO 27001, which provides a framework for information security management, but it also includes additional rules and guidelines specific to cloud computing.\nISO 27017 covers a range of essential security controls for cloud service providers, including data segregation, access control, encryption, logging and monitoring, and incident management. It also includes guidance on managing risks related to third-party cloud services, such as software-as-a-service (SaaS) or infrastructure-as-a-service (IaaS) providers.\nBy implementing ISO 27017, cloud service providers can demonstrate to their customers that they have implemented adequate security controls to protect their data and systems in the cloud. It can also help cloud service providers comply with regulatory requirements related to data protection and privacy.\nISO 27018 🔗 Personally Identifiable Information (PII) protection is a code of practice that helps secure PII for the public cloud computing environment.\nISO 27018 is a standard that provides guidelines for protecting Personally Identifiable Information (PII) in public cloud environments. It is based on ISO 27001, which provides a framework for information security management, but it also includes additional controls and guidelines specific to protecting PII in the cloud.\nISO 27018 covers a range of security controls necessary for cloud service providers, including data protection, retention, portability, and transparency. It also includes guidance on managing risks related to third-party cloud services, such as software-as-a-service (SaaS) or infrastructure-as-a-service (IaaS) providers.\nBy implementing ISO 27018, cloud service providers can demonstrate to their customers that they have implemented adequate security controls to protect their PII in the cloud. It can also help cloud service providers comply with regulatory requirements related to data protection and privacy, such as the European Union’s General Data Protection Regulation (GDPR).\nSOC 2 🔗 Design, implement, and operate controls to meet security, availability, processing integrity, confidentiality, and privacy objectives.\nSOC 2 (Service Organization Control 2) is an audit report that assures the security, availability, processing integrity, confidentiality, and privacy of a service organization’s systems and data. It is conducted by an independent auditor based on the Trust Services Criteria established by the American Institute of Certified Public Accountants (AICPA).\nSOC 2 audits evaluate a service organization’s data security and privacy controls, including physical and logical access controls, network security, system monitoring and alerting, data backup and recovery, and incident management. The audit also assesses the effectiveness of the organization’s policies and procedures related to data management, including data classification, retention, and disposal.\nThe SOC 2 report provides a detailed description of the service organization’s controls and includes an opinion from the auditor on the effectiveness of those controls. Service organizations often use it to demonstrate to their customers that they have implemented adequate security and privacy controls and to provide assurance that their systems and data are protected.\nCSA STAR 🔗 Is a third-party independent, technology-neutral assessment and certification of the security of a cloud service provider.\nThe Cloud Security Alliance (CSA) Security, Trust, and Assurance Registry (STAR) is a program that provides a framework for cloud service providers to demonstrate their security and compliance capabilities. The program includes a registry of cloud service providers that have completed a self-assessment or an independent audit against the CSA’s Cloud Controls Matrix (CCM) and Consensus Assessments Initiative Questionnaire (CAIQ).\nThe CSA STAR program provides a comprehensive set of criteria for assessing the security and compliance of cloud service providers, including data protection, identity and access management, compliance, and risk management. It also provides tools and resources for cloud service providers to improve their security and compliance capabilities, such as CCM and CAIQ.\nBy participating in the CSA STAR program, cloud service providers can demonstrate to their customers and partners that they have implemented adequate security and compliance controls and have undergone an independent assessment of their security and compliance capabilities. This can increase transparency and trust in cloud services and give customers the information they need to make informed decisions about cloud service providers.\nC5 🔗 Is a cloud computing compliance criteria catalog (C5) to define a baseline security level for cloud computing.\nThe C5 (Cloud Computing Compliance Controls Catalog) is a set of cloud security and compliance standards developed by the German Federal Office for Information Security (BSI). The C5 criteria catalog provides a framework for assessing cloud service providers’ security and compliance capabilities and is designed to help German government agencies and other organizations make informed decisions about cloud service providers.\nThe C5 criteria catalog covers a range of security and compliance controls, including data protection, access control, incident management, business continuity, and compliance with legal and regulatory requirements. It also includes specific requirements for cloud service providers related to data sovereignty, transparency, and auditability.\nTo comply with the C5 criteria catalog, cloud service providers must undergo an independent audit by a certified auditor, who evaluates the provider’s security and compliance controls against the C5 criteria. The audit results are then made available to customers and partners, providing transparency and assurance about the provider’s security and compliance capabilities.\nThe C5 criteria catalog is designed to help organizations assess cloud service providers’ security and compliance capabilities and ensure that their data is protected in the cloud. It is particularly relevant for German government agencies and other organizations subject to strict data protection and security regulations.\n","categories":"","description":"","excerpt":" Framework 🔗 ISO 27001 🔗 Information Security Management specifies a …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/exoscale-compliance/content/std-compliance/std-compliance/","tags":"","title":"Standard Compliance"},{"body":" Summary 🔗 Using cloud computing in the CSRD framework highlights its modern approach to sustainability reporting. Cloud technology enables more efficient compliance with CSRD mandates, merging operational efficiency with sustainability and transparency. This showcases the role of digital transformation in achieving sustainability objectives, positioning cloud computing as a key asset in eco-friendly business operations.\n","categories":"","description":"","excerpt":" Summary 🔗 Using cloud computing in the CSRD framework highlights its …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd-and-cloud/content/summary/","tags":"","title":"Summary"},{"body":" USPs 🔗 Performance 🔗 Larger bar is better\nSingle Score - CPU Performance Multi Score - System Performance (transparent bars) European 🔗 Affordable 🔗 Transparent 🔗 Compliant 🔗 Your data processing and storing with Exoscale is fully GDPR-compliant. Exoscale is not effected by the US Cloud Act. GDPR non-compliance fines up to € 20.000.000 or 4 % of global turnover. ","categories":"","description":"","excerpt":" USPs 🔗 Performance 🔗 Larger bar is better\nSingle Score - CPU …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/exoscale/content/unique-selling-point/","tags":"","title":"Unique Selling Points"},{"body":" Volume of Data 🔗 Statista provides incredible insights into our world via the looking glass of statistics. The mind-boggling statistic about the created volume of data we make worldwide every year can be found just below this paragraph.\nYou can quickly see that the amount of data we produce every year doubled, tripled, or even quadrupled in the last years. Depending on how many years you look back, the internet, the smartphone, social media, digital photography, digital music, digital films, and the Internet of Things, to name a few of the contributors to this enormous peak in the growth of data.\nHowever, if we look beyond today’s development, it’s even more shocking or astonishing. But what the heck are zettabytes? The majority is undoubtedly familiar with Megabyte, Gigabyte, and if you are into digital photography or video making Terabyte. But then we still have to jump further powers of ten - Petabytes and Exabytes - to reach the Zettabytes. Zettabyte equals 1.000.000.000.000.000.000.000 bytes, yes 21 zeros.\nIt is difficult to grasp this amount of data. Therefore, follow the experiment of visualizing this enormous data volume and setting it into relation to something that could demonstrate Zettabytes’ monstrosity. The diagram below uses floppy disks from the digital old age and the solar system as a size reference to make the experiment work.\nThe figure the statistic gave us was 79 Zettabytes in 2021. In 2021 the world population is stated as 7.9 billion people. This means; statistically, every person on the planet is creating 10 Terabyte of data in 2021. But, what if we would store all the 79 Zettabytes on good old Floppy Disks? How high, or how long would this stack be?\nAs you can see, the stack out of Floppy Disks in the diagram is pretty BIG. Our solar system expands 4.5 billion km into space there, and you can find the last planet, Neptune. But the stack of Floppy Disks would go on and finally point 181 billion km into space. There is nothing exceptional there, and in terms of interstellar travel, this is a short trip, but we would never get there in a lifetime with the means of transportation available today. All these astronomical facts and figures aside, the horror of Zettabytes starts probably taking shape in your mind now.\nA long and diverting intro for an IT topic like databases, but as we have seen, storing data is an old topic. Moreover, depending on the volume of data, the evolution of the data tools is becoming essential. Hence, database technology is evergreen in information technology.\nBefore we jump into the database topic, let’s explore one last important facet of data today, the data value chain and companies based on data.\n","categories":"","description":"","excerpt":" Volume of Data 🔗 Statista provides incredible insights into our world …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/databases/content/volume/","tags":"","title":"Volume of Data"},{"body":" Volumes 🔗 Containerized (stateless) applications have the same needs as normal (stateless) applications. They need all types of storage to share, read, and access information. Attaching and sharing temporary storage to a Pod, respective to a container or multiple containers, is handled by the volume concept.\nDifferent storage types can be mounted and used by containers, e.g., temporary volumes, network volumes, and configuration maps. The video below shows how volumes are defined and used in manifests, the configuration mechanism in Kubernetes.\nVolumes\" 🔗 Video: Volumes Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Volumes 🔗 Containerized (stateless) applications have the same needs …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/storage/content/volumes/","tags":"","title":"Volumes"},{"body":" Definition 🔗 Cloud Compliance is the art and science of complying with regulatory standards of cloud usage following industry guidelines and local, national, and international laws, like:\nPCI DSS protects credit card information handling HIPAA protects a patient’s healthcare information SOX protects the financial information of public companies GLBA protects the data of financial institution customers … However, they all share a unified goal: keeping sensitive data secure.\nSecurity is a Shared Responsibility 🔗 Cloud involves infrastructure the organization doesn’t own and manage. Hence, many organizations share responsibility with cloud providers for access control.\nThe cloud provider updates and controls access to the components it administers, including the host operating system, virtualization software, hardware, and facilities.\nThe organization, in turn, retains responsibility for updating, patching, and controlling access to the components it layers on top of the cloud infrastructure—applications, guest operating systems, and security software.\nThe organization’s responsibility includes configuring any firewall services provided by the cloud provider that the organization uses for policy enforcement.\nThis partnership is often described as:\ncloud provider is responsible for the security of the cloud organization is responsible for the access to its resources in the cloud Demystifying Acronyms 🔗 If you dive into the realm of compliance, you stumble upon an endless number of acronyms related to standards and compliance frameworks; here is a list of the most often encounters depending on your geography:\nHIPAA … Health Insurance Portability and Accountability Act - mandates the security of electronic healthcare information, confidentiality and privacy of health-related information, and information access for insurance. PCI DSS … Payment Card Industry Data Security Standard - set of security standards enables all organizations to accept, process, store, and transmit credit card and financial information. GLBA … Gramm-Leach-Bliley Act - organizations must communicate how user information is shared and protected, provide the right to opt out, and apply specific mandated protections. PIPEDA … Personal Information Protection and Electronic Documents Act - provides rules for organizations to handle user information in commercial activities. EU GDPR … General Data Protection Regulation - is the most stringent privacy and security regulation, mandates an exhaustive set of requirements on organizations handling the data of European Union (EU) residents. Furthermore, GDPR imposes harsh penalties for noncompliance. SOX … Sarbanes–Oxley Act - mandates requirements on financial disclosures, audits, and controls of information systems processing financial information. NIST … National Institute of Standards and Technology - provides guidelines on technology-related standards, security, innovation, and economic competitiveness. FedRAMP … Federal Risk and Authorization Management Program - is a standardized program for the security assessment and evaluation of cloud-based systems. HDS … Certification Hébergeur de Données de Santé - Health Data Hosting Certification ensures to treat personal health details as sensitive data. It is regulated by law to protect our rights. As a result, this critical information has to be hosted with an adequate security level. The HDS Reference System defines these requirements. ","categories":"","description":"","excerpt":" Definition 🔗 Cloud Compliance is the art and science of complying …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/what-compliance/content/cloud-compliance/","tags":"","title":"What is Cloud Compliance?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/what-compliance/","tags":"","title":"What is Compliance?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/managed-databases/","tags":"","title":"Why Managed Databases?"},{"body":" EU CSRD 🔗 Expanded Scope Double Materiality Digitalization and Accessibility Alignment with Standards Mandatory Assurance Expanded Scope 🔗 The CSRD expands sustainability reporting requirements beyond the NFRD (Non-Financial Reporting Directive), including all large companies and those listed on regulated markets within the EU, significantly increasing the number of businesses required to provide detailed sustainability information.\nDouble Materiality 🔗 The double materiality perspective requires companies to disclose how sustainability issues impact their financial status and how their operations affect society and the environment.\nDigitalization and Accessibility 🔗 The CSRD mandates digital sustainability reporting for better accessibility and analysis, aligning with the ESAP (European Single Access Point) initiative for a centralized digital platform for all company data, including sustainability insights.\nAlignment with Standards 🔗 The directive requires adherence to detailed sustainability reporting standards developed by EFRAG (European Financial Reporting Advisory Group) to guarantee the comparability, relevance, and consistency of disclosed sustainability information.\nMandatory Assurance 🔗 For the first time, reported sustainability information must be verified through an assurance process to improve its reliability. The process starts with limited assurance and potentially advances to reasonable assurance later.\n","categories":"","description":"","excerpt":" EU CSRD 🔗 Expanded Scope Double Materiality Digitalization and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd/content/aspects/","tags":"","title":"Aspects"},{"body":" Explained 🔗 Attracting Investors Attracting Employees Competitive Advantage Risk Management Attracting Investors 🔗 Sustainability Practices Long-term Focus Attractive to Investors Increasingly, investors prioritize companies with robust sustainability practices, seeing them as better long-term bets.\nAttracting Employees 🔗 Sustainability Record Value Alignment Attractive to Employees A strong sustainability record can make a company more attractive to prospective employees, particularly millennials and Gen Z, who prioritize value alignment.\nCompetitive Advantage 🔗 Sustainability Initiatives Differentiate Brand New Markets Sustainability initiatives can differentiate a brand, opening new markets or consolidating existing ones.\nRisk Management 🔗 ESG Risks Avoid Issues Protect Company Identifying and addressing ESG risks can protect companies against regulatory, reputational, and operational issues.\n","categories":"","description":"","excerpt":" Explained 🔗 Attracting Investors Attracting Employees Competitive …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/corporate-sustainability/content/benefits/","tags":"","title":"Benefits"},{"body":" Overview 🔗 Block storage provides high-performance, flexible, and scalable solutions for applications requiring direct access to storage devices. It is well-suited for databases, virtualized environments, and other performance-critical workloads. Block storage offers several benefits compared to other storage systems.\nHere are some critical advantages of block storage:\nPerformance: Block storage provides high-performance capabilities, making it ideal for applications requiring low latency and high IOPS (Input/Output Operations Per Second). It offers direct access to data blocks, allowing efficient and fast data retrieval and processing. Flexibility: Block storage is highly flexible and can be easily integrated into existing infrastructure. It can be used with various operating systems and applications, making it suitable for multiple use cases. Data Consistency: Block storage ensures data consistency using techniques like write caching and synchronous replication. Data modifications are immediately reported to the storage device, providing the data is always current. Scalability: Block storage systems can scale vertically by adding more storage capacity to a single device or horizontally by adding more widgets to a storage cluster. This allows for easy expansion as your storage needs grow. Data Protection: Block storage offers data protection features such as RAID (Redundant Array of Independent Disks) and snapshotting. RAID provides redundancy by distributing data across multiple disks. At the same time, snapshots allow you to create point-in-time copies of your data for backup and recovery purposes. Data Security: Block storage provides data security features such as rest and transit encryption. This ensures your data is protected from unauthorized access and provides additional security for sensitive information. Application Compatibility: Block storage is compatible with many applications and databases that require direct access to storage devices. It allows efficient data management and supports features like data replication and clustering. Data Recovery: Block storage systems offer efficient data recovery options. In case of data loss or corruption, you can restore data from backups or snapshots, minimizing downtime and ensuring business continuity. Virtualization Support: Block storage is commonly used in virtualized environments. It provides storage resources to virtual machines (VMs) and allows for features like live migration and high availability. ","categories":"","description":"","excerpt":" Overview 🔗 Block storage provides high-performance, flexible, and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/block-storage/content/benefits/","tags":"","title":"Benefits - Block Storage"},{"body":" Overview 🔗 Suppose your organization requires a centralized, easily accessible, affordable way to store files and folders. In that case, file-level storage is a good approach.\nThe benefits of file storage include the following:\nSimplicity: File storage is the simplest, most familiar, and most straightforward approach to organizing files and folders on a computer’s hard drive or NAS device. You name files, tag them with metadata, and store them in folders under a hierarchy of directories and subdirectories. It is not necessary to write applications or code to access your data. File Sharing: File storage is ideal for centralizing and sharing files on a Local Area Network (LAN). Files stored on a NAS device are easily accessible by any computer on the network that has the appropriate permission rights. Common Protocols: File storage uses standard file-level protocols such as Server Message Block (SMB), Common Internet File System (CIFS), or Network File System (NFS). Utilize a Windows or Linux operating system (or both). Standard protocols like SMB/CIFS and NFS allow you to read and write files to a Windows-based or Linux-based server over your Local Area Network (LAN). Data Protection: Storing files on a separate, LAN-connected storage device offers you a level of data protection should your network computer experience a failure. Cloud-based file storage services provide additional data protection and disaster recovery by replicating data files across multiple geographically dispersed data centers. Affordability: File storage using a NAS device allows you to move files off of expensive computing hardware and onto a more affordable LAN-connected storage device. Moreover, suppose you choose to subscribe to a cloud file-storage service. In that case, you eliminate the expense of on-site hardware upgrades and the associated ongoing maintenance and operation costs. Accessibility: Files stored in the cloud can be accessed from anywhere with an internet connection, making it convenient for users to retrieve their files on different devices. Scalability: Cloud storage allows users to quickly scale up or down their storage capacity based on their needs. This eliminates the need to purchase additional hardware or upgrade local storage devices. Data Backup and Recovery: Cloud storage providers typically offer built-in data backup and recovery features, ensuring that files are protected from hardware failures, accidents, or other forms of data loss. Collaboration: Cloud storage enables seamless collaboration among users by allowing them to share files and folders with others. Multiple users can work on the same file simultaneously, making it easier to collaborate on projects. Syncing: Cloud storage often includes file synchronization features, automatically updating files across multiple devices. This ensures that users always have the latest version of their files, regardless of the device they are using. ","categories":"","description":"","excerpt":" Overview 🔗 Suppose your organization requires a centralized, easily …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/file-storage/content/benefits/","tags":"","title":"Benefits - File Storage"},{"body":" Overview 🔗 Overall, object storage provides a scalable, durable, and cost-effective solution for storing and managing large volumes of data. It is suitable for various use cases, including backup and recovery, content distribution, data archiving, and big data analytics. Object storage offers several benefits compared to traditional storage systems.\nHere are some critical advantages of object storage:\nScalability: Object storage is highly scalable, allowing you to store and manage vast data. It can handle petabytes or even exabytes of data without any performance degradation. Durability and Reliability: Object storage systems are designed to provide high durability and reliability. They use data replication and error correction techniques to ensure data integrity and protect against hardware failures. Flexibility: Object storage is flexible and can accommodate various data types, including structured, unstructured, and semi-structured data. It does not impose any specific file system hierarchy, allowing you to organize and access data in a way that suits your needs. Cost-effectiveness: Object storage is often more cost effective than traditional storage systems. It eliminates the need for complex storage architectures and reduces administrative overhead. Additionally, object storage systems typically use commodity hardware, which helps lower costs. Accessibility: Object storage provides universal access to data over standard protocols like HTTP/HTTPS. This makes it easy to access and share data from anywhere, using any device with an internet connection. Metadata and Tagging: Object storage allows you to attach metadata and tags to objects, making searching, categorizing, and organizing data easier. This enables efficient data management and retrieval. Data Security: Object storage systems offer built-in data security features like rest and transit encryption. They also provide access control mechanisms to ensure only authorized users can access and modify data. Data Lifecycle Management: Object storage supports data lifecycle management, allowing you to define data retention, replication, and deletion policies. This helps optimize storage resources and comply with data governance requirements. Integration with Cloud Services: Object storage seamlessly integrates with cloud services and platforms, making it an ideal choice for cloud-native applications and hybrid cloud environments. It enables easy data migration and sharing across different cloud providers. ","categories":"","description":"","excerpt":" Overview 🔗 Overall, object storage provides a scalable, durable, and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/object-storage/content/benefits/","tags":"","title":"Benefits - Object Storage"},{"body":" Definition 🔗 Storage is crucial for preserving and accessing computer data, files, applications, and other digital content. Storage refers to the process of storing and retrieving data or information. It is an essential component of any computer system.\nBasic Storage Types 🔗 There are several basic types of storage that are commonly used in computing systems. Here are the most common types:\nPrimary Storage 🔗 It is volatile, meaning its contents are lost when the computer is powered off or restarted. The computer’s immediate storage space that holds processed data is also known as main memory or RAM (Random Access Memory). Primary storage is much faster than secondary storage but has limited capacity and is more expensive.\nSecondary Storage 🔗 It is non-volatile and long-term storage for data and programs, meaning it retains data even when the power is turned off. It includes devices such as hard disk drives (HDDs), solid-state drives (SSDs), optical discs (CDs, DVDs, Blu-ray), USB flash drives, and network-attached storage (NAS) devices. Secondary storage is slower than primary storage but offers much larger storage capacities at a lower cost.\nTertiary Storage 🔗 Tertiary storage is used for archival purposes and is typically slower and less accessible than primary and secondary storage. Examples include magnetic tape drives and optical jukeboxes.\nUsage Storage Types 🔗 There are several usage types of storage that are commonly used in computing systems. Here are the most common types:\nOffline Storage 🔗 Offline storage refers to storage devices that are not constantly connected to the computer system. Examples include external hard drives, USB flash drives, and memory cards.\nNetwork Storage 🔗 This type of storage is accessed over a network and is shared among multiple computers. Network-Attached Storage (NAS) and Storage Area Networks (SAN) are common examples of network storage.\nVirtual Storage 🔗 Virtual storage is a technique that allows the operating system to use secondary storage as if it were primary storage. It uses techniques like paging and swapping to manage memory efficiently.\nCloud Storage Types 🔗 Cloud storage refers to storing data on remote servers accessed over the internet. It offers scalability, accessibility, and data redundancy. Cloud storage is catagorized into:\nEphemeral Storage 🔗 Ephemeral storage refers to temporary storage that is only available for the duration of a session or a specific task. Once the session or task ends, the data stored in ephemeral storage is typically erased or lost. Examples of ephemeral storage include RAM (Random Access Memory) and cache memory.\nEphemeral storage is often associated with cloud computing platforms, where virtual machines or containers are provisioned with temporary storage that is attached to the instance during its runtime. This storage is typically local to the instance and is not shared with other instances or persisted when the instance is terminated or restarted.\nEphemeral storage is commonly used for temporary files, caches, logs, or any other data that is only needed temporarily and can be easily recreated or regenerated if lost. It is not suitable for storing important or permanent data, as it is not designed for durability or long-term persistence.\nPersistent Storage 🔗 Persistent storage is designed to retain data even when the power is turned off or the session ends. It is used for long-term storage of data that needs to be preserved and accessed across multiple sessions. Examples of persistent storage include hard disk drives (HDDs), solid-state drives (SSDs), optical drives, and cloud storage services.\nPersistent storage is typically provided by external storage systems such as hard disk drives (HDDs), solid-state drives (SSDs), network-attached storage (NAS), storage area networks (SAN), or cloud-based object storage services. These storage systems are designed to ensure data durability and availability, even in the face of hardware failures or other disruptions.\nPersistent storage is typically slower than ephemeral storage, as it often involves accessing data over a network or from disk-based storage devices. However, it provides the advantage of durability and the ability to retain data even in the event of system failures or restarts.\nEphemeral vs Persistent 🔗 The distinction between ephemeral storage and persistent storage lies in the duration of data retention. Ephemeral storage is temporary and volatile, while persistent storage is permanent and non-volatile. The categories for ephemeral storage and persistent storage are as follows:\nUnlike ephemeral storage, which is temporary and local to a computing instance, persistent storage is designed to retain data over extended periods of time. It is commonly used for storing critical application data, databases, user files, configuration files, and other important information that needs to be preserved and accessed across multiple instances or sessions.\nIn contrast to ephemeral storage, persistent storage is designed for long-term data storage and is typically provided by external storage systems such as network-attached storage (NAS), storage area networks (SAN), or cloud-based object storage services. Persistent storage is durable and can survive system restarts or failures, ensuring that data is not lost.\nStorage Tier 🔗 A storage tier refers to a specific level or category of storage within a storage system. It is used to classify and organize data based on its performance, availability, and cost characteristics. Storage tiers are typically defined based on the underlying technology, such as solid-state drives (SSD), hard disk drives (HDD), or tape drives.\nStorage tiers provide a way to categorize and manage data based on its characteristics, allowing organizations to optimize storage resources and meet the specific needs of different types of data.\nStorage Performance 🔗 The purpose of implementing storage tiers is to optimize the utilization of storage resources and match the requirements of different types of data. By assigning data to appropriate storage tiers, organizations can ensure that frequently accessed or critical data is stored on high-performance storage media, while less frequently accessed or less critical data is stored on lower-cost storage media.\nStorage Management 🔗 Storage tiers are often associated with hierarchical storage management (HSM) systems or storage virtualization technologies. These systems automatically move data between different tiers based on predefined policies and data access patterns. This dynamic data movement helps to balance performance, cost, and capacity requirements.\nStorage Type vs Storage Tier 🔗 The terms storage type and storage tier are related but have different meanings in the context of storage systems. Here’s a breakdown of the differences:\nStorage Type 🔗 A storage type refers to the underlying technology or medium used for storing data. It describes the physical or logical storage device or system. For example, common storage types include solid-state drives (SSD), hard disk drives (HDD), tape drives, or cloud storage. Each storage type has its own characteristics in terms of performance, capacity, cost, and durability.\nStorage Tier 🔗 A storage tier, on the other hand, refers to a specific level or category within a storage system that is based on performance, availability, and cost. It is a way of organizing and classifying data based on its importance or access patterns. Storage tiers are typically defined within a storage system and can consist of one or more storage types.\nIn other words, a storage tier is a logical grouping or classification of data based on its characteristics, while a storage type refers to the actual technology or medium used for storing the data.\nStorage tiers are often implemented within storage systems to optimize data placement and resource utilization. By assigning data to appropriate storage tiers, organizations can ensure that data is stored on the most suitable storage types based on its importance, performance requirements, and cost considerations. This allows for efficient data management and cost-effective storage solutions.\n","categories":"","description":"","excerpt":" Definition 🔗 Storage is crucial for preserving and accessing computer …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/why-storage/content/defination/","tags":"","title":"Calculate Scenario Pricing"},{"body":" Calculate Scenario Pricing 🔗 For an overall scenario pricing, we have to add up all component prices - like the ones we calculated before - in our scenario, add data transfers to the internet and amount of storage in rest to the equation.\nAdditional storage costs are associated with the Simple Object Storage (SOS). A scalable, reliable, and cost-effective solution to support your application. Backup or serve your data from any Exoscale zone with no hidden fees, using your existing S3-compatible tooling and a familiar API.\nApplication Server Data Transfer Calculation 🔗 6 x 720 x 1.42 GB = 6134.40 GB/month\ndata transfer to the Internet: 1000 GB/month free tier definition = 1.42 GB/h/instance The free tier for our web-application consisting of 6 instances is 6134 GB; the monthly data transfer is 1000 GB to the Internet; hence it is below the free tier for our scenario.\nPublic File Bucket Calculation 🔗 200 x 0.020 + 10000 x 0.020 = €204.00/month\n200 GB data stored 10 TB data transferred (10000 GB) Backup Bucket Calculation 🔗 1000 x 0.020 = €20.00/month 1 TB data stored (1000 GB) Calculation of Complete Scenario 🔗 Item Cost (€/month) Application Server Instances € 87.35 Database Server Instances € 221.75 Backup Server Instance € 15.54 Elastic IP € 20.00 DNS € 1.00 Application Server Data Transfer € 0.00 Public File Bucket € 204.00 Backup Bucket € 20.00 TOTAL € 569.64 ","categories":"","description":"","excerpt":" Calculate Scenario Pricing 🔗 For an overall scenario pricing, we have …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/pricing/pricing/content/calculate-scenario-pricing/","tags":"","title":"Calculate Scenario Pricing"},{"body":" Definition 🔗 Cloud storage is a service that provides a convenient and scalable solution for storing, backing up, and accessing data over the Internet, eliminating the need for physical storage devices like hard drives or USB sticks.\nIt is a form of secondary storage where users can save their files on remote servers managed by cloud storage providers. These providers offer various storage plans and features designed to meet diverse user requirements, allowing for easy storage capacity expansion and accessibility from any location with an Internet connection.\nThis approach simplifies data management, enhances data security, and promotes collaboration through shared access to files among users.\nBenefits 🔗 Cloud storage offers several advantages over traditional local storage. Here are some key features and benefits of cloud storage:\nScalability 🔗 Cloud storage providers typically offer flexible plans, allowing users to increase or decrease their storage capacity as needed. This scalability makes it easy to accommodate changing storage requirements.\nAccessibility 🔗 Cloud storage allows users to access their files anywhere with an internet connection. This makes it convenient for users to access their data from different devices, such as computers, smartphones, or tablets.\nCollaboration 🔗 Cloud storage services often include collaboration features, allowing multiple users to access and work on the same files simultaneously. This makes it easier for teams to collaborate on projects and share documents.\nCost-effectiveness 🔗 Cloud storage eliminates the need for users to invest in and maintain their physical storage infrastructure. Users typically pay for the storage they use on a subscription basis, making it a cost-effective solution for individuals and businesses.\nBackup and Recovery 🔗 Cloud storage providers often have robust backup and recovery mechanisms in place. This helps protect data from loss due to hardware failures, accidents, or other unforeseen events. Users can quickly restore their files from backups stored in the cloud.\n","categories":"","description":"","excerpt":" Definition 🔗 Cloud storage is a service that provides a convenient …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/why-cloud-storage/content/cloud-storage/","tags":"","title":"Cloud Storage"},{"body":" GDPR vs. CLOUD Act 🔗 GDPR: protect personal data, valid for all companies operating in the EU, fines at non-compliance\nCLOUD Act: hand over personal data to the US government, valid for all US-owned companies, fines or prison at non-compliance\nExoscale’s Compliance - Frameworks / Web / Centers 🔗 Exoscale’s Compliance Web\nFull compliance to global and local security frameworks, certified by world’s most stringent auditors, to enable a smooth and safe adoption of our cloud platform by SaaS providers and enterprises in every sector.\nexoscale.com/compliance/\nExoscale’s Compliance Center\nExoscale is committed to helping our customers achieve and maintain compliance with industry and government regulations. In our Compliance Center, you will find all the information you need to know about our compliance posture.\nThis includes information about our security controls, policies and procedures, as well as certificates, attestations and compliance reports. We will continue to update this center as our compliance posture evolves. If you have any questions about our compliance posture or would like more information, please contact:\nsupport@exoscale.com\n","categories":"","description":"","excerpt":" GDPR vs. CLOUD Act 🔗 GDPR: protect personal data, valid for all …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/exoscale/content/compliance/","tags":"","title":"Compliance"},{"body":" Summary 🔗 CloudAssess by Exoscale is a groundbreaking initiative that blends sustainability with digital innovation, making cloud services environmentally transparent and friendlier. Despite making strides in understanding the environmental impact of cloud usage, challenges in data availability persist.\nHowever, CloudAssess serves as an influential model for future cloud computing innovation, encouraging providers to equip their users with detailed environmental impact insights.\n","categories":"","description":"","excerpt":" Summary 🔗 CloudAssess by Exoscale is a groundbreaking initiative that …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd-and-exoscale/content/summary/","tags":"","title":"Conclusion"},{"body":" Introduction 🔗 Configuring Meshery is foundational to effectively managing and deploying your infrastructure. By understanding what Teams, Workspaces,Connections and Environments are and how to set them up, you can effectively manage and monitor your cloud-native infrastructure, as well as collaborate with your team by sharing and organizing your resources.\nPrerequisite\nAccess to Meshery (Self-Hosted or Meshery Playground). To start configuring Meshery:\nNavigate to the left sidebar of Meshery. Click on the Lifecycle dropdown and you will see all the menu items we need in this chapter. Creating Teams 🔗 Creating a Team is the first step in configuring Meshery. In Meshery, a team is a user group that manages and shares access to resources such as Workspaces, Designs, and Environments. Teams enable efficient collaboration and permission management, facilitating organized operations within an organization. Teams offer control access to workspaces and to workspace resources such as Environments and Connections.\nFollow the steps below to create a team:\nTeams are visible when you visit the Identity page in Layer5 Cloud. Select Add Team, enter a name for your team, add Team Members, and Create Team. Creating Workspaces 🔗 Creating a Workspace is an important step in configuring Meshery. A workspace in Meshery is a logical grouping of resources that helps organize and manage your infrastructure more effectively. It is used to separate different projects or environments within your team, enabling better collaboration and resource management. Every user is assigned a default workspace, which can be customized as needed.\nTo create a workspace:\nNavigate to Workspaces under the Lifecycle dropdown. Click Create. Enter a name for your workspace and Save. Once saved, the new Workspace will be created and displayed on the screen. In the subsequent sections, we will discuss Environments and Designs shown in the image above.\nCreating Environments and Associating Connections 🔗 Environments and Connections are essential for efficient resource management in Workspaces. Environments logically group related Connections and Credentials, simplifying management and sharing. Connections within Environments become immediately available for use in any assigned Workspaces.\nCreating an Environment 🔗 To create an environment:\nNavigate to Environments under the Lifecycle dropdown. Click Create. Enter the name of the environment and Save. Once saved, the new Environment will be created and displayed on the screen. Next We’ll learn how to assign Connections to Environments.\nAssociating Connections with an Environment 🔗 Examples of connections managed by Meshery include GitHub integrations, Prometheus connections, Kubernetes clusters, and more. It’s essential to assign these connections to an environment and link that environment to your selected workspace.\nTo add a connection to an environment:\nClick on the arrows icon, and a modal displaying your available connections will appear. Available Connections shows a list of Kubernetes clusters that are currently managed by Meshery. Select the connection(s) you want to assign. Use the appropriate arrow icons to manage your selections: The first arrow assigns all connections. The second arrow assigns only the selected connections. The third arrow removes the selected connection. The fourth arrow removes all connections. Click Save to confirm your changes. The Connections have now been assigned to the Environment. Navigating the Connections Page 🔗 The Connections page serves as a central hub for managing the clusters you are connected to. This page provides valuable information and functionality to help you understand and interact with all your connections.\nTo access the Connections page, click on Connections under the Lifecycle dropdown.\nAdding Cluster Connection 🔗 On the Connections page, you can easily add a Kubernetes cluster connection. To do this:\nClick the Add Cluster button to get started. When the modal appears, navigate to the location of your kubeconfig file and upload it to add your cluster as a connection. After uploading the kubeconfig file, the modal should indicate that your cluster has been successfully added as a connection.\nIf you uploaded an invalid kubeconfig file, you might see an error message statement like the one below. Please ensure you are uploading the correct file with the proper configuration and try again.\nViewing Cluster Connection Details 🔗 Each connection has key details associated with it such as:\nEnvironment: This displays the environments associated with the connection. Click on the drop-down to add your connection to your Environment and see which Environments the connection is linked to. Status: Indicates the currently assigned state and information about what state the connection may or may not transition to. You can learn about Connection Status If MeshSync is actively running in your cluster, clicking the “Flush MeshSync” button will update MeshSync with the latest data, ensuring it matches the current state of your cluster. This ensures that MeshSync’s data is refreshed and accurately reflects the current state of your cluster’s infrastructure and resources. Integrating Workspaces with Environments and Designs 🔗 You can assign your created Environments to Workspaces, enabling effective collaboration and resource utilization across your team. This feature allows you to organize your Environments based on specific projects, teams, or use cases, making it easier to manage and share resources within your organization.\nIntegrating designs with Workspaces enables effective collaboration and sharing of infrastructure configurations across your team. When you assign an Environment containing your designs to a Workspace, team members with access to that Workspace can view, manage, and build upon the shared designs and share feedback.\nAssigning Environments and Designs to Workspaces 🔗 Navigate to Workspaces under Lifecycle. In the image below, there are two arrows for associating Environments and Designs to Workspaces, respectively. Click the appropriate arrow to assign the selected Environments(or Designs) and Save. Confirm Workspace Association 🔗 Now, the environment and design are associated with the workspace. You get standardized resource deployment and cross-team collaboration. This enables efficient resource management and tracking, and makes team members more productive.\nConclusion 🔗 In this chapter, you learned how to effectively configure Meshery, laying the groundwork for managing your infrastructure and optimizing workload within Kubernetes environments. Starting with the creation of workspaces, you established collaborative environments where teams can organize and deploy resources efficiently.\nYou explored the setup of environments, which serve as logical groupings for managing Kubernetes connections and other resources. By adding cluster connections and associating them with environments, you ensured seamless integration and management of your infrastructure components.\nThroughout the configuration process, you encountered various steps to integrate workspaces with environments and designs. This integration not only streamlined resource management but also facilitated standardized deployments across Kubernetes clusters associated with your workspace.\n","categories":"","description":"This chapter covers the steps for configuring meshery, including the process of creating teams, workspaces, environments, and connections. It also explains how they associate with each other.","excerpt":"This chapter covers the steps for configuring meshery, including the …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-meshery/introduction-to-meshery/configuring-meshery/","tags":"","title":"Configuring Meshery"},{"body":" Checkpoints 🔗 Cloud computing offers distinct advantages for corporate sustainability, but it’s essential to navigate this space thoughtfully! Not all cloud providers prioritize sustainability equally.\nProvider Selection 🔗 Cloud Sustainability Renewable Energy Carbon Reduction Not all cloud providers prioritize sustainability equally. Companies should assess providers’ commitment to sustainability, including their use of renewable energy and efforts to reduce carbon emissions.\nData Transfer and Storage 🔗 Data Management Cloud Migration Sustainability Benefits Migrating to the cloud can lead to significant data transfer and increased storage requirements. Organizations should strategize to minimize unnecessary data accumulation and ensure efficient data management to avoid offsetting the sustainability benefits.\nLifecycle Management 🔗 Cloud Transition Legacy Hardware Minimize e-Waste As companies transition to the cloud, they must responsibly manage the lifecycle of their legacy hardware to minimize e-waste.\n","categories":"","description":"","excerpt":" Checkpoints 🔗 Cloud computing offers distinct advantages for …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/cs-and-cloud/content/considerations/","tags":"","title":"Considerations"},{"body":" Container Benefits 🔗 dev and ops separation of concerns create application container images at build/release time rather than deployment time, thereby decoupling applications from infrastructure continuous development, integration, and deployment provides for reliable and frequent container image build and deployment with quick and efficient rollbacks – due to image immutability environmental consistency across dev, test, and prod runs the same on a laptop as it does on an on-premises server, virtualized server, and in the cloud\nOS distribution and cloud portability runs on Ubuntu, RHEL, CoreOS, on major public clouds, on-premises, and anywhere else\nresource utilization and isolation benefits higher efficiency and density due to better utilization and predictable application performance due to isolation\nloosely coupled, distributed, elastic microservices applications are broken into smaller, independent pieces and can be deployed and managed dynamically – not a monolithic stack running on one big single-purpose machine\nagile application creation and deployment increased ease and efficiency of container image creation compared to VM image use\napplication-centric management raises abstraction level:\nfrom running applications on an OS using virtual hardware to running applications on an OS using logical resources ","categories":"","description":"","excerpt":" Container Benefits 🔗 dev and ops separation of concerns create …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-containers/content/container-benefits/","tags":"","title":"Container Benefits"},{"body":" Explained 🔗 Continuous Delivery is a software development practice where code changes are automatically built, tested, and deployed to production safely and reliably, with minimal human intervention.\nIt aims to deliver software updates frequently and consistently, allowing organizations to respond quickly to changing market needs and customer feedback.\nContinuous Delivery is a powerful approach to software development that can help businesses achieve greater agility, efficiency, and quality in their software delivery process. By automating the build, test, and deployment process, developers can focus on writing code and delivering new features rather than spending time on manual processes. Additionally, Continuous Delivery can help to improve software quality by catching errors and bugs early in the development process before they can cause problems in production. However, there are also some potential challenges to implementing Continuous Delivery.\nFor example, it can require significant investment in tools, infrastructure, and training to implement effectively. Additionally, Continuous Delivery can introduce additional complexity and risk, mainly when working with microservices or distributed systems. Despite these challenges, Continuous Delivery is becoming increasingly popular in modern software development, allowing businesses to deliver software faster and more reliably while improving quality and reducing costs.\n","categories":"","description":"","excerpt":" Explained 🔗 Continuous Delivery is a software development practice …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud-native/content/continuous/","tags":"","title":"Continuous Delivery"},{"body":" Create and Configure Secret for MySQL Database 🔗 In this step, you create a Kubernetes secret component for the MySQL database. This is necessary because the configuration below is in the environment variables section of the mysql-deployment YAML file.\nenv: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password Before you proceed, choose a password and convert it into base64 format. You can use an online tool for this conversion. For this example, the password is password and its base64 encoding is cGFzc3dvcmQ=.\nClick on the Kubernetes icon on the dock, search for secret, and click on it or drag it to the canvas. Figure: Create secret component\nClick on the Secret component to open the configuration window.\nSet the name as mysql-pass Set the Type as Opaque Click + next to Data and add the secret as a key-value pair password:cGFzc3dvcmQ= Figure: Configure secret\nClick outside the window to close the configuration tab. Create Persistent Volumes 🔗 MySQL and WordPress each require a Persistent Volume (PV) to store their data.\nFor this tutorial, you will use the manual StorageClassName and set the Persistent Volume to use the hostPath type.\nPlease note that using hostPath for Persistent Volumes is generally not recommended for production environments because it ties the volume to the node’s filesystem, which can lead to data loss if the node fails. However, you can use it in this tutorial for development purposes.\nClick on the Kubernetes icon on the dock, search for Persistent Volume, and select it. Create two PVs. Figure: Create persistent volume\nClick on the wordpress PV to open the configuration window.\nChange the “name” to wp-pv Set the “StorageClassName” as manual Click + next to “AccessMode” and enter ReadWriteOnce Figure: Configure persistent volume\n- Scroll down to \"Capacity\" and enter the key pair `storage:20Gi` Figure: Persistent volume capacity\n- Scroll down to \"Hostpath\" and input `mnt/data/wp-pv` for the _path_ and `DirectoryOrCreate` for the _type_. Figure: Persistent volume hostpath\nRepeat similar steps for the MySQL Persistent Volume\nClick on the MySQL PV to open the configuration window. Change the “name” to mysql-pv Set the “StorageClassName” to manual Click + next to “AccessMode” and set it to ReadWriteOnce Scroll down to “Capacity” and enter the key pair storage:20Gi Scroll down to “Hostpath” and input mnt/data/mysql-pv for the path and DirectoryOrCreate for the type. Click on wp-pv-claim and mysql-pv-claim and set their “StorageClassName” as manual.\n","categories":"","description":"In this section you will create and configure Persistent Volumes, Persistent Volume Claims and a Secret for the Database.","excerpt":"In this section you will create and configure Persistent Volumes, …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/deploying-wordpress-and-mysql-with-persistent-volumes-with-meshery/sql/configure-components/","tags":"","title":"Create and Configure Kubernetes Components"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd/","tags":"","title":"CSRD"},{"body":" Import Redis Helm Chart and Deploy to Cluster 🔗 Follow the same steps used for the Dapr deployment to import this Redis helm chart into Meshery and deploy. Click Actions to deploy, then click Open In Visualizer. In Visualizer mode, use the filter to adjust the views of the resources in the cluster. For View Selector select Single Node. For Kinds select the resources you want to see including Deployments, Pods, Services, Statefulset, Secret, Replicaset, Endpoints and Endpoint slices. For Namespaces select dapr_system and default. These filter settings will allow you to view both Dapr resources within the dapr-system namespace and Redis resources within the default namespace.\nNext, let’s deploy the Dapr state store component that will manage this Redis state store.\n","categories":"","description":"This chapter covers the deployment of Redis Statestore","excerpt":"This chapter covers the deployment of Redis Statestore","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/explore-dapr-with-meshery/dapr/deploy-redis/","tags":"","title":"Deploy Redis"},{"body":" Explained 🔗 Public Cloud 🔗 Public Cloud is a cloud computing model where the service provider owns, manages, and maintains the physical infrastructure, such as data centers, servers, networking equipment, and storage. Users access virtualized computing, networking, and storage resources as services, which can be quickly and easily provisioned and scaled as needed.\nPublic Cloud is accessible over the internet and is typically offered on a pay-per-use basis. It allows organizations to avoid the costs and complexities of managing their hardware and infrastructure and instead focus on their core business needs.\nPrivate Cloud 🔗 Private Cloud is a cloud computing model where the cloud infrastructure is provisioned for exclusive use by a single organization. The infrastructure can be on-premises or on a public cloud, such as a Virtual Private Cloud (VPC), and is owned, managed, and operated by the organization or the cloud provider.\nPrivate Cloud provides organizations greater control and security over their data and infrastructure and can be customized to meet their business needs.\nHybrid Cloud 🔗 Hybrid Cloud is a cloud computing model in which an organization’s on-premises private and third-party public cloud are connected as a single, flexible infrastructure. This allows organizations to leverage both Public and Private clouds’ features and benefits and move workloads between them as needed.\nAs a result, the Hybrid cloud provides organizations with greater flexibility, scalability, and cost-efficiency while allowing them to maintain control over their data and infrastructure.\n","categories":"","description":"","excerpt":" Explained 🔗 Public Cloud 🔗 Public Cloud is a cloud computing model …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud/content/deployment-models/","tags":"","title":"Deployment Models"},{"body":" Explained 🔗 Block storage is a fundamental component of cloud computing infrastructure, offering a versatile and scalable solution for storing and managing data. It allows dividing data into fixed-sized blocks, which can be accessed and manipulated independently. This flexibility allows for efficient data storage and retrieval, making block storage an ideal choice for applications requiring high-performance storage, such as databases and virtual machines. By leveraging block storage, organizations can focus on their core workflows and applications without the burden of managing file storage and locations. With the ability to scale horizontally and integrate seamlessly with other cloud services, block storage empowers customers to overcome the limitations of legacy storage systems, ensuring optimal performance and cost-effectiveness at any scale.\nDefinition 🔗 Block storage is an architecture that divides data into fixed-size blocks, each with a unique address. Block storage devices like hard disk and solid-state drives use block-level access protocols like iSCSI and Fibre Channel to read and write data. This storage method is ideal for low-latency, high-performance environments where data access speed is crucial. However, it can be more complex and expensive to set up initially.\nCritical characteristics of block storage include:\nBlock-level data access: Block storage systems provide direct access to the underlying storage blocks, allowing high-speed data reads and writes. This low-latency access is particularly beneficial for applications requiring rapid data access, such as databases and virtual machines. File system abstraction: Block storage devices present a file system abstraction to the host operating system, meaning they appear as a traditional file system to the user. This abstraction allows users and applications to interact with the storage device using familiar file and directory structures, simplifying data storage and retrieval. Data consistency: Block storage systems employ data consistency techniques, such as journaling and copy-on-write, to ensure that data remains consistent and accurate. These techniques help to prevent data corruption and loss, making block storage a reliable option for critical data storage requirements. Now that we’ve explained the basic capabilities of object and block storage, let’s see how they go head-to-head. ","categories":"","description":"","excerpt":" Explained 🔗 Block storage is a fundamental component of cloud …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/block-storage/content/details/","tags":"","title":"Details - Block Storage"},{"body":" Explained 🔗 File storage has been around for considerably longer than object or block storage. It is something most people are familiar with. You name your files, place them in folders, and nest them under more folders to form a set path. This way, files are organized into a hierarchy, with directories and sub-directories. Each file also has a limited set of metadata associated with it, such as the file name, the date it was created, and the date it was last modified.\nHierarchical file storage is practical for organizing structured data. This works well up to a point, but as capacity grows, the file model becomes burdensome for two reasons. First, performance suffers beyond a certain capacity. A NAS system has limited processing power, making the processor a bottleneck. Second, performance also suffers with the massive database – the file lookup tables – that accompany capacity growth.\nScaling requires adding more hardware or upgrading to higher-capacity devices, which can be costly. Cloud-based file storage services offer a solution by allowing multiple users to access and share files stored in off-site data centers. With a monthly subscription fee, you can keep your files in the cloud, quickly scale up capacity, and specify performance and protection criteria. Additionally, you save on the expense of maintaining on-site hardware since the cloud service provider manages the infrastructure.\nDefinition 🔗 File storage in the cloud refers to the practice of storing and managing files on remote servers that are accessible via the Internet. Instead of saving files on local storage devices like hard drives or USB drives, cloud storage allows users to upload and store files on servers maintained by a cloud storage provider. These files can then be accessed, shared, and synced across multiple devices and platforms.\nExamples of file cloud storage services include Dropbox, Google Drive, and Microsoft OneDrive. These services typically offer a certain amount of free storage, with additional storage available for a fee.\nCloud File Storage: is a method for storing data in the cloud that provides servers and applications access to data through shared file systems. This compatibility makes cloud file storage ideal for workloads that rely on shared file systems and provides simple integration without code changes. Cloud File System: is a hierarchical storage system in the cloud that provides shared access to file data. Users can create, delete, modify, read, and write files, as well as organize them logically in directory trees for intuitive access. Cloud File Sharing: is a service that provides simultaneous access for multiple users to a common set of files stored in the cloud. Security for online file storage is managed with user and group permissions so administrators can control access to the shared file data. Requirements for Cloud File Storage 🔗 An ideal file-based data storage solution in the cloud must deliver the proper performance and capacity for today while also being capable of scaling as business needs change. The solution should include the following features:\nPerformance: provides consistent throughput, scalable storage space, and low-latency performance Security: provides network security and access control permissions for sensitive data protection Affordability: pay only for capacity used with no upfront provisioning costs or licensing fees. Availability: Redundancy across multiple sites and always accessible when needed. Compatibility: integrates seamlessly with existing applications with no new code to write Fully Managed: provides a system that can be launched in minutes with no physical hardware required or ongoing software maintenance Different Cloud File Storage Services 🔗 The benefits of cloud file storage are clear, but it is important to note that not all cloud file storage solutions are created equal; various solutions exist. Cloud file storage can be delivered in one of two ways:\nthrough fully managed solutions with minimal setup and little to no maintenance or through do-it-yourself solutions with separate compute, storage, software, and licensing, which require staffed expertise to configure and maintain. ","categories":"","description":"","excerpt":" Explained 🔗 File storage has been around for considerably longer than …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/file-storage/content/details/","tags":"","title":"Details - File Storage"},{"body":" Explained 🔗 Object storage has become the foundation for web-scale architectures in public or private clouds. Its appeal is due to its potential to handle massive scale while minimizing complexity and cost. It allows application developers and users to focus more on their workflow and logic and not worry about managing file storage and file locations. Customers struggling with large-scale data storage deployments are turning to object storage to overcome the limitations that legacy storage systems face at scale.\nDefiniton 🔗 Object storage is an architecture that manages data as distinct units, known as objects. Each object contains the actual data, metadata, and a unique identifier. The metadata stored alongside the data can include information such as creation date, size, and custom attributes. This technique manages and manipulates data storage using objects in a central location, not structured as files within folders. This approach to data storage offers substantial benefits in terms of scalability, cost-efficiency, and ease of use. It is a primary form of storage used in cloud computing infrastructure.\nCritical characteristics of object storage include:\nFlat Address space: Object storage uses a flat address space, meaning that there’s no hierarchical structure like you’d find with traditional file systems. This lack of hierarchy allows for massive scalability, as there’s no need to manage complex file paths or directory structures. Objects can be stored and retrieved using their unique identifier, making it easy to find and access data regardless of size or location. Data Durability and redundancy: Object storage solutions often employ erasure coding and data replication techniques to ensure data durability and redundancy. Erasure coding breaks data into smaller pieces and adds redundancy. In contrast, data replication creates copies of the data across multiple storage nodes. Both techniques help to protect against data loss, ensuring that your information remains secure and available even in the event of hardware failures. RESTful APIs: Object storage systems typically use RESTful APIs (Application Programming Interfaces) for communication between clients and the storage system. These APIs allow developers to easily integrate object storage into their applications, streamlining data storage and retrieval processes. S3-compatible: Amazon’s Simple Storage Service definition is the de facto standard for object storage. S3-compatible object storage refers to any storage service that supports the same application programming interface (API) as Amazon’s Simple Storage Service (S3). The term “S3 compatible” means that the storage service can interact with the S3 API, accepting the same commands and returning the expected responses as Amazon S3. This compatibility enables developers and applications designed to work with S3 to also work with other S3-compatible storage services without changing the code.\nKey features of S3-compatible object storage typically include:\nRESTful interface: S3 compatibility is primarily about adhering to the RESTful interface provided by Amazon S3, which uses standard HTTP/HTTPS methods like GET, PUT, POST, and DELETE.\nBucket and Object Model: The data storage model is organized into buckets (containers for storage) and objects (the individual pieces of data). This model is a fundamental aspect of S3.\nScalability: Like S3, compatible storage services can typically scale to store large amounts of data without degrading performance.\nDurability and Availability: These services are usually designed to be highly durable, ensuring data is not lost and highly available, meaning data can be accessed when needed.\nSecurity: S3 compatible services often offer similar security features, such as encryption in transit (SSL/TLS) and at rest, access controls, and possibly integration with identity services.\nData Lifecycle Management: They may provide features for managing data’s lifecycle, including automatic deletion or transitioning to lower-cost storage tiers based on age or other criteria.\nAdopting S3-compatible object storage is common for businesses that wish to avoid vendor lock-in or prefer to use an alternative to Amazon S3 for reasons such as cost, performance, data sovereignty, or a preference for on-premises storage. Various storage solution providers, including cloud services and on-premises storage vendors, offer S3-compatible object storage solutions.\nMost backup software directly supports S3-compatible storage, and many programming languages have library functions that allow easy interaction with this storage type from the program code level.\n","categories":"","description":"","excerpt":" Explained 🔗 Object storage has become the foundation for web-scale …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/object-storage/content/details/","tags":"","title":"Details - Object Storage"},{"body":" Evolution of Data Services 🔗 There is a rapidly increasing popularity of managed data services, and many companies are on the lookout to find the most fitting service for the tasks at hand. So, not over stressing running a data center but still trying to drive this essential message home. Questions like:\nHow many servers do I need? Do I have to cluster my services? Which database do I need? Do I need more than one database technology? The questions can go on and on; thinking about them and solving them can easily cause bad headaches. So, finally, it comes down to an easy decision on the question: “Self-Hosted vs Managed?” And if you are not in the hosting business, you’re set and done.\nThe last question from the list above is essential in modern applications because usually one database technology is not enough. Application architecture has changed a lot, and the usage of data technologies with it. Look at the diagram below for a glimpse into the new application design and the impact on database features needed and used. Self-hosted is out of the race; managed is the way to go, end of the story. Managed Databases are the next step in the evolution, but an extensive DBaaS portfolio is the ultimate goal.\nLooking back on the diagram of the example application shows that more and more data technologies are being utilized. So, sourcing different database technologies from a managed service provider is definitely not the final solution there. However, they are reducing complexity, increasing convenience for data service usage.\nMeaning we have to look for a managed database technology portfolio, ideally from one provider. Hence, Exoscale DBaaS is precisely that.\nIf you want to have a QuickStart with these technologies, go to our DBaaS Documentation here.\n","categories":"","description":"","excerpt":" Evolution of Data Services 🔗 There is a rapidly increasing popularity …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/managed-databases/content/evolution/","tags":"","title":"Evolution of Data Services"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/exoscale-compliance/","tags":"","title":"Exoscale's Compliance"},{"body":" The components deployed on the service mesh by default are not exposed outside the cluster. An Ingress Gateway is deployed as a Kubernetes service of type LoadBalancer (or NodePort). To make Bookinfo accessible external to the cluster, you have to create an Istio Gateway for the Bookinfo application and also define an Istio VirtualService with the routes we need.\nInspecting the Istio Ingress Gateway 🔗 The ingress gateway gets exposed as a normal Kubernetes service of type LoadBalancer (or NodePort):\nkubectl get svc istio-ingressgateway -n istio-system -o yaml Because the Istio Ingress Gateway is an Envoy Proxy you can inspect it using the admin routes. First find the name of the istio-ingressgateway:\nkubectl get pods -n istio-system Copy and paste your ingress gateway’s pod name. Execute:\nkubectl -n istio-system exec -it \u003cistio-ingressgateway-...\u003e bash You can view the statistics, listeners, routes, clusters and server info for the Envoy proxy by forwarding the local port:\ncurl localhost:15000/help curl localhost:15000/stats curl localhost:15000/listeners curl localhost:15000/clusters curl localhost:15000/server_info See the admin docs for more details.\nAlso it can be helpful to look at the log files of the Istio ingress controller to see what request is being routed.\nBefore we check the logs, let us get out of the container back on the host:\nexit Now let us find the ingress pod and output the log:\nkubectl logs istio-ingressgateway-... -n istio-system View Istio Ingress Gateway for Bookinfo 🔗 View the Gateway and VirtualServices 🔗 Check the created Istio Gateway and Istio VirtualService to see the changes deployed:\nkubectl get gateway kubectl get gateway -o yaml kubectl get virtualservices kubectl get virtualservices -o yaml Find the external port of the Istio Ingress Gateway by running: 🔗 kubectl get service istio-ingressgateway -n istio-system -o wide To just get the first port of istio-ingressgateway service, we can run this:\nkubectl get service istio-ingressgateway -n istio-system --template='{{(index .spec.ports 1).nodePort}}' Create a DNS entry: 🔗 Modify your local /etc/hosts file to add an entry for your sample application.\n127.0.0.1. bookinfo.meshery.io\nThe HTTP port is usually 31380.\nOr run these commands to retrieve the full URL:\necho \"http://$(kubectl get nodes --selector=kubernetes.io/role!=master -o jsonpath={.items[0].status.addresses[?\\(@.type==\\\"InternalIP\\\"\\)].address}):$(kubectl get svc istio-ingressgateway -n istio-system -o jsonpath='{.spec.ports[1].nodePort}')/productpage\" Docker Desktop users please use http://localhost/productpage to access product page in your browser.\nApply default destination rules 🔗 Before we start playing with Istio’s traffic management capabilities we need to define the available versions of the deployed services. They are called subsets, in destination rules.\nUsing Meshery, navigate to the Custom yaml page, and apply the below to create the subsets for BookInfo:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage subsets: - name: v1 labels: version: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: ratings spec: host: ratings subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v2-mysql labels: version: v2-mysql - name: v2-mysql-vm labels: version: v2-mysql-vm --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: details spec: host: details subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 This creates destination rules for each of the BookInfo services and defines version subsets\nIn a few seconds we should be able to verify the destination rules created by using the command below:\nkubectl get destinationrules kubectl get destinationrules -o yaml Browse to BookInfo 🔗 Browse to the website of the Bookinfo. To view the product page, you will have to append /productpage to the url.\nReload Page 🔗 Now, reload the page multiple times and notice how it round robins between v1, v2 and v3 of the reviews service.\n###Inspect the Istio proxy of the productpage pod\nTo better understand the istio proxy, let’s inspect the details. Let us exec into the productpage pod to find the proxy details. To do so we need to first find the full pod name and then exec into the istio-proxy container:\nkubectl get pods kubectl exec -it productpage-v1-... -c istio-proxy sh Once in the container look at some of the envoy proxy details by inspecting it’s config file:\nps aux ls -l /etc/istio/proxy cat /etc/istio/proxy/envoy-rev0.json For more details on envoy proxy please check out their admin docs.\nAs a last step, lets exit the container:\nexit Alternative: Manual installation 🔗 Follow this if the above steps did not work for you\nDefault destination rules 🔗 Run the following command to create default destination rules for the Bookinfo services:\nkubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml Configure the Bookinfo route with the Istio Ingress gateway 🔗 We can create a virtualservice \u0026 gateway for bookinfo app in the ingress gateway by running the following:\nkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml ","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/advance-concepts-of-service-meshes/expose-services/","tags":"","title":"Expose services through Istio Ingress Gateway"},{"body":" The components deployed on the service mesh by default are not exposed outside the cluster. An Ingress Gateway is deployed as a Kubernetes service of type LoadBalancer (or NodePort). To make Bookinfo accessible external to the cluster, you have to create an Istio Gateway for the Bookinfo application and also define an Istio VirtualService with the routes we need.\nInspecting the Istio Ingress Gateway 🔗 The ingress gateway gets exposed as a normal Kubernetes service of type LoadBalancer (or NodePort): kubectl get svc istio-ingressgateway -n istio-system -o yaml Because the Istio Ingress Gateway is an Envoy Proxy you can inspect it using the admin routes. First find the name of the istio-ingressgateway:\nkubectl get pods -n istio-system Copy and paste your ingress gateway’s pod name. Execute:\nkubectl -n istio-system exec -it \u003cistio-ingressgateway-...\u003e bash You can view the statistics, listeners, routes, clusters and server info for the Envoy proxy by forwarding the local port:\ncurl localhost:15000/help curl localhost:15000/stats curl localhost:15000/listeners curl localhost:15000/clusters curl localhost:15000/server_info See the admin docs for more details.\nAlso it can be helpful to look at the log files of the Istio ingress controller to see what request is being routed.\nBefore we check the logs, let us get out of the container back on the host:\nexit Now let us find the ingress pod and output the log:\nkubectl logs istio-ingressgateway-... -n istio-system View Istio Ingress Gateway for Bookinfo 🔗 View the Gateway and VirtualServices 🔗 Check the created Istio Gateway and Istio VirtualService to see the changes deployed:\nkubectl get gateway kubectl get gateway -o yaml kubectl get virtualservices kubectl get virtualservices -o yaml Find the external port of the Istio Ingress Gateway by running: 🔗 kubectl get service istio-ingressgateway -n istio-system -o wide To just get the first port of istio-ingressgateway service, we can run this:\nkubectl get service istio-ingressgateway -n istio-system --template='{{(index .spec.ports 1).nodePort}}' Create a DNS entry: 🔗 Modify you local /etc/hosts file to add an entry for your sample application.\n127.0.0.1. bookinfo.meshery.io\nThe HTTP port is usually 31380.\nOr run these commands to retrieve the full URL:\necho \"http://$(kubectl get nodes --selector=kubernetes.io/role!=master -o jsonpath={.items[0].status.addresses[?\\(@.type==\\\"InternalIP\\\"\\)].address}):$(kubectl get svc istio-ingressgateway -n istio-system -o jsonpath='{.spec.ports[1].nodePort}')/productpage\" Docker Desktop users please use http://localhost/productpage to access product page in your browser.\nIn case you are using a managed kubernetes cluster like AKS, EKS, or GCE please follow the procedure described below:\nGet the external IP of the service istio-ingressgateway using the following command:\nkubectl get service istio-ingressgateway -n istio-system Using Meshery, navigate to the Custom yaml page, and apply the manifest given below to allow all hosts instead of allowing bookinfo.meshery.io only and you are good to access the page using the following url http://\u003cexternal-ip of istio-ingressgateway\u003e/productpage.\napiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: bookinfo spec: gateways: - sample-app-gateway hosts: - \"*\" http: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 Apply default destination rules 🔗 Before we start playing with Istio’s traffic management capabilities we need to define the available versions of the deployed services. They are called subsets, in destination rules.\nUsing Meshery, navigate to the Custom yaml page, and apply the below to create the subsets for BookInfo:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage subsets: - name: v1 labels: version: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: ratings spec: host: ratings subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v2-mysql labels: version: v2-mysql - name: v2-mysql-vm labels: version: v2-mysql-vm --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: details spec: host: details subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 This creates destination rules for each of the BookInfo services and defines version subsets\nIn a few seconds we should be able to verify the destination rules created by using the command below:\nkubectl get destinationrules kubectl get destinationrules -o yaml Browse to BookInfo 🔗 Browse to the website of the Bookinfo. To view the product page, you will have to append /productpage to the url.\nReload Page 🔗 Now, reload the page multiple times and notice how it round robins between v1, v2 and v3 of the reviews service.\nInspect the Istio proxy of the productpage pod 🔗 To better understand the istio proxy, let’s inspect the details. Let us exec into the productpage pod to find the proxy details. To do so we need to first find the full pod name and then exec into the istio-proxy container:\nkubectl get pods kubectl exec -it productpage-v1-... -c istio-proxy sh Once in the container look at some of the envoy proxy details by inspecting it’s config file:\nps aux ls -l /etc/istio/proxy cat /etc/istio/proxy/envoy-rev0.json For more details on envoy proxy please check out their admin docs.\nAs a last step, lets exit the container:\nexit Alternative: Manual installation 🔗 Follow this if the above steps did not work for you\nDefault destination rules 🔗 Run the following command to create default destination rules for the Bookinfo services:\nkubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml Configure the Bookinfo route with the Istio Ingress gateway 🔗 We can create a virtualservice \u0026 gateway for bookinfo app in the ingress gateway by running the following:\nkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml ","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/introduction-to-service-meshes/expose-services/","tags":"","title":"Exposing services through Istio Ingress Gateway"},{"body":" Overview 🔗 The average age of our server fleet is over three years old, and we have a program in place to repurpose older servers for non-customer-facing workloads to keep them in use as long as they are practical.\nBenefits 🔗 Extended Server Fleet Life refers to extending the lifespan of servers in a data center beyond their original expected lifespan. This is achieved by implementing regular maintenance, upgrades, and optimization measures to ensure that the servers continue operating efficiently and effectively.\nBy extending the life of servers, organizations can save money on purchasing new servers and reduce the environmental impact of disposing of old servers. It also helps to maintain a stable and consistent IT infrastructure, as there is less disruption caused by replacing servers.\nHowever, it is essential to note that extending the lifespan of servers requires careful planning and management to ensure that they remain secure and reliable. Regular security updates and patches must be applied, and any hardware failures or issues must be addressed promptly to prevent downtime and data loss.\n","categories":"","description":"","excerpt":" Overview 🔗 The average age of our server fleet is over three years …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/sustainable-cloud/content/server-fleet/","tags":"","title":"Extended Server Fleet Life"},{"body":" Features 🔗 Kubernetes provides the following features 🔗 self-healing 🔗 Kubernetes restarts containers that fail, replaces containers, kills containers that don’t respond to your user-defined health check, and doesn’t advertise them to clients until they are ready to serve.\nautomatic bin packing 🔗 you provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and RAM each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources.\nautomated rollouts and rollbacks 🔗 you can describe the desired state for your deployed containers using Kubernetes. It can change the actual state to the desired state at a controlled rate; e.g. you can automate Kubernetes to create new containers, remove existing containers and adopt all their resources to the new container.\nsecret and configuration management 🔗 Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images without exposing secrets in your setup.\nservice discovery and load balancing 🔗 Kubernetes can expose a container using the DNS name or using their IP address. If traffic to a container is high, Kubernetes can load balance and distribute the network traffic to stabilize the deployment.\nstorage orchestration 🔗 Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more.\n","categories":"","description":"","excerpt":" Features 🔗 Kubernetes provides the following features 🔗 self-healing …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-details/content/features/","tags":"","title":"Features"},{"body":"","categories":"","description":"Application management","excerpt":"Application management","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/helm/","tags":"","title":"Helm"},{"body":" Imperative vs Declarative 🔗 Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services that facilitates both automation and declarative configuration.\"\nConfiguration Differences 🔗 What a difference a configuration makes :).\nThe significant change in this new IT world is that well-trained and practiced processes are outdated and useless. The imperative was the old world, you defined step-by-step guidance, and you also executed or oversaw the execution of those configuration steps. The declarative way is to determine the desired state and an intelligent system conducts and supervises the configuration and operation steps.\n","categories":"","description":"","excerpt":" Imperative vs Declarative 🔗 Kubernetes is a portable, extensible, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/kubernetes/content/imperative-vs-declarative/","tags":"","title":"Imperative vs Declarative"},{"body":" Ingress Config 🔗 How to configure an Ingress? The Ingress service consists of two parts, first, the Ingress Controller and second, the Ingress Configuration (called Ingress in Kubernetes). Kubernetes is reading the configuration and deploying it to the controller.\nThe video below shows the flow of the configuration process.\nIngress Config 🔗 Video: Ingress Config Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Ingress Config 🔗 How to configure an Ingress? The Ingress service …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/ingress-config/","tags":"","title":"Ingress Config"},{"body":" Exploring the Relationships Between Edge Stack Resources with a Kanvas Design 🔗 Services and Deployments\nThe design below shows the traffic flow between some major components in the Ambassador Edge Stack (AES) system.\nThe components include:\nedge-stack-agent Deployment edge-stack Deployment edge-stack-admin Service and edge-stack Service Let’s take a look at the roles of each component and the ports used for communication.\nedge-stack Service: This serves as the primary entry point for incoming traffic. It listens on ports 80 and 443, handling HTTP and HTTPS traffic respectively. This component routes the incoming requests to the appropriate internal services within the AES system.\nedge-stack-agent: This is responsible for specific tasks within the AES system. It receives traffic from the edge-stack service on port 80/TCP. The agent handles various operational tasks, including diagnostics and reporting to the Ambassador Cloud.\nedge-stack-admin Service: This Service provides administrative functions and health checks for the AES system. It communicates with the edge-stack component on port 8877/TCP for administrative purposes.\nedge-stack Deployment: The edge-stack Deployment component is a core part of the Ambassador Edge Stack, handling the main processing and routing of traffic. It receives traffic from the edge-stack service on port 80/TCP and communicates with the edge-stack-admin component on port 8877/TCP for administrative tasks.\nService Account Roles 🔗 The diagram above shows one of the role assignments and service account relationships within the Ambassador Edge Stack (AES) system. You can see that the Service Account (edge-stack) is linked to both the ClusterRole (edge-stack) and the Role (edge-stack-apiext) through ClusterRoleBinding and RoleBinding.\nWith the help of Kanvas, these connections become clear and easy to understand.\n","categories":"","description":"This chapter explores the relationships between different components in the Ambassador Edge Stack (AES) system using a Kanvas design. It covers the roles and communication ports of each component, as well as the service account roles and relationships within the AES system.","excerpt":"This chapter explores the relationships between different components …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/ambassador-edge-stack-api-gateway-with-meshery/edge/design-interpretation/","tags":"","title":"Interpreting the Edge Stack Meshery Design"},{"body":"","categories":"","description":"Are you looking for an introduction to Exoscale's data center processes and compliance topics? This INTRO Compliance Learning Path is perfect for non-technical individuals and covers the ground of all related topics. You'll learn about the benefits and challenges of compliance, sustainability, technical security, contractual setup, and response \u0026 support in modern IT scenarios, understand key concepts and terminology, and discover why these new rules are so important. ","excerpt":"Are you looking for an introduction to Exoscale's data center …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/","tags":"","title":"Intro Compliance"},{"body":"","categories":"","description":"This INTRO DBaaS - Learning Paths covers the foundational topics of DBaaS for a non-technical audience and conveys the benefits of data services and databases as a service for modern IT scenarios. It will help you learn the basics of terminology associated, understand the essential components' functions, and why these new technologies are so important.","excerpt":"This INTRO DBaaS - Learning Paths covers the foundational topics of …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/","tags":"","title":"INTRO DBaaS"},{"body":"","categories":"","description":"This section provides an introduction to Kubernetes, its architecture, and how it is used to manage containerized applications at scale.","excerpt":"This section provides an introduction to Kubernetes, its architecture, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/kubernetes/","tags":"","title":"Kubernetes"},{"body":" Kubernetes is… 🔗 … providing the building blocks for creating developer and infrastructure platforms but preserves user choice and flexibility where it is essential.\n… extensible, and lets users integrate their logging, monitoring, alerting, and many more solutions because it is not monolithic, and these solutions are optional and pluggable.\nKubernetes does NOT … 🔗 … limit the types of applications supported.\nKubernetes aims to support a highly diverse workload, including stateless, stateful, and data-processing workloads. If an application can run in a container, it should run great on Kubernetes.\n… deploy source code and does not build your application.\nOrganizational cultures determine Continuous Integration, Delivery, and Deployment (CI/CD) workflows and preferences and technical requirements.\n… provide application-level services;\nsuch as middleware, data-processing frameworks, databases, caches, nor cluster storage systems as built-in services. Application access through portable mechanisms the components mentioned above - both are running Kubernetes.\n… provide nor adopt any comprehensive machine management.\nThe task requires additional components for system configuration, system management \u0026 maintenance, etc…\nKubernetes is NOT … 🔗 … a traditional, all-inclusive PaaS system.\nKubernetes operates at the container level rather than at the hardware level. It provides some generally helpful features common to PaaS offerings, such as deployment, scaling, load balancing.\n… a mere orchestration system.\nIt eliminates the need for orchestration. The definition of orchestration is executing a defined workflow:\nfirst, do A, then B, then C → imperative. Kubernetes comprises independent, composable control processes that continuously drive the current state:\ntowards the desired state → declarative. ","categories":"","description":"","excerpt":" Kubernetes is… 🔗 … providing the building blocks for creating …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-details/content/kubernetes-is/","tags":"","title":"Kubernetes ..."},{"body":"","categories":"","description":"Explore the intricate details of Kubernetes, its architecture, components, and how it orchestrates containerized applications effectively.","excerpt":"Explore the intricate details of Kubernetes, its architecture, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-details/","tags":"","title":"Kubernetes Details"},{"body":"","categories":"","description":"Learn how to setup a service mesh and build an application","excerpt":"Learn how to setup a service mesh and build an application","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/","tags":"","title":"Mastering Service Meshes for Developers"},{"body":" Explained 🔗 As our reliance on technology grows, so does our need for efficient and cost-effective data storage solutions. We’ll explore two popular storage technologies, object storage and block storage, their fundamental differences, and their strengths and weaknesses so that you can choose the right solution for your organization. Object vs Block: Cost\nCost is a crucial factor in data storage, and both object and block storage solutions have their cost considerations.\nCost Efficiency: Object storage is generally more cost-efficient than block storage, primarily due to its scalability and cost-effectiveness at scale. As the volume of data increases, object storage’s flat address space and erasure coding techniques can provide substantial cost savings compared to block storage. Access Costs: One potential drawback of object storage is access costs. Retrieving data from object storage can be more expensive than block storage due to the additional overhead associated with the retrieval process. On the other hand, block storage provides direct access to data and typically incurs lower access costs. Hardware Costs: Large-scale block storage often requires specialized hardware, such as Fiber Channel switches, which can be expensive to implement and maintain. On the other hand, object storage can use commodity hardware, making it a more cost-effective option for smaller organizations or those with limited budgets. Object vs Block: Selection 🔗 Choosing between object and block storage ultimately depends on your specific data storage and management requirements. Object storage is ideal for storing unstructured data such as multimedia files, backups, and archives. It’s also well-suited for distributed data storage, analytics, and big data applications. Block storage is ideal for storing structured data such as databases and virtual machines. It’s also well-suited for high-performance applications that require low latency and high throughput.\nWorkload Consideration: Consider the workload for storing data. Applications that require high-speed data access may benefit from block storage. In contrast, those with large data volumes may benefit more from object storage’s scalability and cost-effectiveness. Cost Evaluation: Object storage is generally more cost-effective than block storage, particularly at scale. However, retrieval costs can be higher with object storage, so evaluating the total cost of ownership for each storage solution is essential. Evaluate Scalability: Object storage’s scalability and elasticity make it an ideal solution for distributed data storage. In contrast, block storage may be more limiting in terms of scalability. Evaluate Performance: Block storage typically offers lower latency and higher performance for applications that require rapid data access. In contrast, object storage can provide higher throughput for large data access requests. Object vs Block: Scalability 🔗 Scalability is critical in data storage, mainly as data volumes grow exponentially.\nScalability at Scale: Object storage’s flat address space and erasure coding techniques make it highly scalable, even at petabyte-scale data volumes. Block storage’s reliance on file systems and hierarchical structures can limit scalability, particularly as data volumes increase. Elasticity: Object storage’s scalability extends to elasticity, allowing organizations to adjust storage capacity on-demand to meet changing data storage requirements. Block storage’s scalability is typically more rigid, requiring more planning and management to scale effectively. Data Distribution: Object storage’s scalability and elasticity make it an ideal solution for distributed data storage. As organizations grow and expand, object storage can provide a cost-effective and efficient way to store data across multiple locations. Object vs Block: Performance 🔗 Performance is another crucial consideration in data storage, particularly for applications that require rapid data access.\nLatency: Block storage typically offers lower latency than object storage, making it better suited for applications that require rapid data access, such as databases and virtual machines. Object storage’s additional overhead can result in higher latency, particularly for small data access requests. Throughput: Object storage can provide higher throughput than block storage, particularly for large data access requests. Object storage’s erasure coding and data replication techniques allow for parallel data access, resulting in faster data transfers. Workload: Optimization Choosing the proper storage solution for your workload is critical in optimizing performance. Organizations that require high-speed data access may benefit from block storage. In contrast, those with large data volumes may benefit more from object storage’s scalability and cost-effectiveness. ","categories":"","description":"","excerpt":" Explained 🔗 As our reliance on technology grows, so does our need for …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/object-vs-block/content/object-vs-block/","tags":"","title":"Object vs Block"},{"body":" Persistent Volumes 🔗 If you want to run stateful workloads, temporary storage managed by volumes is not suitable for this application category. Running a database, for example, needs persistent volumes and storage technologies like, e.g., Block Storage, iSCSI, NFS.\nUnder no circumstances should you use non-persistent local storage types for this category of data applications. Kubernetes uses highly dynamic resource scheduling to optimize existing physical resources, which are provided via nodes. Pods get moved around, recycled, deleted, and with this, local data can get lost.\nPersistent Volumes 🔗 Video: Persistent Volumes Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Persistent Volumes 🔗 If you want to run stateful workloads, temporary …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/storage/content/persistence-volumes/","tags":"","title":"Persistent Volumes"},{"body":" Powered by Aiven 🔗 Our DBaaS is powered by Aiven, one of the leading European companies for managing Open Source data infrastructure in the cloud. The partnership offers Exoscale customers an integrated environment for their complete cloud infrastructure – without any security compromise. Both companies are GDPR-compliant to ensure the highest standards for customers’ data. The sole use of Open Source projects ensures that customers are not vendor locked-in and always at the latest technology standard. About Aiven 🔗 Fast Facts Aiven 🔗 aiven.io founded in 2016 team of 200 employees delivering bullet-proof data ops global team of open-source and cloud experts compliant: SOC2, ISO27000, GDPR, HIPPA \u0026 PCI-DSS ","categories":"","description":"","excerpt":" Powered by Aiven 🔗 Our DBaaS is powered by Aiven, one of the leading …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/exoscale-dbaas/content/powered-aiven/","tags":"","title":"Powered by Aiven"},{"body":"","categories":"","description":"Learn about Exoscale Products, their features, and how to use them effectively.","excerpt":"Learn about Exoscale Products, their features, and how to use them …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/products/","tags":"","title":"Products"},{"body":" REPLICASETs 🔗 A ReplicaSet’s purpose is to maintain a stable set of replica Pods running at any given time to guarantee the availability of a specified number of identical Pods. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods and other useful features. Therefore, Deployments are recommended instead of directly using ReplicaSets.\n","categories":"","description":"","excerpt":" REPLICASETs 🔗 A ReplicaSet’s purpose is to maintain a stable set of …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-building-blocks/content/replicaet/","tags":"","title":"REPLICASETs"},{"body":"","categories":"","description":"Learn and Practice Main Resources","excerpt":"Learn and Practice Main Resources","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/","tags":"","title":"Resources"},{"body":" What do I need … 🔗 Considering the next steps based on the Node.js sample app from before.\nWhat do I need to run apps in container? 🔗 Node.js – in the correct Version All dependencies of the App (e.g. via Package Manager NPM) Possibly … when the Application gets bigger … 🔗 Database (and its requirements …) Private Network Running Apps 🔗 Video: Running App Your browser does not support the video tag. ","categories":"","description":"","excerpt":" What do I need … 🔗 Considering the next steps based on the Node.js …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/containers/content/running-application/","tags":"","title":"Running Applications"},{"body":"","categories":"","description":"This  SKS ADVANCED - Learning Paths covers the deeper technical topics of (managed) Kubernetes for an expert audience and conveys the benefits of containers and container orchestration for modern IT scenarios. It will help you learn how to leverage this new technology, use the terminology associated, understand the components and functions, and why these new technologies are so important.","excerpt":"This  SKS ADVANCED - Learning Paths covers the deeper technical topics …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/","tags":"","title":"SKS ADVANCED"},{"body":" Framework 🔗 TISAX (Trusted Information Security Assessment Exchange) 🔗 Is a certification designed for the automotive industry’s assessment and exchange mechanism of information security.\nTISAX stands for Trusted Information Security Assessment Exchange. It is a framework for information security assessments developed by the German Association of the Automotive Industry (VDA) to ensure the security of shared data among automotive industry suppliers.\nTISAX is based on the ISO/IEC 27001 standard for information security management systems and is designed to ensure the confidentiality, integrity, and availability of information exchanged between automotive industry partners. TISAX assessments are conducted by accredited third-party auditors who evaluate a company’s information security management system against defined criteria.\nTISAX assessments cover various information security topics, including access control, data protection, incident management, business continuity, and physical security. Companies that complete a TISAX assessment are granted a TISAX certificate, demonstrating that they meet the information security requirements of the automotive industry.\nTISAX certification is becoming increasingly important for companies that supply products or services to the automotive industry, as it demonstrates a commitment to information security and provides a competitive advantage in the marketplace.\nHDS (Health Data Hosting) 🔗 Is a certification designed to reinforce personal health data protection and build a trustworthy environment around eHealth and patient monitoring.\nHDS, or Health Data Hosting, is a French legal framework regulating personal health data storage and processing. It applies to any company or organization that provides data hosting services for healthcare data in France.\nUnder the HDS framework, healthcare data must be stored and processed in compliance with strict security and privacy requirements. These include:\nPhysical security: HDS requires that data centers be equipped with adequate physical security measures, such as access controls, surveillance cameras, and fire suppression systems.\nTechnical security: HDS requires that data be encrypted at rest and in transit and that access to the data be restricted to authorized personnel.\nOrganizational security: HDS requires that data hosting providers implement various policies and procedures to ensure healthcare data’s confidentiality, integrity, and availability.\nPrivacy: HDS requires that data hosting providers obtain explicit consent from patients to collect, store, and process their health data and comply with all applicable data protection laws and regulations.\nFailure to comply with HDS requirements can result in significant penalties and fines. As such, healthcare data hosting providers in France must take HDS compliance very seriously and implement robust security and privacy measures to protect personal health data.\nFINMA (Swiss Financial Market Supervisory Authority) 🔗 Is a government financial regulation for supervising banks, insurance companies, stock exchanges, securities dealers, and other financial intermediaries in Switzerland.\nFINMA stands for Swiss Financial Market Supervisory Authority. It is an independent regulatory body responsible for supervising and regulating financial institutions and markets in Switzerland. FINMA was established in 2009 as part of a comprehensive Swiss financial market supervisory system reform.\nFINMA’s primary mission is to ensure the stability and integrity of the Swiss financial system, protect investors and consumers, and combat financial crime. It supervises and regulates banks, insurance companies, securities dealers, asset managers, and other financial intermediaries operating in Switzerland.\nFINMA has the authority to issue regulations, conduct investigations, and impose sanctions on financial institutions and individuals who violate Swiss financial laws and regulations. It also works closely with other national and international regulatory bodies to promote financial stability and combat cross-border financial crime.\nFINMA plays a critical role in maintaining the integrity and stability of the Swiss financial system and ensuring that financial institutions and markets operate safely and soundly.\n","categories":"","description":"","excerpt":" Framework 🔗 TISAX (Trusted Information Security Assessment Exchange) …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/exoscale-compliance/content/spc-compliance/","tags":"","title":"Specific Compliance"},{"body":"","categories":"","description":"Learn how to manage storage in Kubernetes, including persistent volumes and storage classes.","excerpt":"Learn how to manage storage in Kubernetes, including persistent …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/storage/","tags":"","title":"Storage"},{"body":" Object Storage 🔗 Exoscale’s Simple Object Storage (SOS) is a scalable and cost-effective solution for storing and managing large amounts of data. It offers highly available multi-redundancy storage, ensuring data safety and accessibility. You can store various files and objects, such as assets, backups, and media files. Your data remains in the exact location you store it, and Exoscale replicates it in at least three physical copies for maximum safety.\nFeatures Overview:\nS3 compatible Direct HTTP/S access Metadata support ACL and CORS support For any data Pay for what you use Free inbound traffic The S3-compatible API allows for easy integration with existing workflows and applications. SOS provides low latency, high bandwidth, and secure HTTP(s) access, allowing fast and secure data management from any location. You can enhance this with Exoscale’s CDN integration.\nNOTE! Here, you can find all the details in the online documentation for STORAGE.\nCDN 🔗 Exoscale’s CDN service, developed with Ducksify, makes distributing your assets globally with Akamai’s delivery network simple. It improves performance and user experience by caching assets in multiple locations. You can easily integrate it with our SOS service to make content available through the CDN endpoint.\nFeatures Overview:\nModern protocol support World-class delivery availability Improved download completion rates Leveraging the Akamai Intelligent Platform QUIC (Quick UDP Internet Connections) support Enable on your SOS bucket Volume-based pricing Powered by Ducksify The CDN offers predictable pricing and is a reliable solution for enhancing your application’s performance. NOTE! Here, you can find all the details in the online documentation for CDN.\n","categories":"","description":"","excerpt":" Object Storage 🔗 Exoscale’s Simple Object Storage (SOS) is a scalable …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/products/content/storage/","tags":"","title":"Storage"},{"body":" Overview 🔗 Criteria Description Performance Storage read and write operations access speed Scalability Increase or decrease the amount of storage Redundancy Replicating data across multiple storage devices Retention Period of time that data is stored and maintained Capacity Amount of data that can be stored Archive Service that is designed for long-term retention Costs Expenses associated with storing data ","categories":"","description":"","excerpt":" Overview 🔗 Criteria Description Performance Storage read and write …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/why-storage/content/overview/","tags":"","title":"Storage Criteria"},{"body":" Explained 🔗 Storage Duration or Data Retention refers to the period of time that data is stored and maintained in a system or storage medium. It represents the duration for which data is preserved and remains accessible for retrieval or reference.\nData retention policies are typically defined by organizations to determine how long different types of data should be retained. These policies are influenced by various factors, including legal requirements, industry regulations, business needs, and data management best practices.\nThe purpose of data retention is to ensure that data is available for future use, analysis, compliance, or reference. It helps organizations meet legal and regulatory obligations, support business operations, facilitate data analysis and reporting, and address potential disputes or investigations.\nData retention periods can vary depending on the type of data and the specific requirements of the organization or industry. For example, financial records may need to be retained for a certain number of years, while customer data may have different retention requirements.\nIt is important for organizations to establish clear data retention policies and procedures to ensure compliance, data integrity, and efficient data management throughout the data lifecycle.\n","categories":"","description":"","excerpt":" Explained 🔗 Storage Duration or Data Retention refers to the period …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/why-storage/content/storage-duration/","tags":"","title":"Storage Duration"},{"body":" Definition 🔗 When it comes to storage in the cloud, there are three main types:\nFile Storage Object Storage Block Storage Each type has its unique characteristics and use cases:\nType Use Case File Storage Suitable for structured file-based data Object Storage Ideal for unstructured data and cloud-native applications Block Storage Provides low-level access for high-performance applications The choice between these storage types depends on your specific requirements and the nature of the data you need to store in the cloud.\nLet’s take a closer look at each of them.\nFiles Storage 🔗 File storage is designed to store and manage files in a hierarchical structure, similar to how files are organized in a traditional file system. It provides a shared file system that can be accessed by multiple users or applications simultaneously. File storage is suitable for scenarios where you need to store and access files in a structured manner, such as shared drives, file sharing, or hosting web content.\nObject Storage 🔗 Object storage, on the other hand, is a storage architecture that manages data as objects. Each object consists of data, metadata (attributes or properties associated with the object), and a unique identifier. Object storage is highly scalable and offers virtually unlimited storage capacity. It is ideal for storing unstructured data like images, videos, documents, backups, and logs. Object storage is accessed using APIs, making it suitable for cloud-native applications and distributed systems.\nBlock Storage 🔗 Block storage works at the lowest storage level. It provides raw storage volumes that can be mounted as block devices by virtual machines or servers. It offers high-performance storage with low latency and is often used for databases, virtual machines, and applications that require direct access to storage at the block level. The operating system manages block storage and requires a file system to organize and manage data.\n","categories":"","description":"","excerpt":" Definition 🔗 When it comes to storage in the cloud, there are three …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/what-types-of-storage/content/types-of-storage/","tags":"","title":"Storage Types"},{"body":" Overview 🔗 These are just a few examples of the many uses of block storage. Its versatility and scalability make it a crucial component in various IT environments, enabling efficient data storage and management for multiple applications and workloads. Block storage has many uses in cloud computing and other IT environments.\nHere are some example use cases for block storage:\nDatabases: Block storage is often used to store databases due to its high-performance capabilities. It provides the necessary speed and reliability for database operations, ensuring efficient data access and retrieval. Virtual Machines: Block storage commonly stores virtual machine (VM) images and data. It allows for easy provisioning and management of VM storage, enabling organizations to scale their virtualized infrastructure as needed. Applications: Many applications require persistent storage for their data. Block storage provides a reliable and scalable solution for storing application data, ensuring it is readily accessible and protected against data loss. Backup and Disaster Recovery: Block storage is often used for backup and disaster recovery purposes. It allows organizations to create snapshots or replicas of their data, providing a reliable and efficient way to restore data in case of data loss or system failures. Big Data Analytics: Block storage is used in big data analytics environments to store and process large volumes of data. It provides the necessary performance and scalability to handle the massive amounts of data generated by analytics workloads. Content Delivery: Block storage is used in content delivery networks (CDNs) to store and deliver static content, such as images, videos, and files. It ensures fast and reliable content delivery to end-users by caching and distributing it across multiple servers. High-Performance Computing: Block storage is commonly used in high-performance computing (HPC) environments to store and process large datasets. It provides the speed and reliability required for complex scientific calculations and simulations. ","categories":"","description":"","excerpt":" Overview 🔗 These are just a few examples of the many uses of block …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/block-storage/content/use-cases/","tags":"","title":"Use Cases - Block Storage"},{"body":" Overview 🔗 Cloud file storage provides the flexibility to support and integrate with existing applications, plus the ease of deploying, managing, and maintaining all your files in the cloud. These two key advantages allow organizations to support various applications and verticals. Use cases such as large content repositories, development environments, media stores, and user home directories are ideal workloads for cloud-based file storage.\nHere are some example use cases for file storage:\nWeb Serving: The need for shared file storage for web-serving applications can be challenging when integrating backend applications. Typically, multiple web servers deliver a website’s content, each needing access to the same set of files. Since cloud file storage solutions adhere to standard file-level protocols, file naming conventions, and permissions that web developers are accustomed to, cloud file storage can be integrated into your web applications. Content Management: A content management system (CMS) requires a common namespace and access to a file system hierarchy. Like web-serving use cases, CMS environments typically have multiple servers that need access to the same set of files to serve up content. Since cloud file storage solutions adhere to the expected file system semantics, file naming conventions, and permissions that developers are accustomed to, storing documents and other files can be integrated into existing CMS workflows. Analytics: Analytics can require massive amounts of data storage that can scale further to keep up with growth. This storage must also provide the performance necessary to deliver data to analytics tools. Many analytics workloads interact with data through a file interface, rely on features like file locks, and require the ability to write to portions of a file. Since cloud-based file storage supports standard file-level protocols and can scale capacity and performance, it is ideal for delivering a file-sharing solution that is easy to integrate into existing big data and analytics workflows. Media and Entertainment: Digital media and entertainment workflows are constantly changing. Many businesses use a hybrid cloud deployment and need standardized access using file system protocols (NFS or SMB) or concurrent protocol access. These workflows require flexible, consistent, and secure access to data from off-the-shelf, custom-built, and partner solutions. Since cloud file storage adheres to existing file system semantics, storage of rich media content for processing and collaboration can be integrated for content production, digital supply chains, media streaming, broadcast play-out, analytics, and archives. Home Directories: The use of home directories for storing files only accessible by specific users and groups can be beneficial for many cloud workflows. Businesses looking to take advantage of the scalability and cost benefits of the cloud are extending access to home directories for many of their users. Since cloud file storage systems adhere to common file-level protocols and standard permissions models, customers can lift and shift applications to the cloud that need this capability. Database Backups: Backing up data using existing mechanisms, software, and semantics can create an isolated disaster recovery scenario with little locational flexibility for recovery. Many businesses want to take advantage of the flexibility of storing database backups in the cloud, either for temporary protection of previous versions during updates or for development and testing. Since cloud file storage presents a standard file system that can be mounted from database servers, it can be an ideal platform for creating portable database backups using native application tools or enterprise backup applications. Development Tools: Development environments can be challenged to share unstructured data safely and securely as they collaborate to develop their latest innovations. With the need to share code and other files in an organized way, using shared cloud file storage provides an organized and secure repository accessible within their cloud development environments. Cloud-based file storage delivers a scalable and highly available solution ideal for collaboration. End User Computing: End User Computing (EUC) is a combination of technologies that gives your employees secure, remote access to applications, desktops, and data they need to complete their work. Modern enterprises use EUC so their employees can work from wherever they are, across multiple devices, in a safe and scalable way. EUC technologies like persistent desktops and document management systems require secure, reliable, and scalable file storage systems. ","categories":"","description":"","excerpt":" Overview 🔗 Cloud file storage provides the flexibility to support and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/file-storage/content/use-cases/","tags":"","title":"Use Cases - File Storage"},{"body":" Overview 🔗 These are just a few examples of the many uses of object storage. Its scalability, durability, and cost-effectiveness make it a popular choice for storing and managing large volumes of unstructured data in various industries and applications. Object storage is a versatile solution that finds applications in multiple industries and use cases.\nHere are some example use cases for object storage:\nCloud Storage: Object storage is the foundation of many cloud storage services, allowing users to store and retrieve data over the Internet. It provides scalable and durable storage for files, documents, images, videos, and other unstructured data types. Backup and Archiving: Object storage is well-suited for backup and long-term data retention. Its ability to handle massive scale and durability makes it ideal for storing backups, archives, and historical data that must be retained for compliance or regulatory purposes. Content Distribution: Object storage is used in content delivery networks (CDNs) to efficiently distribute and deliver content to end-users. Object storage enables fast and reliable content delivery by caching and replicating objects across multiple servers in different geographic locations, reducing latency and improving user experience. Data Lakes and Analytics: Object storage is often used as a data lake for storing large volumes of structured and unstructured data. Data lakes are a centralized repository for data used in analytics, machine learning, and data mining. Object storage’s scalability and cost-effectiveness make it an ideal choice for storing and processing big data. Internet of Things: Object storage is used in Internet of Things (IoT) applications to store and manage the vast amount of data generated by connected devices. IoT devices generate a continuous stream of data, and object storage provides a scalable and reliable solution for storing and analyzing this data. Collaboration and File Sharing: Object storage is used in collaboration platforms and file-sharing services to store and share files among users. It allows multiple users to access and collaborate on files simultaneously, providing a centralized and scalable storage solution. Media and Entertainment: Object storage is widely used in the media and entertainment industry for storing and managing large media files, such as videos, images, and audio. It provides the necessary scalability and performance to handle the demands of media production, distribution, and streaming. ","categories":"","description":"","excerpt":" Overview 🔗 These are just a few examples of the many uses of object …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/object-storage/content/use-cases/","tags":"","title":"Use Cases - Object Storage"},{"body":" Value Chain of Data 🔗 “Like water, data needs to be accessible, it needs to be clean, and it is needed to survive.” - Dan Vesset (IDC)\nWe looked at the importance of data and want to briefly cover the last introduction aspect relevant to our database and database as a service journey. If you break down the diagram above, you see two significant aspects of data interactions. Production of data and usage of data, those two categories are further divided into collection and publication on the left-hand side and uptake and impact on the right-hand side.\nRelevance? Think about the tools and the required features to address those tasks effectively and efficiently, keep on thinking, and factor in scalability, availability, and performance. These are all traits of good data handling tools, and there are prominent examples of very successful companies out there who mastered that data value endeavor perfectly.\nLook at Uber and Airbnb and how they successfully challenged their respective industries with an approach based on data, software solutions, and a customer-first mindset focused on convenience. And convenience always wins.\n","categories":"","description":"","excerpt":" Value Chain of Data 🔗 “Like water, data needs to be accessible, it …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/databases/content/value-chain/","tags":"","title":"Value Chain of Data"},{"body":"","categories":"","description":"An overview of the main cloud storage types block, file, and object and how they support diverse workloads.","excerpt":"An overview of the main cloud storage types block, file, and object …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/what-types-of-storage/","tags":"","title":"What Types of Storage?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/exoscale-dbaas/","tags":"","title":"Why Exoscale DBaaS?"},{"body":"Create and manage Pods, Deployments, and other workload resources.\nThis section is a refresher that provides an overview of the primary Kubernetes resources related to workloads. At the end of this section, please complete the exercises to put these concepts into practice.\nPod 🔗 Purpose 🔗 A Pod is the smallest workload unit in Kubernetes. It’s an abstraction containing one or more containers in charge of running applications.\nSample specification 🔗 Below is the simplest version of a Pod, this one runs a container based on the stefanprodan/podinfo image.\napiVersion: v1 kind: Pod metadata: name: podinfo spec: containers: - name: podinfo image: stefanprodan/podinfo We create the Pod with the usual kubectl command from this YAML definition.\nkubectl apply -f pod.yaml As a Pod does not expose the application it is running (this is the role of the Service resource, which we’ll detail later in this workshop), we can access the application using a port-forward command as follows.\nkubectl port-forward podinfo 9898:9898 --address 0.0.0.0 This command opens port 9898 on the machine it is run from and forwards traffic to port 9898 in the Pod. The –address 0.0.0.0 flag ensures this port is available on all the network interfaces of the host machine (otherwise limited to localhost).\nEnhanced specification 🔗 The simple specification we saw above is too simple and must not be run in a production environment. Below is a more complete specification, including additional properties:\nresources, which specifies the requirements and limits in terms of CPU and RAM livenessProbe, which ensures the application is healthy readinessProbe, which ensures the application is ready to accept requests securityContext, which adds security constraints apiVersion: v1 kind: Pod metadata: name: podinfo labels: app: podinfo spec: containers: - image: stefanprodan/podinfo:6.1.0 name: podinfo resources: requests: cpu: 50m memory: 64Mi limits: cpu: 50m memory: 64Mi livenessProbe: httpGet: path: /healthz port: 9898 initialDelaySeconds: 3 periodSeconds: 3 readinessProbe: httpGet: path: /readyz port: 9898 initialDelaySeconds: 3 periodSeconds: 3 securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 10000 runAsNonRoot: true seccompProfile: type: RuntimeDefault capabilities: drop: - ALL Deployment 🔗 Purpose 🔗 A Deployment runs a given number of identical Pods across the cluster. The number of replicas can easily be scaled (manually or with an HPA).\nSample specification 🔗 The following specification defines a Deployment in charge of 5 Pods based on the nginx:1.24 image.\napiVersion: apps/v1 kind: Deployment metadata: name: www spec: replicas: 5 selector: matchLabels: app: www template: metadata: labels: app: www spec: containers: - name: www image: nginx:1.24 ports: - containerPort: 80 DaemonSet 🔗 Purpose 🔗 A DaemonSet ensures a Pod (usually an agent) is running on each cluster’s Node.\nSample specification 🔗 The following specification defines a DaemonSet in charge of running a fluentbit Pod on each node of the cluster.\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluent-bit spec: selector: matchLabels: k8s-app: fluent-bit-logging template: metadata: labels: k8s-app: fluent-bit-logging spec: containers: - name: fluent-bit image: fluent/fluent-bit:1.5 volumeMounts: - name: varlog mountPath: /var/log - name: varlogcontainers mountPath: /var/log/containers volumes: - name: varlog hostPath: path: /var/log - name: varlogcontainers hostPath: path: /var/log/containers Job 🔗 Purpose 🔗 A Job allows running several Pods in parallel or sequence. This must not be used to run long-running tasks, like application servers.\nSample specification 🔗 The following specification defines a Job running 3 Pods in parallel to train a machine learning model.\napiVersion: batch/v1 kind: Job metadata: name: training spec: completions: 3 parallelism: 3 template: spec: restartPolicy: OnFailure containers: - name: training image: org/ml-training:1.2 CronJob 🔗 Purpose 🔗 A CronJob is in charge of launching Jobs according to a schedule, similar to the Linux crontab.\nSample specification 🔗 The following specification defines a CronJob in charge of running a backup Job every 6 hours.\napiVersion: batch/v1 kind: CronJob metadata: name: dump spec: schedule: \"0 */6 * * *\" jobTemplate: spec: template: spec: restartPolicy: OnFailure containers: - name: dump image: org/db-dump:2.3 ConfigMap 🔗 Purpose 🔗 A ConfigMap is not used to run Pods, unlike the resources above. Instead, we use it to configure the application running in Pods. A ConfigMap contains files or key/value pairs that we provide to the containers either:\nvia environment variables or mounted as a volume Sample specification 🔗 The following specification defines a ConfigMap containing a nginx configuration file.\napiVersion: v1 kind: ConfigMap metadata: name: proxy-config data: nginx.conf: | user nginx; worker_processes 4; pid /run/nginx.pid; events { worker_connections 768; } http { server { listen *:80; location = /whoami { proxy_pass http://whoami/; } } } Usage in a Pod 🔗 The following specification illustrates how to use this ConfigMap in a Pod. The nginx.conf file is made available in the /etc/nginx/config.conf in the Pod’s container.\napiVersion: v1 kind: Pod metadata: name: www spec: containers: - name: proxy image: nginx:1.24 volumeMounts: - name: config mountPath: \"/etc/nginx/\" volumes: - name: config configMap: name: nginx-config Secret 🔗 Purpose 🔗 A Secret is very similar to a ConfigMap except that it is used to handle sensitive information (credentials, ssh keys) as it can be encrypted. As for ConfigMap, a Secret contains files or key/value pairs that we can provide to the containers either:\nvia environment variables or mounted as a volume When viewing a Secret’s specification, we can see base64 encoded (not encrypted) content.\nSample specification 🔗 The following specification defines a Secret containing an encoded MongoDB connection string.\napiVersion: v1 kind: Secret metadata: name: mongo-credentials data: mongo_url: dG9rZW49Y2IzNDU2YTU0RUI1Cg== type: Opaque Usage in a Pod 🔗 The specification below illustrates how to use a Secret in a Pod. The Secret’s unique key is available as an environment variable in the Pod’s container.\napiVersion: v1 kind: Pod metadata: name: api spec: containers: - name: api image: api:1.2 env: - name: MONGO_URL valueFrom: secretKeyRef: name: mongo-credentials key: mongo_url Namespaces 🔗 Namespaces are Kubernetes resources that allow grouping resources. The image below illustrates the Namespaces created by default.\nAs Namespaces do not offer strong isolation, specific resources must be applied to a Namespace to limit CPU, RAM usage, and network rules allowed within and across Namespaces. Those resources are:\nResourceQuota Limits NetworkPolicy ","categories":"","description":"Create and manage Pods, Deployments, and other workload resources.","excerpt":"Create and manage Pods, Deployments, and other workload resources.","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/workload/","tags":"","title":"Workloads"},{"body":" Add Labels 🔗 To make it easier to filter and manage your resources during visualization, add labels to all of them.\nYou can also choose to use the existing label, app:wordpress, but a new one, dev:tutorial, is recommended for this tutorial to prevent your resources from getting mixed up with others in the public playground cluster.\nClick on the label icon. Click the + sign next to Labels. Add the label dev:tutorial. Do this for all the resources on the canvas. Figure: Add label\nNow click on Save As and save the design. Group Components 🔗 The Group Components icon on the dock below allows you to group resources based on shared labels or annotations.\nThis functionality aids in visualizing the relationships between various resources, making it easier to manage them, troubleshoot issues, and understand the overall cluster architecture.\nFigure: Group Components\n","categories":"","description":"In this section you will add labels to components and learn how to group them.","excerpt":"In this section you will add labels to components and learn how to …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/deploying-wordpress-and-mysql-with-persistent-volumes-with-meshery/sql/group-components/","tags":"","title":"Add labels and Group Components"},{"body":" Cloud Automation 🔗 Start a virtual computer in seconds, and integrate current on-premises or hybrid-cloud deployments using standard DevOps tooling.\nAnsible, Terraform, Kubernetes, or the like.\nThe cloud computing service model of Exoscale is called Infrastructure as a Service.\n","categories":"","description":"","excerpt":" Cloud Automation 🔗 Start a virtual computer in seconds, and integrate …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/exoscale/content/benefits/","tags":"","title":"Benefits"},{"body":" Watch out! 🔗 Initial Costs Evolving Expectations Complex Measurement Initial Costs 🔗 Upfront Investment Sustainable Practices Potential Barrier Implementing sustainable practices often requires upfront Investment, which can be a barrier for some businesses.\nEvolving Expectations 🔗 Corporate Sustainability Societal Expectations Core Strategies Corporate sustainability is a dynamic approach that evolves to meet societal expectations. It ensures relevance by integrating sustainability into core strategies to mitigate risks and create value for the planet and its inhabitants.\nComplex Measurement 🔗 Sustainability Initiatives Measurement Challenge Quantifying Impact Quantifying the impact of sustainability initiatives, particularly social impacts, can be challenging. However, this measurement is crucial for businesses to understand the effectiveness of their efforts and make informed decisions for the future.\n","categories":"","description":"","excerpt":" Watch out! 🔗 Initial Costs Evolving Expectations Complex Measurement …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/corporate-sustainability/content/challenges/","tags":"","title":"Challenges"},{"body":"","categories":"","description":"Learn the fundamentals of cloud-native technologies, including Kubernetes, microservices, and containerization.","excerpt":"Learn the fundamentals of cloud-native technologies, including …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud-native/","tags":"","title":"Cloud-Native"},{"body":" Cluster Structure 🔗 Let’s have a look at the Kubernetes Cluster and its components. Although it is quite a complex structure, the beauty of SKS is that we manage the complexity. Kubernetes as a managed service remains a very flexible solution because the possibility of shaping the services with add-ons makes it very customizable.\nCluster Structure\" 🔗 Video: Cluster Structure Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Cluster Structure 🔗 Let’s have a look at the Kubernetes Cluster and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/cluster-structure/","tags":"","title":"Cluster Structure"},{"body":"In this tutorial, you explored the Ambassador Edge Stack (AES) system using Meshery Playground. By leveraging Kanvas, you were able to clearly visualize resource relationships, providing a view of the functionalities and roles within the AES system. You should now have a solid understanding of how to configure, deploy, and manage the Ambassador Edge Stack with Meshery.\nHere’s a recap of the lessons learned in this course:\nThe Ambassador Edge Stack (AES) system can be explored using Meshery Playground. Kanvas provides a clear visualization of resource relationships in the AES system. Configuring, deploying, and managing the Ambassador Edge Stack with Meshery is now well understood. By following this tutorial, you have gained a comprehensive understanding of the Ambassador Edge Stack and its integration with Meshery. Congratulations on completing the course!\n","categories":"","description":"Concluding this tutorial, you will gain a comprehensive understanding of how to configure, deploy, and manage the Ambassador Edge Stack with Meshery.","excerpt":"Concluding this tutorial, you will gain a comprehensive understanding …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/ambassador-edge-stack-api-gateway-with-meshery/edge/conclusion/","tags":"","title":"Conclusion"},{"body":" Summary 🔗 Cloud computing offers an array of opportunities for organizations to enhance their sustainability practices and reduce their environmental footprint. Companies must choose cloud solutions that align best with their sustainability goals!\nIncorporating cloud computing into the corporate sustainability strategy can offer significant environmental benefits, driving efficiencies, reducing waste, and minimizing carbon footprints.\nAs part of a broader sustainability framework, companies must choose cloud solutions that align with their sustainability goals, ensuring that their move to the cloud advances their commitment to building a more sustainable, resilient, and efficient business model.\n","categories":"","description":"","excerpt":" Summary 🔗 Cloud computing offers an array of opportunities for …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/cs-and-cloud/content/conclusion/","tags":"","title":"Conclusion"},{"body":" Explained 🔗 Containers are lightweight, portable, and self-contained software packages that include all the necessary components to run an application, such as code, libraries, and dependencies.\nContainers are isolated from the host operating system and other containers, allowing applications to run consistently across different environments and platforms. They are often used in cloud computing and DevOps environments to simplify application deployment, scaling, and management.\nContainers are a popular technology for deploying microservices. They provide a lightweight and efficient way to package and deploy individual services. By encapsulating each microservice in its container, businesses can achieve greater modularity and flexibility in their application architecture. Additionally, containers can be easily scaled up or down to match changing demand, making it easier to manage resources and optimize performance.\nHowever, there are also some potential drawbacks to using containers. For example, many containers can be complex and require additional tools and expertise. Additionally, containers can introduce security risks if not properly configured and managed. Despite these challenges, containers are becoming increasingly popular in modern software development. They provide a powerful and flexible way to deploy and manage microservices in cloud-native and DevOps environments.\n","categories":"","description":"","excerpt":" Explained 🔗 Containers are lightweight, portable, and self-contained …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud-native/content/container/","tags":"","title":"Container"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd-and-cloud/","tags":"","title":"CSRD \u0026 Cloud"},{"body":" Details 🔗 Exoscale’s end-to-end encrypted database as a service (DBaaS) offering is a powerful solution for businesses to host their data and databases in the cloud securely. With this service, users can start within minutes, making it easy to quickly deploy and manage their databases without any delays or downtime. In addition, Exoscale’s DBaaS offering is entirely GDPR-compliant, ensuring businesses can meet regulatory requirements and keep their data safe and secure. Furthermore, as a fully managed service, Exoscale takes care of all the maintenance and management of the databases, allowing users to focus on their core business activities.\nFeatures Overview:\nFull lifecycle management Termination protection Automatic backup policy Available in all zones Dedicated instances Finally, Exoscale’s DBaaS offering supports a wide range of open-source databases, allowing users to choose the best database and providing a robust and secure solution for businesses that host their data and databases in the cloud. DBaaS Overview 🔗 Managed PostgreSQL Service often referred to as Postgres is an advanced, open-source relational database management system (RDBMS). Renowned for its robustness, performance, and extensive feature set, it supports complex queries, transactions, and advanced data types. PostgreSQL is highly extensible and standards-compliant with SQL. Due to its reliability, data integrity, and concurrency features, it is widely used in various environments, from small-scale applications to large-scale enterprise systems. Additionally, it supports numerous programming languages and can handle massive amounts of data efficiently.\nManaged MySQL Service is a widely used, open-source relational database management system (RDBMS) known for its speed, reliability, and ease of use. MySQL, developed by Oracle Corporation, supports standard SQL and provides a powerful, flexible, scalable database management solution. It is commonly used for web applications, often in conjunction with PHP, due to its integration with various platforms and ability to handle large volumes of data efficiently. MySQL offers strong support for transactional processing, data replication, and security, making it a popular choice for developers and enterprises seeking robust database performance.\nManaged Kafka Service is an open-source stream-processing platform developed by the Apache Software Foundation. It is designed to build real-time data pipelines and streaming applications. Kafka efficiently handles high-throughput, low-latency data transfer and can process millions of messages per second. It operates as a distributed system that ensures fault tolerance and scalability. Kafka’s core components—producers, consumers, brokers, topics, and partitions—enable the reliable streaming and storage of data across various systems. It is widely used for log aggregation, event sourcing, real-time analytics, and integrating disparate systems.\nManaged OpenSearch Service is an open-source search and analytics engine derived initially from Elasticsearch and maintained by the OpenSearch community and Amazon Web Services (AWS). It provides capabilities for indexing, searching, and analyzing large volumes of data in real-time. OpenSearch is designed to be scalable, highly available, and secure, supporting full-text search, structured search, and complex data analysis. It includes OpenSearch Dashboards for data visualization, enabling users to create interactive charts, graphs, and dashboards. OpenSearch is widely used in log and event data analysis, monitoring, and business intelligence applications.\nManaged Caching Service (Redis compatible - Remote Dictionary Server) is an open-source, in-memory data structure store used as a database, cache, and message broker. Known for its high performance, Redis supports various data structures such as strings, lists, sets, hashes, and more. It offers sub-millisecond latency, making it ideal for real-time applications like caching, session management, and analytics. Exoscale for Caching includes features like replication, persistence, and clustering to ensure reliability and scalability. Its versatility and efficiency make it popular for developers aiming to improve application speed and responsiveness.\nManaged Grafana Service is an open-source analytics and monitoring platform that allows users to visualize, analyze, and alert on data from multiple sources. Known for its customizable and interactive dashboards, Grafana supports a wide range of data sources, including Prometheus, Graphite, InfluxDB, and Elasticsearch. It provides powerful query capabilities, real-time alerting, and flexible visualization options like graphs, heatmaps, and histograms. Commonly used for monitoring system performance, application metrics, and business KPIs, Grafana helps teams make data-driven decisions by providing clear, comprehensive insights into their data.\nNOTE! Here, you can find all the details in the online documentation for DBAAS.\n","categories":"","description":"","excerpt":" Details 🔗 Exoscale’s end-to-end encrypted database as a service …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/products/content/dbaas/","tags":"","title":"DBaaS"},{"body":" Installing Dependencies … 🔗 … on Ubuntu\n# Install repository $ curl -fsSL https://deb.nodesource.com/setup_current.x | sudo -E bash –sudo apt-get update # Install NodeJS $ sudo apt-get install -y nodejs # Install Dependencies of the app $ npm install Running … 🔗 … the application\n# Download or Upload app.js to the server somehow… # Run $ node app.js Dependencies 🔗 Video: Dependencies Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Installing Dependencies … 🔗 … on Ubuntu\n# Install repository $ curl …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/containers/content/dependencies/","tags":"","title":"Dealing with Dependencies"},{"body":" Create, Configure and Deploy Dapr Redis State Store Component 🔗 Following the deployment of the Redis state store, we will use Meshery to create and configure the Dapr state store component. This involves specifying essential details, including the redisHost and redisPassword fields in the Dapr component configuration, which tell Dapr where to find the Redis server and how to authenticate.\nThis setup ensures that Dapr connects to the correct Redis instance, allowing it to handle state management seamlessly without direct involvement from your application.\nSearch for Component 🔗 On the Dock at the bottom of the design canvas, click on Components. Search for “dapr”. Click on the drop-down and drag and drop component to the design canvas. This is the Dapr component custom resource we discussed when learning about the Dapr control plane. Configure State Store and Deploy 🔗 Now you can start configuring the Dapr state-store.\nClick on the component to open the configuration tab. Enter state-store as the name. Enter kubernetes in the Secret store field. Enter state.redis in the Type field. Enter v1 in the version field. Click on the metadata drop-down and begin to fill in the info. For Name, enter redisHost. For Value, enter redis-master.default.svc.cluster.local:6379. Under Secret Key Ref, enter the pair redis:redis-password. Click the Actions button and deploy. Next we will deploy the Python and Node.js applications.\n","categories":"","description":"","excerpt":" Create, Configure and Deploy Dapr Redis State Store Component 🔗 …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/explore-dapr-with-meshery/dapr/deploy-dapr-statestore-component/","tags":"","title":"Deploy Dapr StateStore Component"},{"body":" DEPLOYMENTs 🔗 A Deployment is a higher-order abstraction that controls deploying and maintaining a set of Pods. Behind the scenes, it uses a ReplicaSet to keep the Pods running, but it offers sophisticated logic for deploying, updating, and scaling a set of Pods within a cluster. Deployments support rollbacks and rolling updates. Rollouts can be paused if needed.\n","categories":"","description":"","excerpt":" DEPLOYMENTs 🔗 A Deployment is a higher-order abstraction that …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-building-blocks/content/deployments/","tags":"","title":"DEPLOYMENTs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/sustainable-cloud/","tags":"","title":"Exoscale's Sustainable Cloud"},{"body":" Features 🔗 Kubernetes Feature Details 🔗 self-healing 🔗 Kubernetes restarts containers that fail, replaces containers, kills containers that don’t respond to your user-defined health check, and doesn’t advertise them to clients until they are ready to serve.\nautomatic bin packing 🔗 You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and RAM each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources.\nautomated rollouts and rollbacks 🔗 You can describe the desired state for your deployed containers using Kubernetes. Furthermore, it can change the actual state to the desired state at a controlled rate; e.g., you can automate Kubernetes to create new containers, remove existing containers and adopt all their resources to the new container.\nsecret and configuration management 🔗 Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images without exposing secrets in your setup.\nservice discovery and load balancing 🔗 Kubernetes can expose a container using the DNS name or using their IP address. If traffic to a container is high, Kubernetes can load balance and distribute the network traffic to stabilize the deployment.\nstorage orchestration 🔗 Kubernetes allows you to automatically mount a storage system of your choice, such as local storage, public cloud providers, and more.\n","categories":"","description":"","excerpt":" Features 🔗 Kubernetes Feature Details 🔗 self-healing 🔗 Kubernetes …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/kubernetes/content/features/","tags":"","title":"Features"},{"body":"","categories":"","description":"A hierarchical storage method that organizes data in files and folders, suitable for shared access and traditional workloads.","excerpt":"A hierarchical storage method that organizes data in files and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/file-storage/","tags":"","title":"File Storage"},{"body":" Overview 🔗 By packing more servers and other equipment into high-density racks, Exoscale reduces its overall data center footprint and the associated costs of building and maintaining a larger facility. In addition, smaller data centers require less energy to power and cool.\nBenefits 🔗 The high-density rack design is an approach to the layout and organization of server racks in a data center that maximizes the computing power that can be packed into a given space. This is achieved through specialized equipment and techniques for greater power and cooling efficiency, better cable management, and airflow.\nHigh-density racks typically feature a more significant number of servers per rack than traditional designs, with each server consuming more power and generating more heat. High-density racks often incorporate hot-aisle/cold-aisle containment, in-row cooling units, and high-efficiency power supplies to accommodate this.\nThe benefits of high-density rack design include increased computing power, reduced footprint and energy consumption, and lower overall costs. However, it also requires careful planning and maintenance to ensure that the equipment remains cool and functional and has good power and cooling infrastructure to support the higher density.\n","categories":"","description":"","excerpt":" Overview 🔗 By packing more servers and other equipment into …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/sustainable-cloud/content/rack-design/","tags":"","title":"High-Density Rack Design"},{"body":" History of Application Deployments 🔗 Traditional Deployment 🔗 Early on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a physical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can be instances where one application would take up most of the resources, and as a result, the other applications would underperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources were underutilized, and it was expensive for organizations to maintain many physical servers.\nVirtualized Deployment 🔗 As a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical server’s CPU. Virtualization allows applications to be isolated between VMs and provides a level of security as the information of one application cannot be freely accessed by another application. Virtualization allows better utilization of resources in a physical server and allows better scalability because an application can be added or updated easily, reduces hardware costs, and much more. With virtualization, you can present a set of physical resources as a cluster of disposable virtual machines. Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware.\nContainer Deployment 🔗 Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications. Therefore, containers are considered lightweight. Similar to a VM, a container has its own file system, share of CPU, memory, process space, and more. As they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions.\n","categories":"","description":"","excerpt":" History of Application Deployments 🔗 Traditional Deployment 🔗 Early …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-containers/content/history-of-deployment/","tags":"","title":"History of Application Deployments"},{"body":"","categories":"","description":"External access management","excerpt":"External access management","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/ingress/","tags":"","title":"Ingress"},{"body":"","categories":"","description":"New to the Exoscale pricing topic and wondering where to begin? This Level 100 INTRO Pricing - Learning Path focuses on pricing only. It will help you learn the basics and the more sophisticated Exoscale pricing topics and enable you to calculate product pricing for a given scenario and the whole scenario.","excerpt":"New to the Exoscale pricing topic and wondering where to begin? This …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/pricing/","tags":"","title":"INTRO Pricing"},{"body":"","categories":"","description":"This section covers the fundamental components of Kubernetes, including Pods, Services, Deployments, and more. Understanding these building blocks is essential for effectively deploying and managing applications in a Kubernetes environment.","excerpt":"This section covers the fundamental components of Kubernetes, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-building-blocks/","tags":"","title":"Kubernetes Building Blocks"},{"body":" What is a Service ? 🔗 A Service is used to expose Pods so that other Pods, or the external world, can access them. It uses labels to group Pods. There are different types of Services:\nClusterIP to expose Pods inside the cluster NodePort, LoadBalancer to expose Pods to the outside world Each time a Service receive a request, this one is load-balanced between the Pods exposed by a Service.\nService of type ClusterIP 🔗 Let’s consider 2 Pods running inside a cluster. The www Pods is running a web interface, how can this Pod access the api Pods running a backend component ? The calling Pod could use the IP address of the Pod it needs to call, but this is not how this should be done.\nInstead, we use a Service which exposes the api Pod. The www Pod calls this new Service to access the api Pod.\nA Service uses labels to expose Pods 🔗 A Service exposes the Pods which labels match its selector.\nThe following specification defines a Service with selector app: api:\napiVersion: v1 kind: Service metadata: name: api spec: selector: app: api type: ClusterIP ports: - port: 80 targetPort: 5000 The specification below defines a Pod which labels app: api corresponding to the Service’s selector:\napiVersion: v1 kind: Pod metadata: name: api labels: app: api spec: containers: - name: api image: org/api:1.2 ports: - containerPort: 5000 Because the Service’s selector matches the Pod’s labels, the Pod is exposed by the Service, so that other Pods can reach it internally.\nService of type NodePort 🔗 A NodePort Service is a ClusterIP Service which also exposes Pods to the outside of the cluster.\nWhen specifying a NodePort Service, we need to set the .spec.type property to NodePort. We can also set the port this Service should listen on via the .spec.ports.nodePort property, if it’s not set Kubernetes will automatically select a port on the 30000-32767 range.\nThe following specification defines a Service of type NodePort exposing the Pods with labels app: ghost. This Service is reachable from any node on port 31000.\napiVersion: v1 kind: Service metadata: name: ghost-np spec: selector: app: ghost type: NodePort ports: - port: 80 targetPort: 2368 nodePort: 31000 When an application is exposed with a NodePort Service, it’s accessible via the selected port from any cluster’s node.\nService of type LoadBalancer 🔗 A LoadBalancer Service is a NodePort Service which also triggers the creation of an external Load Balancer on the cloud provider infrastructure.\nLoadBalancer Service only triggers the creation of a Load Balancer, thus gets an external IP address, when used in a cluster managed by a cloud provider\nThe following specification defines a Service of type LoadBalancer exposing the Pods with labels app: ghost to the external world.\napiVersion: v1 kind: Service metadata: name: ghost-lb spec: selector: app: ghost type: LoadBalancer ports: - port: 80 targetPort: 2368 When created, this Service creates a LoadBalancer, with a dedicated IP address, in the cloud provider infrastructure.\nWe can reach the application exposed by the Service (a Pod running the ghost image in this example) using the Service IP address.\nBase commands 🔗 Creating a Service kubectl apply -f svc.yaml Listing the existing Service kubectl get svc Getting a Service’s details kubectl get svc ghost -o yaml Getting the Service main properties kubectl describe svc ghost Deleting a Service kubectl delete svc ghost Using Services in the VotingApp 🔗 The previous exercise left the app with the following components:\nWe will now add Services to the VotingApp to get the first working version of this demo app running fine in Kubernetes.\n","categories":"","description":"","excerpt":" What is a Service ? 🔗 A Service is used to expose Pods so that other …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/service/content/learn/","tags":"","title":"Learn"},{"body":" Install Telemetry Add-ons 🔗 Using Meshery, install Istio telemetry add-ons. In the Istio management page:\nClick the (+) icon on the Apply Service Mesh Configuration card. Select each of the following add-ons: Prometheus Grafana Jaeger You will use Prometheus and Grafana for collecting and viewing metrics and Jaeger collecting and viewing distributed traces. Expose each add-on external to the cluster. Each the service network typs are set to “LoadBalancer”.\nService Mesh Performance and Telemetry 🔗 Many of the labs require load to be placed on the sample apps. Let’s generate HTTP traffic against the BookInfo application, so we can see interesting telemetry.\nVerify access through the Ingress Gateway:\nkubectl get service istio-ingressgateway -n istio-system Once we have the port, we can append the IP of one of the nodes to get the host.\nThe URL to run a load test against will be http://\u003cIP/hostname of any of the nodes in the cluster\u003e:\u003cingress port\u003e/productpage\nPlease note: If you are using Docker Desktop, please use the IP address of your host. You can leave the port blank. For example: http://1.2.3.4/productpage. Managed kubernetes service users will need to use http://\u003cexternal-ip of istio-ingressgateway\u003e/productpage as done in expose services section.\nUse the computed URL above in Meshery, in the browser, to run a load test and see the results.\nConnect Grafana (optionally, Prometheus) to Meshery. 🔗 On the Settings page:\nNavigate to the Metrics tab. Enter Grafana’s URL:port number and submit. Use Meshery to generate load and analyze performance. 🔗 On the Performance page:\ngive this load test a memorable name enter the URL to the BookInfo productpage select Istio in the Service Mesh dropdown enter a valid number for Concurrent requests enter a valid number for Queries per second enter a valid Duration (a number followed by s for seconds (OR) m for minutes (OR) h for hour) use the host IP address in the request Tab and in the advanced options, type in the header as Host:\u003capp-name\u003e Click on Run Test. A performance test will run and statistical analysis performed. Examine the results of the test and behavior of the service mesh.\nNext, you will begin controlling requests to BookInfo using traffic management features.\nAlternative: Manual installation 🔗 Follow these steps if the above steps did not work Install Add-ons: 🔗 Prometheus\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.7/samples/addons/prometheus.yaml Grafana\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.7/samples/addons/grafana.yaml Jaeger\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.7/samples/addons/jaeger.yaml Exposing services 🔗 Istio add-on services are deployed by default as ClusterIP type services. We can expose the services outside the cluster by either changing the Kubernetes service type to NodePort or LoadBalancer or by port-forwarding or by configuring Kubernetes Ingress.\nOption 1: Expose services with NodePort To expose them using NodePort service type, we can edit the services and change the service type from ClusterIP to NodePort\nOption 2: Expose services with port-forwarding Port-forwarding runs in the foreground. We have appeneded \u0026 to the end of the above 2 commands to run them in the background. If you donot want this behavior, please remove the \u0026 from the end.\nPrometheus 🔗 You will need to expose the Prometheus service on a port either of the two following methods:\nOption 1: Expose services with NodePort\nkubectl -n istio-system edit svc prometheus To find the assigned ports for Prometheus:\nkubectl -n istio-system get svc prometheus Option 2: Expose Prometheus service with port-forwarding:\nExpose Prometheus service with port-forwarding:\nkubectl -n istio-system port-forward \\ $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath='{.items[0].metadata.name}') \\ 9090:9090 \u0026 Browse to http://\u003cip\u003e:\u003cport\u003e and in the Expression input box enter: istio_request_bytes_count. Click the Execute button.\nGrafana 🔗 You will need to expose the Grafana service on a port either of the two following methods:\nkubectl -n istio-system edit svc grafana Once this is done the services will be assigned dedicated ports on the hosts.\nTo find the assigned ports for Grafana:\nkubectl -n istio-system get svc grafana Expose Grafana service with port-forwarding:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana \\ -o jsonpath='{.items[0].metadata.name}') 3000:3000 \u0026 ##### **Distributed Tracing** The sample Bookinfo application is configured to collect trace spans using Zipkin or Jaeger. Although Istio proxies are able to automatically send spans, it needs help from the application to tie together the entire trace. To do this applications need to propagate the appropriate HTTP headers so that when the proxies send span information to Zipkin or Jaeger, the spans can be correlated correctly into a single trace.\nTo do this the application collects and propagates the following headers from the incoming request to any outgoing requests:\nx-request-id x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags x-ot-span-context Exposing services 🔗 Istio add-on services are deployed by default as ClusterIP type services. We can expose the services outside the cluster by either changing the Kubernetes service type to NodePort or LoadBalancer or by port-forwarding or by configuring Kubernetes Ingress. In this lab, we will briefly demonstrate the NodePort and port-forwarding ways of exposing services.\nOption 1: Expose services with NodePort To expose them using NodePort service type, we can edit the services and change the service type from ClusterIP to NodePort\nFor Jaeger, either of tracing or jaeger-query can be exposed.\nkubectl -n istio-system edit svc tracing Once this is done the services will be assigned dedicated ports on the hosts.\nTo find the assigned ports for Jaeger:\nkubectl -n istio-system get svc tracing Option 2: Expose services with port-forwarding To port-forward Jaeger:\nkubectl -n istio-system port-forward \\ $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath='{.items[0].metadata.name}') \\ 16686:16686 \u0026 View Traces 🔗 Let us find the port Jaeger is exposed on by running the following command:\nkubectl -n istio-system get svc tracing You can click on the link at the top of the page which maps to the right port and it will open Jaeger UI in a new tab.\n","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/introduction-to-service-meshes/observability/","tags":"","title":"Observability"},{"body":" Use a Secret to secure the connection to Postgres 🔗 In a secret-db.yaml file, add the specification for a Secret containing the key password with the associated value dbpass.\nModify the db Deployment to reference this Secret key (instead of specifying the password in plain text).\nAdd the POSTGRES_PASSWORD environment variable in the containers of the worker and result Deployments, ensuring that the value of this variable references the key of the Secret created earlier.\nDeploy the application defined in this specification and verify that you have access to both the voting and result interfaces.\nDelete the application.\nSolution The password we want to store in the Secret is dbpass. First, we encode this password in base64:\n$ echo \"dbpass\" | base64 ZGJwYXNzCg== Next, we create the following file defining a Secret:\napiVersion: v1 kind: Secret metadata: name: db data: password: ZGJwYXNzCg== We modify the db Deployment specification to reference the contents of the Secret key password instead of using the password in plain text: apiVersion: apps/v1 kind: Deployment metadata: labels: app: db name: db spec: replicas: 1 selector: matchLabels: app: db template: metadata: labels: app: db spec: containers: - image: postgres:15.1-alpine3.17 name: postgres env: - name: POSTGRES_PASSWORD valueFrom: secretKeyRef: name: db key: password ports: - containerPort: 5432 name: postgres We modify the worker and result Deployments (the two microservices connecting to db) to add the POSTGRES_PASSWORD environment variable, which retrieves its value from the db Secret. The new worker Deployment specification:\napiVersion: apps/v1 kind: Deployment metadata: labels: app: worker name: worker spec: replicas: 1 selector: matchLabels: app: worker template: metadata: labels: app: worker spec: containers: - image: voting/worker:latest name: worker env: - name: POSTGRES_PASSWORD valueFrom: secretKeyRef: name: db key: password The new result Deployment specification:\napiVersion: apps/v1 kind: Deployment metadata: labels: app: result name: result spec: replicas: 1 selector: matchLabels: app: result template: metadata: labels: app: result spec: containers: - image: voting/result:latest name: result env: - name: POSTGRES_PASSWORD valueFrom: secretKeyRef: name: db key: password We deploy the application with the following command from the manifests directory: kubectl apply -f . As before, using the IP address of one of the cluster nodes, we can access the voting and result interfaces via ports 31000 and 31001, respectively.\nWe delete the application with the following command from the manifests directory: kubectl delete -f . ","categories":"","description":"","excerpt":" Use a Secret to secure the connection to Postgres 🔗 In a …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/configuration/content/practice/","tags":"","title":"Practice"},{"body":" Run the VotingApp in Pods 🔗 Create the votingapp folder on your local machine. Inside this one, create the YAML files containing the Pod specifications for each microservice, following the instructions from the table below:\nMicroservice File Name Pod Name Container Image Vote UI pod-voteui.yaml vote-ui voting/vote-ui:latest Vote pod-vote.yaml vote voting/vote:latest Redis pod-redis.yaml redis redis:7.0.8-alpine3.17 Worker pod-worker.yaml worker voting/worker:latest Postgres pod-db.yaml db postgres:15.1-alpine3.17 Result pod-result.yaml result voting/result:latest Result UI pod-resultui.yaml result-ui voting/result-ui:latest For the db Pod, make sure to specify an environment variable POSTGRES_PASSWORD with the value postgres.\nDeploy the application defined by all of these specifications\nWhat do you notice ?\nDelete the application\nSolution The specifications are as follows: apiVersion: v1 kind: Pod metadata: name: vote-ui spec: containers: - image: voting/vote-ui:latest name: vote-ui apiVersion: v1 kind: Pod metadata: name: vote spec: containers: - image: voting/vote:latest name: vote apiVersion: v1 kind: Pod metadata: name: redis spec: containers: - image: redis:7.0.8-alpine3.17 name: redis apiVersion: v1 kind: Pod metadata: name: worker spec: containers: - image: voting/worker:latest name: worker imagePullPolicy: Always apiVersion: v1 kind: Pod metadata: name: db spec: containers: - image: postgres:15.1-alpine3.17 name: postgres env: - name: POSTGRES_PASSWORD value: postgres apiVersion: v1 kind: Pod metadata: name: result spec: containers: - image: voting/result:latest name: result apiVersion: v1 kind: Pod metadata: name: result-ui spec: containers: - image: voting/result-ui:latest name: result-ui The application can be launched with the following command: kubectl apply -f votingapp If a folder is specified, all the YAML files in that directory are created\nWhat do you notice ? Some Pods are in error:\n$ kubectl get po NAME READY STATUS RESTARTS AGE db 1/1 Running 0 25s redis 1/1 Running 0 25s result 1/1 Running 0 25s result-ui 0/1 CrashLoopBackOff 1 (4s ago) 24s vote 1/1 Running 0 25s vote-ui 0/1 CrashLoopBackOff 1 (3s ago) 25s worker 1/1 Running 0 25s If we take the vote-ui Pod as an example, the logs show that it cannot connect to vote:\n$ kubectl logs vote-ui /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh /docker-entrypoint.sh: Configuration complete; ready for start up 2024/02/08 11:10:20 [emerg] 1#1: host not found in upstream \"vote\" in /etc/nginx/nginx.conf:44 nginx: [emerg] host not found in upstream \"vote\" in /etc/nginx/nginx.conf:44 Moreover, the logs from the worker Pod indicate that it cannot connect to the Redis Pod:\n$ kubectl logs worker ... Waiting for Redis dial tcp: lookup redis on 10.96.0.10:53: no such host The Pods for the different microservices are created, but they cannot communicate with each other because we need to create Services. We will add this in the next step, which will allow us to have a fully functional application.\nWe delete the application with the following command: kubectl delete -f votingapp ","categories":"","description":"","excerpt":" Run the VotingApp in Pods 🔗 Create the votingapp folder on your local …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/pod/content/practice/","tags":"","title":"Practice"},{"body":" Add Services to the VotingApp 🔗 In the votingapp directory, create YAML files containing the specifications for the Services of each microservice in the application, according to the table below:\nMicroservice File Name Service Type Service Details Vote UI svc-voteui.yaml NodePort (31000) nodePort 31000, port: 80, targetPort: 80 Vote svc-vote.yaml ClusterIP port: 5000, targetPort: 5000 Redis svc-redis.yaml ClusterIP port: 6379, targetPort: 6379 Postgres svc-db.yaml ClusterIP port: 5432, targetPort: 5432 Result svc-result.yaml ClusterIP port: 5000, targetPort: 5000 Result UI svc-resultui.yaml NodePort (31001) nodePort 31001, port: 80, targetPort: 80 Note that it is not necessary to expose the worker Pod with a Service as no Pod needs to connect to it. Instead, it is the worker Pod that connects to redis and db.\nFor each Pod/Service pair, make sure to properly define a label in the Pod and the corresponding selector in the Service.\nDeploy the application defined by these specifications\nAccess the vote and result interfaces via the NodePort Services\nDelete the application\nSolution The Service specifications are as follows: apiVersion: v1 kind: Service metadata: labels: app: vote-ui name: vote-ui spec: type: NodePort ports: - port: 80 targetPort: 80 nodePort: 31000 selector: app: vote-ui apiVersion: v1 kind: Service metadata: labels: app: vote name: vote spec: ports: - port: 5000 targetPort: 5000 selector: app: vote apiVersion: v1 kind: Service metadata: labels: app: redis name: redis spec: type: ClusterIP ports: - port: 6379 targetPort: 6379 selector: app: redis apiVersion: v1 kind: Service metadata: labels: app: db name: db spec: type: ClusterIP ports: - port: 5432 targetPort: 5432 selector: app: db apiVersion: v1 kind: Service metadata: labels: app: result name: result spec: ports: - port: 5000 targetPort: 5000 selector: app: result apiVersion: v1 kind: Service metadata: labels: app: result-ui name: result-ui spec: type: NodePort ports: - port: 80 targetPort: 80 nodePort: 31001 selector: app: result-ui Deploy the application with the following command from the votingapp directory: kubectl apply -f . The different Pods are now in Running status: $ kubectl get po,svc NAME READY STATUS RESTARTS AGE pod/db 1/1 Running 0 20s pod/redis 1/1 Running 0 20s pod/result 1/1 Running 0 20s pod/result-ui 1/1 Running 0 20s pod/vote 1/1 Running 0 20s pod/vote-ui 1/1 Running 0 21s pod/worker 1/1 Running 0 20s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/db ClusterIP 10.100.10.36 \u003cnone\u003e 5432/TCP 20s service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 29m service/redis ClusterIP 10.107.167.249 \u003cnone\u003e 6379/TCP 20s service/result ClusterIP 10.105.157.142 \u003cnone\u003e 5000/TCP 20s service/result-ui NodePort 10.101.30.191 \u003cnone\u003e 80:31001/TCP 20s service/vote ClusterIP 10.96.108.192 \u003cnone\u003e 5000/TCP 20s service/vote-ui NodePort 10.104.203.9 \u003cnone\u003e 80:31000/TCP 20s Using the IP address of one of the cluster nodes, we can access the vote and result interfaces via ports 31000 and 31001 respectively.\nDelete the application with the following command: kubectl delete -f . ","categories":"","description":"","excerpt":" Add Services to the VotingApp 🔗 In the votingapp directory, create …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/service/content/practice/","tags":"","title":"Practice"},{"body":" Persist the VotingApp’s databases 🔗 By default, k3s comes with the default StorageClass named local-path which can be verified using the following command.\n$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 1h4m In a file named pvc-redis.yaml, define the specification for a PersistentVolumeClaim with the following characteristics: name: redis ReadWriteOnce mode request for 1G storage Then modify the redis Deployment specification by adding a volume based on this PersistentVolumeClaim, and use the volumeMounts instruction to ensure that the associated PersistentVolume is mounted in the /data directory of the redis container.\nIn a file named pvc-db.yaml containing the specification for a PersistentVolumeClaim with the following characteristics:\nname: db ReadWriteOnce mode request for 1G of storage Then modify the db Deployment specification by adding a volume based on this PersistentVolumeClaim, and use the volumeMounts instruction to ensure that the associated PersistentVolume is mounted in the /var/lib/postgresql/data directory of the postgres container.\nDeploy the application defined in these specifications and verify that it is working correctly.\nList the PersistentVolumeClaim resources. What do you observe?\nDelete the application.\nSolution The specification to define the PersistentVolumeClaim named redis: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: redis spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi The redis Deployment is modified as follows:\napiVersion: apps/v1 kind: Deployment metadata: labels: app: redis name: redis spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - image: redis:7.0.8-alpine3.17 name: redis volumeMounts: - name: data mountPath: /data volumes: - name: data persistentVolumeClaim: claimName: redis The specification to define the PersistentVolumeClaim named db: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: db spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi The db Deployment is modified as follows:\napiVersion: apps/v1 kind: Deployment metadata: labels: app: db name: db spec: replicas: 1 selector: matchLabels: app: db template: metadata: labels: app: db spec: containers: - image: postgres:15.1-alpine3.17 name: postgres env: - name: POSTGRES_PASSWORD valueFrom: secretKeyRef: name: db key: password volumeMounts: - name: data mountPath: /var/lib/postgresql/data ports: - containerPort: 5432 name: postgres volumes: - name: data persistentVolumeClaim: claimName: db Deploy the application using the following command from the manifests directory: kubectl apply -f . Using the IP address of one of the cluster nodes, you can access the vote and result interfaces via ports 31000 and 31001 respectively.\nYou can list the PersistentVolumeClaim resources and observe that a PersistentVolume has been created for each of the 2 PVCs. List of PersistentVolumeClaims:\n$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/redis Bound pvc-789e3c5c-4402-4b96-b09d-ee441e8ade1d 1Gi RWO local-path 39s persistentvolumeclaim/db Bound pvc-75b9a32c-eab5-4452-a9b8-12d41dd74e7a 1Gi RWO local-path 39s List of PersistentVolumes:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-789e3c5c-4402-4b96-b09d-ee441e8ade1d 1Gi RWO Delete Bound default/redis local-path 32s persistentvolume/pvc-75b9a32c-eab5-4452-a9b8-12d41dd74e7a 1Gi RWO Delete Bound default/db local-path 32s Delete the application using the following command from the manifests directory: kubectl delete -f . ","categories":"","description":"","excerpt":" Persist the VotingApp’s databases 🔗 By default, k3s comes with the …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/storage/content/practice/","tags":"","title":"Practice"},{"body":" Pricing Calculator 🔗 A simple and convenient tool to get product pricing for various configurations always available here:\nwww.exoscale.com/calculator\n","categories":"","description":"","excerpt":" Pricing Calculator 🔗 A simple and convenient tool to get product …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/pricing/pricing/content/pricing-calculator/","tags":"","title":"Pricing Calculator"},{"body":" Framework 🔗 Understanding the risks that could significantly impact the organization and its ability to achieve its objectives is critical.\nRisks are typically grouped into three categories:\nBusiness Risks Compliance Risks Operational Risks Business Risks 🔗 that could impact the organization’s ability to achieve its objectives, for example, a natural disaster disrupting operations.\nBusiness risk refers to the potential for financial loss or other negative consequences arising from internal or external factors affecting a company’s ability to achieve its objectives. This can include risks related to market conditions, competition, regulatory changes, financial performance, operational issues, and other factors that can impact the success or sustainability of a business. Practical risk assessment and management strategies can help enterprises to identify and mitigate potential risks, minimize financial losses, and protect against other negative impacts.\nCompliance Risks 🔗 that could lead to non-compliance with legal or regulatory requirements, for example, a data breach that leads to a fine from a data privacy regulatory body.\nCompliance risk is the potential for financial loss or legal penalties arising from a company’s failure to comply with laws, regulations, or industry standards. This can include risks related to data privacy, environmental regulations, labor laws, financial reporting requirements, and other areas of regulatory compliance. Non-compliance can result in fines, legal action, damage to reputation, and other negative consequences that can impact a company’s financial performance and overall success. Practical risk assessment and management strategies can help businesses identify and address compliance risks, ensure regulatory compliance, and minimize the potential for financial and legal consequences.\nOperational Risks 🔗 that could impact day-to-day operations, for example, a system failure that disrupts business processes.\nOperational risk is the potential for financial loss or other negative consequences arising from a company’s internal processes, systems, or human error. This can include risks related to technology failures, supply chain disruptions, employee misconduct, fraud, and other operational issues that can impact a company’s ability to conduct business effectively. Operational risks can also arise from external events, such as natural disasters or cyberattacks. Practical risk assessment and management strategies can help enterprises to identify and mitigate potential operational risks, improve processes and systems, and minimize the potential for financial losses or other negative impacts.\n","categories":"","description":"","excerpt":" Framework 🔗 Understanding the risks that could significantly impact …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/exoscale-compliance/content/assessment/","tags":"","title":"Risk Assessment"},{"body":" Generic IaaS Use Cases 🔗 Lift \u0026 Shift 🔗 One speaks of Lift \u0026 Shift when moving existing, traditional workloads into an IaaS cloud as unaltered as possible is unavoidable. Management Tools for IT Operations are, as far as possible, supported by the existing Data Center Operation taken over and implemented. Cloud-native functions have little or no application.\nImprove \u0026 Move 🔗 One speaks of Improve \u0026 Move if you adapt and modify IT processes in this approach to take advantage of cloud-native functions at least partially. You should use the migration process to achieve a more substantial standardization, a higher degree of automation, and free existing processes from legacy burdens.\nDesign \u0026 Build 🔗 One speaks of Design \u0026 Build if you redefine all or all new business and IT processes. Further, you use agile, cloud-native methods in your developments wherever possible. Tools used are cloud-native and DevOps-oriented throughout the application life cycle, focusing strongly on Infrastructure as Code (IaC) and automation.\nSpecific IaaS Use Cases 🔗 Test Cloud 🔗 Problem: Since every failure of the online betting portal brings an enormous loss of earnings, NoName Inc. wants to build a more stable web presence with reduced or no downtime. Simultaneously, the web portal enables many browsers to reach additional markets. Before releasing new web portal products to customers, all changes need testing in various browsers (Edge, Chrome, Firefox, Safari, …) to increase the overall stability. New development standards should provide prompt feedback whenever a developer makes a change. This way, detecting problems is possible before releasing new products. Parallel execution of individual test scenarios enables quicker feedback to the developer but requires approximately 1200 servers temporarily for each test run.\nSolution: Exoscale has several interfaces (REST API, Tool Integrations, e.g., Terraform), allowing a flexible and automatic deployment. Creating new servers is achieved within a few minutes. Execution and the afterward decommission of used test-serves are done in no time.\nAdvantages: The high degree of automation provided by Exoscale makes such a modern and performance test scenario possible. This solution eliminates the expensive acquisition of servers and complex internal procurement. With the Pay-Per-Use principle, the second charges the customer for used server resources.\nProductivity 🔗 Problem: The young software company NoName Ltd wants to simplify joint digital team collaboration utilizing video chat. With little effort, the platform should offer teams a high-performance, straightforward solution to discuss open topics daily. The newly developed and patented streaming algorithm led to massive performance improvement. Integrations to task management, CRM, and knowledge base systems facilitate team collaboration during a digital meeting. The platform uses a separate server for each session, and the server is needed in the timeframe when the meeting takes place. Depending on the requests, a strongly varying number of servers may be necessary.\nSolution: Due to the excellent integration of tools that allow the creation, configuration, and control of the infrastructure, Exoscale offers an ideal foundation to meet flexible requirements. Thus, NoName Ltd can provide infrastructure fast and automated.\nAdvantages: Due to the need for a flexible number of servers, NoName Ltd would have to purchase a large pool of servers. Exoscale flexibility contributes to risk minimization and resource management by providing components in a Pay-Per-Use model billed by the second. Creating more servers for digital meetings is easy if more customers decide to use the platform.\nImage Analysis 🔗 Problem: A new and modern image analysis algorithm developed by NoName LLC should ensure the high weekly production volume’s product quality. However, a considerable amount of high-performance computing resources is required to ensure the algorithm’s quality and timely completion of the machine learning training phase. NoName LLC wants to offload the complexity associated with this task and does not wish to operate or purchase any servers.\nSolution: Exoscale provides modern GPU instances optimized for advanced computational tasks like machine learning, artificial intelligence, and high-performance computing.\nAdvantages: Outsourcing the complexity of the needed infrastructure configuration and using the high-cost hardware equipment in a Pay-Per-Use model delivers many operational advantages simultaneously. For example, the GPUs architecture has simpler calculation units but performs more calculations per time to optimize, e.g., ideal for machine learning tasks.\n","categories":"","description":"","excerpt":" Generic IaaS Use Cases 🔗 Lift \u0026 Shift 🔗 One speaks of Lift \u0026 Shift …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud/content/scenarios/","tags":"","title":"Scenarios"},{"body":"This section is a refresher that provides an overview of the main properties involved in the scheduling phase. At the end of this section, please complete the exercises to apply these concepts.\nPurpose 🔗 The scheduling step is where Kubernetes decides on which Node a Pod will run on. The Scheduler, running on the control plane, is the process in charge of this action.\nThere are various properties/items which can influence the scheduling decision, including:\nnodeName nodeSelector nodeAffinity podAffinity / podAntiAffinity topologySpreadConstraints taint / toleration available resources priorityClass runtimeClass nodeName 🔗 The nodeName property bypasses the scheduling process, indicating directly in the Pod’s specification the name of the Node this Pod must be deployed to.\nnodeSelector 🔗 The nodeSelector property uses Node’s labels to schedule a Pod.\nnodeAffinity 🔗 The nodeAffinity property also uses Node’s label. Still, it is more granular than nodeSelector as it can use the following operators on the labels: In, NotIn, Exists, DoesNotExist, Gt, and Lt.\nTwo rules are available when using nodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution defines a hard constraint: if the scheduler does not manage to schedule the Pod according to the specification, then the Pod will remain in Pending preferredDuringSchedulingIgnoredDuringExecution defines a soft constraint: if the scheduler does not manage to schedule the Pod, according to the specification, then it will do its best to schedule it anyway, even if the requirements are not satisfied Example: 🔗 spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name Pod must be scheduled on a node having the label operator: In kubernetes.io/e2e-az-name with the value e2e-az1 or e2e-az2 values: - e2e-az1 - e2e-az2 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: Pod preferably scheduled on a node having the label - key: disktype disktype with the value ssd operator: In values: - ssd podAffinity / podAntiAffinity 🔗 We use the podAffinity and podAntiAffinity properties to schedule a Pod based on the labels of already existing Pods.\nIt uses the same rules as the nodeAffinity property:\nrequiredDuringSchedulingIgnoredDuringExecution defines a hard constraint preferredDuringSchedulingIgnoredDuringExecution defines a soft constraint It also uses a property named topologyKey to specify geographical preferences (among other things):\nhostname region az … Example 1 🔗 The following Deployment’s Pods cannot all be created on a cluster with less than 4 Nodes as it specifies that two Pods with the app: cache label must not be on the same Node.\napiVersion: apps/v1 kind: Deployment metadata: name: cache spec: replicas: 4 selector: matchLabels: app: cache template: metadata: labels: app: cache spec: containers: - name: redis image: redis:6 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - cache topologyKey: \"kubernetes.io/hostname\" Example 2 🔗 The following specification illustrates the usage of both podAffinity and podAntiAffinity properties.\nspec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: Pod must be scheduled on a node which is in the availability - labelSelector: zone where already exists a Pod with the label security: S1 matchExpressions: - key: security operator: In values: - S1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: Pod should not be scheduled on a node where already exists - podAffinityTerm: a Pod with the label security: S2 labelSelector: matchExpressions: - key: security operator: In values: - S2 topologyKey: kubernetes.io/hostname TopologySpreadConstraints 🔗 The topologySpreadConstraints property defines how to spread Pods across a cluster topology, ensuring application resiliency.\nThe following Deployment uses the topologySpreadConstraints property to ensure Pods are correctly balanced between AZ and Node.\napiVersion: apps/v1 kind: Deployment metadata: name: www spec: replicas: 4 selector: matchLabels: app: www template: metadata: labels: app: www spec: containers: - name: nginx image: nginx:1.24 topologySpreadConstraints: - maxSkew: 1 The difference in the number of matching Pods between topologyKey: \"kubernetes.io/hostname\" any two nodes should be no more than 1 (maxSkew) whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: app: www - maxSkew: 1 Ensures that the Pods are spread across different topologyKey: \"topology.kubernetes.io/zone\" availability zones whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: app: www Taints \u0026 Tolerations 🔗 In contrast to the properties we saw earlier, we use a Taint to prevent certain Pods from being scheduled on a node. A Pod must tolerate a Taint, using a toleration property, to be scheduled on a Node having that Taint.\nA Taint has 3 properties:\nkey: it can be arbitrary string content, same format as labels value: it can be an arbitrary string content, same format as labels effect: among NoSchedule, PreferNoSchedule and NoExecute By default, a kubeadm cluster sets a Taint on the control plane Nodes. This Taint prevents a Pod from being deployed on these Nodes unless the Pod explicitly tolerates this Taint.\nThe following command lists the Taints existing on the controlplane Node.\n$ kubectl get no controlplane -o jsonpath='{.spec.taints}' | jq [ { \"effect\": \"NoSchedule\", \"key\": \"node-role.kubernetes.io/control-plane\" } ] When creating the following Deployment, with 10 replicas of nginx Pods, none of the Pods will land on the controlplane because of this specific Taint.\napiVersion: apps/v1 kind: Deployment metadata: name: www spec: replicas: 10 selector: matchLabels: app: www template: metadata: labels: app: www spec: containers: - image: nginx:1.24 name: nginx We must add a toleration for the Taint, so the scheduler can schedule Pods on the controlplane.\napiVersion: apps/v1 kind: Deployment metadata: name: www spec: replicas: 10 selector: matchLabels: app: www template: metadata: labels: app: www spec: tolerations: - key: node-role.kubernetes.io/control-plane effect: NoSchedule containers: - image: nginx:1.24 name: nginx PriorityClass 🔗 A PriorityClass is a Kubernetes resource that defines the priority of Pods. It influences scheduling decisions and preemption - meaning that higher-priority Pods can evict lower-priority ones if resources are scarce. A PriorityClass can be preempting (default behavior) or non-preempting, depending on the preemtionPolicy property.\nThe following specifications define a high-priority PriorityClass and a Pod using it.\napiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 1000000 globalDefault: false --- apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx priorityClassName: high-priority RuntimeClass 🔗 A RuntimeClass allows to have multiple container runtimes in the cluster, and to select the one which best fits a workload. There are several container runtimes, each one of them addresses specific use cases, including:\nContainerd is a general-purpose container runtime (CNCF Graduated) CRI-O is a light container runtime focusing on Kubernetes (CNCF Graduated) GVisor is used to run an unsecure workload in a sandbox Kata-Container uses Micro VMs for workload isolation Firecracker uses Micro VMs, mainly for serverless WasmEdge is dedicated to run Wasm workload In a Pod specification, we can specify the container runtime configuration needed by this workload. The scheduler ensures that Pods are placed on Nodes that support the required container runtime configuration.\nLet’s say our cluster already has a RuntimeClass named gvisor, then we can use it in the Pod specification as follows.\napiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx runtimeClass: gvisor ","categories":"","description":"Learn Pod placement strategies using labels, taints, affinities and more.","excerpt":"Learn Pod placement strategies using labels, taints, affinities and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/scheduling/","tags":"","title":"Scheduling Pods"},{"body":" Storage Technologies 🔗 Kubernetes, due to its extensible nature, can leverage a multitude of storage technologies. Especially for the persistent volume scenarios, there are various solutions available. Of course, there are also open-source variants.\nLinstor makes building, running, and controlling block storage with native integration to Kubernetes simple. Linstor is open-source software designed to manage block storage devices and provide persistent block storage for Kubernetes environments. LINBIT builds upon 20 years of experience developing and supporting production storage and high availability workloads, combined with modern best practices from the community.\nLonghorn is an official CNCF project that delivers a powerful cloud-native distributed storage platform for Kubernetes that can run anywhere. When combined with Rancher, Longhorn makes deploying highly available persistent block storage in your Kubernetes environment easy, fast, and reliable.\nOpenEBS is the leading open-source project which offers cloud-native storage solutions for Kubernetes deployments. Unlike any other storage option, OpenEBS can easily be integrated with Kubernetes, making it a highly rated cloud-native storage on the CNCF landscape.\nRook is a top-rated open-source storage solution for Kubernetes, but it differs from others due to its storage orchestrating capacities. In addition, Rook is a production-grade solution transforming storage volumes into self-scaling, self-healing, and self-managing storage systems to integrate with the Kubernetes environment perfectly.\nGlusterFS is a notable open-source project that provides a mechanism to deploy native storage services onto a Kubernetes cluster quickly. It is a precisely defined file storage framework that can scale to petabytes, utilize any on-disk file system backing different features, and handle many users.\nPortworx is another container storage solution intended for Kubernetes, with a focus on highly available clusters. It is host-attached storage, where every volume directly maps to the host to which it is attached. These volumes are accessed through I/O technology and provide auto-tuning based on the I/O protocol used.\nMany more technologies are available. A complete, up-to-date list of supported technologies is on kubernetes-storage. In the following two units, we picked Longhorn as an example and demonstrate how to use such a storage technology with SKS.\n","categories":"","description":"","excerpt":" Storage Technologies 🔗 Kubernetes, due to its extensible nature, can …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/storage/content/storage-technologies/","tags":"","title":"Storage Technologies"},{"body":" Tools for Data 🔗 Database Word Cloud\nUsing databases to organize data started long before the age of computers. Sumerian clay tablets, ship manifests, card catalogs, and product inventories are all databases. Computers enabled the automation of databases. For computer-based databases, we have to distinguish the database model from the software implementation of that model.\nThe first database model in the computer era was the Flat File Model, a simple consecutive list of records that mimicked the non-computational model from the past as card catalogs or ship manifests. In the mid-1960s, IBM developed a hierarchical database model for their software solution called IMS (Information Management System), which was debuted in 1968 and supported NASA’s Moon Mission efforts.\nComputer History Museum - Databases\nThe Dawn of the Database as we know it today was initiated in 1970 by Ted Codd, a computer scientist at IBM. A relational database model organized the data in simple tables, making it easier to access, merge, and change. In hindsight, this was the game-changer, and the seed from that grew an entirely new industry. The idea was picked up by academia first (UC Berkley) and enterprises later to release new software products based on the new relational database model. Even IBM started implementing an experimental solution called System R in 1975.\nIf you want to know the entire history with more details, you can watch it (History of Databases by Computer History Museum) on YouTube in less than six minutes here. A quick structured overview of the History of the Database Evolution is put together in the next unit.\n","categories":"","description":"","excerpt":" Tools for Data 🔗 Database Word Cloud\nUsing databases to organize data …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/databases/content/tools-data/","tags":"","title":"Tools for Data"},{"body":"This section presents some common deployment errors and how to troubleshoot them.\nPod stuck in Pending state 🔗 A Pod can remain in Pending state for various reasons.\nkubectl get po NAME READY STATUS RESTARTS AGE www 0/1 Pending 0 6s To get more information about the root cause, run the following command (replacing the placeholders with the name of the Pod and the name of the Namespace in which it is running).\nkubectl describe pod POD_NAME -n NAMESPACE The Pod requires more resources than available 🔗 Let’s consider a Pod named www for which the describe command returns information similar to the following.\n$ kubectl describe po www ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 110s default-scheduler 0/2 nodes are available: 2 Insufficient cpu, 2 Insufficient memory. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod. This means the Pod’s specification requests more resources than currently available. There are several cases:\nthe Pod’s container actually needs this amount of resources\nIn this case, adding a Node with more resources could fix the issue\nthe requests were not configured currently and are higher than what is needed\nIn this case, decreasing the resources requests (for memory and/or CPU), so it fits the application needs, should be explored. The Pod specification below shows these properties for a simple nginx container.\napiVersion: v1 kind: Pod metadata: labels: run: www name: www spec: containers: - image: nginx:1.24 name: www resources: requests: memory: 4G # memory and cpu requests are taken into account during the scheduling phase cpu: 2 # decreasing the amount of mempry and cpu may fix the issue in some cases The Pod has a placement constraint that cannot be satisfied 🔗 Some Pods need to be scheduled on a specific type of Nodes; for instance, an ML/AI workload usually needs to run on a Node with a GPU. These schedule constraints can be defined in the Pod specification as in the example below.\napiVersion: v1 kind: Pod metadata: name: ml spec: containers: - image: myMLApp:1.4.2 name: ml affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node.kubernetes.io/instance-type operator: In values: - gpua5000 # This Pod needs to be deployed on a node with - gpu3080ti # a 3080TI or a A5000 GPU When run on a cluster which does not have this type of Node, this Pod will remain in Pending state, the output of the describe command being similar to the one below.\n$ kubectl describe pod ml ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 11s default-scheduler 0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling. To fix this issue, we could either:\nmodify the Pod specification referencing a GPU instance type existing in the cluster add a new Node into the cluster, this one of the type defined in the Pod specification The Pod needs storage that cannot be created 🔗 Some Pods need persistent storage to store data; for instance, a database needs to persist data on a real storage, not in the container’s file system. This is done through the usage of a PersistentVolumeClaim resource, which is referenced in the Pod specification.\nThe example below defines a Pod running MongoDB and persisting its data via a PVC named mongo-data.\napiVersion: v1 kind: Pod metadata: name: mongo spec: containers: - image: mongo:6 name: mongo volumes: - name: data persistentVolumeClaim: claimName: mongo-data apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mongo-data spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 1Gi Non-existing StorageClass There may be situation where the describe command returns an error message similar to the following one. This indicates that no PersistentVolume could be created and associated to the PVC.\n... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 43m default-scheduler 0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling. Using the describe command on the PVC may provide additional information. In the example below, the error is due to an invalid name for the StorageClass.\n$ kubectl describe pvc mongo-data ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 3m22s (x481 over 166m) persistentvolume-controller storageclass.storage.k8s.io \"manual\" not found We can change the PVC definition, using the name of an existing StorageClass (local-path in this example).\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: mongo-data spec: storageClassName: local-path accessModes: - ReadWriteOnce resources: requests: storage: 1Gi After recreating the PVC using the updated specification, the PVC should be bound to a PV, and the Pod should transition to the Running state.\nThere may be other reasons which prevent the underlying PV to be created such as capacity limits of the underlying storage ","categories":"","description":"This section presents some common deployment errors and how to troubleshoot them.","excerpt":"This section presents some common deployment errors and how to …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"Learn how to troubleshoot common issues in Kubernetes, including networking, storage, and application problems.","excerpt":"Learn how to troubleshoot common issues in Kubernetes, including …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" Documentation 🔗 Learn how to use Exoscale’s cloud services in our community section.\ncommunity.exoscale.com/documentation\nQuick Start Guides\nHow To’s\nTools\nOrganization Setup\n","categories":"","description":"","excerpt":" Documentation 🔗 Learn how to use Exoscale’s cloud services in our …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/exoscale/content/community/","tags":"","title":"Community"},{"body":" Security Referential 🔗 Organizations can need help maintaining an ever-growing number of compliance standards and security frameworks.\nExoscale has extensive compliance documentation for various standards and security frameworks, making it easy for organizations to meet all the requirements.\nThe Security Referential is a set of standards and best practices that we follow to ensure the security of our infrastructure and services. It includes guidelines on access control, network security, data protection, and incident response.\nThe Security Referential is based on industry standards and is regularly updated to address new security threats and vulnerabilities. Customers can use the Security Referential as a reference for their security needs and compliance requirements.\nCertified Security 🔗 The security of your data is our highest priority, and we work hard to ensure that our platform meets the highest security standards.\nWe believe trust is essential. Therefore, we regularly undergo third-party audits to help you meet your compliance obligations.\nSecure Control Framework 🔗 To maintain that documentation, Exoscale has developed its security referential based on the 32 Control Domains of the SCF framework.\nThis referential allows us to implement a single set of security controls while meeting all the requirements of the targeted standards and frameworks.\nOur Security Control Domains 🔗 Security \u0026 Privacy Governance Asset Management Business Continuity \u0026 Disaster Recovery Capacity \u0026 Performance Planning Change Management Cloud Security Compliance Configuration Management Continuous Monitoring Cryptographic Protections Data Classification \u0026 Handling Embedded Technology Endpoint Security Human Resources Security Identification \u0026 Authentication Incident Response Information Assurance Maintenance Mobile Device Management Network Security Physical \u0026 Environmental Security Privacy Project \u0026 Resource Management Risk Management Secure Engineering \u0026 Architecture Security Operations Security Awareness \u0026 Training Technology Development \u0026 Acquisition Third-Party Management Threat Management Vulnerability \u0026 Patch Management Web Security ","categories":"","description":"","excerpt":" Security Referential 🔗 Organizations can need help maintaining an …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/exoscale-compliance/content/security/","tags":"","title":"Compliance Security"},{"body":" Containers 🔗 Containers 🔗 Video: Containers Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Containers 🔗 Containers 🔗 Video: Containers Your browser does not …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/containers/content/containers/","tags":"","title":"Containers"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd-and-providers/","tags":"","title":"CSRD \u0026 Providers"},{"body":" DEAMONSETs 🔗 DaemonSets have many use cases – one frequent pattern is to use DaemonSets to install or configure each host node. DaemonSets provide a way to ensure that a Pod copy is running on every node in the cluster. As a cluster grows and shrinks, the DaemonSet spreads these specially labelled Pods across all nodes.\n","categories":"","description":"","excerpt":" DEAMONSETs 🔗 DaemonSets have many use cases – one frequent pattern is …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-building-blocks/content/deamonset/","tags":"","title":"DEAMONSETs"},{"body":" Deploy Node.js Application 🔗 Clone this repo and cd into quickstarts/tutorials/hello-kubernetes/deploy directory to get the Node.js and Python YAML files. Import the Node.js application into Meshery. The Dapr annotations in the manifest file will let the Dapr sidecar injector know that it is supposed to inject a sidecar container into this pod during creation.\nannotations: dapr.io/enabled: \"true\" dapr.io/app-id: \"nodeapp\" dapr.io/app-port: \"3000\" dapr.io/enable-api-logging: \"true\" Click the Actions button and deploy. Deploy Python Application 🔗 Follow previous steps to import and Deploy the python application.\nSwitch to visualizer and filter according to these specifications:\nFor View Selector select Single Namespace. For Kinds select the resources you want to see including Deployments, Pods, Services, Replicaset. For Labels select app=node and app=python. You should see the daprd sidecar containers in both the python and node pods.\n","categories":"","description":"","excerpt":" Deploy Node.js Application 🔗 Clone this repo and cd into …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/explore-dapr-with-meshery/dapr/deploy-python-and-nodejs-application/","tags":"","title":"Deploy Python and NodeJS application"},{"body":" Details 🔗 Exoscale’s cloud-native DNS provides a powerful solution for businesses looking to take complete control of their DNS and automate deployments. With Exoscale’s DNS, users can easily manage new records and zones, giving them complete control over their infrastructure. Exoscale’s DNS is also built on an anycast network, providing low-latency resolution for users worldwide. This ensures users can access their applications quickly and easily without delays or interruptions.\nFeatures Overview:\nAll common records available GEO replication Easy redirects ALIAS support Anycast DNS Per zone pricing Powered by DNSimple Easily integrate with Let’s Encrypt Exoscale’s DNS also offers geo-replicated redundancy, providing optimal uptime and ensuring that users’ applications are always available, even in a failure. Overall, Exoscale’s cloud-native DNS is a robust and reliable solution for businesses looking to manage their DNS and ensure the availability of their applications. NOTE! Here, you can find all the details in the online documentation for DNS.\n","categories":"","description":"","excerpt":" Details 🔗 Exoscale’s cloud-native DNS provides a powerful solution …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/products/content/dns/","tags":"","title":"DNS"},{"body":" Overview 🔗 We design our servers and custom storage node chassis from carefully selected components with energy efficiency in mind and a strong focus on reducing energy requirements and operating costs.\nBenefits 🔗 Energy-efficient design refers to the practice of designing buildings, appliances, and other systems in a way that reduces energy consumption and waste. This can be achieved through various strategies such as using energy-efficient materials, optimizing insulation and ventilation, incorporating renewable energy sources, and implementing intelligent control systems. The goal of energy-efficient design is to minimize the amount of energy needed to operate a building or system, reducing energy costs and positively impacting the environment by reducing greenhouse gas emissions.\n","categories":"","description":"","excerpt":" Overview 🔗 We design our servers and custom storage node chassis from …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/sustainable-cloud/content/efficient-design/","tags":"","title":"Energy-Efficient Design"},{"body":" History of Databases 🔗 Part 1 of the History of Database Evolution covers the events until the initiation of an industry around database technologies. This was also illustrated in the last unit and the linked History of Databases YouTube video.\nPart 2 of the History of Database Evolution is about the influence of new technologies and the emergence of new players with new clever ideas (new database models). Today is all about combining, improving, optimizing, and automizing all available database models and their respective implementations (closed source and open source). The future is focused on driving business innovations with specialized database models specifically built/developed for business requirements.\nA glimpse of the future can be found in this excellent wired.com blog post infographic from approx. 2016 (The Future of the Database – © wired.com). Unfortunately, the article is not available anymore, but the graphic survived on the internet. It gives an excellent summary of the past and predicts the future, as seen in 2016.\n","categories":"","description":"","excerpt":" History of Databases 🔗 Part 1 of the History of Database Evolution …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/databases/content/history/","tags":"","title":"History of Databases"},{"body":" Introduction 🔗 Meshery Designs provide a visual blueprint for your infrastructure, showcasing the various components and their relationships. These designs can encompass Kubernetes resources and their relationships, annotations, and the numerous cloud-native and application integrations that Meshery supports.\nIn this guide, you will learn how to identify these components and gain a clear understanding of their significance within the design.\nMeshery Designs and Models 🔗 Meshery Designs leverage the power and flexibility of Meshery Models as their foundation.\nMeshery Models are standardized packages that bundle components and characterize their relationships. Models provide a structured, reusable, and extensible way to represent infrastructure components and their relationships, going beyond just code. This enables a more flexible and adaptable approach to modeling complex systems.\nUnderstanding Meshery Components 🔗 A Meshery component is a unit within a Meshery model, essentially representing and defining specific infrastructure elements or resources. They represent and configure specific resources, like network setups, service configurations, and deployment details.\nFor example:\nA Kubernetes model will have components that represent resources such as Deployments, Services, ConfigMaps etc Similarly, an AWS EC2 model includes components such as NAT Gateway, Route Table and Subnet. These components encapsulate the resources and include their detailed configurations and interaction rules within the larger infrastructure. Not only do they serve as blueprints, but they can also be integrated and deployed to your cluster using Meshery, transforming your designs into reality!\nMeshery Design at a Glance 🔗 The design above shows a simple Meshery Design that consists of Kubernetes components. It specifically illustrates some kubernetes resources that are required to deploy an application to a Kubernetes cluster.\nIn the Meshery Design, you can see various Kubernetes components that make up this infrastructure, including services, deployments, and custom resources. The relationships between these components are also clearly depicted through arrows.\nMeshery Component Representation 🔗 Components are represented by various shapes and icons that effectively illustrate their roles in the infrastructure. These icons are designed to intuitively convey their functions, making them easy to identify. Users also have the flexibility to customize these icons to suit their preferences, enabling a more personalized design experience.\nMajor Meshery components that can be found in a design include:\nKubernetes Components: Resources like Deployments, Services, and ConfigMaps. Meshery Integration Components: Configurable components that can be deployed to a cluster, representing integrations with various applications and cloud native infrastructure. For example, resources such as Subnet, VPC, and NAT Gateway are specific to the AWS integration with Meshery. First let’s take a look at the Kubernetes Components.\nKubernetes Components 🔗 Kubernetes components are represented by blue icons, distinguishing them from other component types. These icons and shapes visually signify the roles and functions of each Kubernetes component within your infrastructure.\nAs you familiarize yourself with Meshery Designs, you’ll quickly recognize the unique icons and shapes representing these Kubernetes components.\nBasic Concepts of Kubernetes Components in Meshery 🔗 Some Kubernetes components can be categorized by their distinct shapes and icons, which help distinguish them based on their roles within the system.\nThis makes it intuitive to identify each component’s function, enhancing the user experience when designing and interpreting infrastructure blueprints.\nLet’s explore a few of these categories.\nTriangles:\nCategory: Networking and Service Management.\nDescription: Triangular shapes are used for components that manage or interact with networking and service-related functions. This includes defining how services are exposed and connected within the cluster.\nExamples: API Service, Service.\nRectangles:\nCategory: Hierarchical and Parent Components\nDescription: In Meshery, hierarchical or parent components are represented as transparent squares, which signify their capacity to contain or organize other resources within the cluster.\nAs elements are added, these squares expand into rectangles, visually reinforcing their role in managing and structuring resources. The transparency of these shapes highlights their function as parent components, differentiating them from other types of Kubernetes resources.\nExamples: Namespace, Node, Pod, ReplicaSet, DaemonSet, Horizontal Pod Autoscaler.\nUser Icons:\nCategory: Role-based access control (RBAC) components\nDescription: These components are related to user permissions and access management in Kubernetes.\nExamples: ClusterRole, Role, Service Account etc.\nCylinders:\nCategory: Storage and Stateful Components\nDescription: In Meshery, cylindrical shapes are used to represent components associated with storage and state management in Kubernetes. These shapes symbolize persistent storage and resources that manage or interact with storage systems.\nExamples: CSI Node, CSI Driver, CSI Storage Capacity, Persistent Volume, Persistent Volume Claim (PVC), StatefulSet, Volume Attachment, Storage Class.\nComponent Shape Guide To learn more about the usage of shapes and why specific shapes were chosen for Kubernetes resources, see the Component Shape Guide. Kubernetes Components Categories These categories also serve as a guide for users customizing Kubernetes components. By using these established shapes and icons as a reference, you can ensure that your customizations maintain semantic meaning and align with the existing representations, helping to preserve clarity and consistency in your designs. Arrows in Meshery Designs 🔗 Arrows in Meshery designs visually represent relationships between components. Meshery uses two primary arrow styles: one for edge relationships and another for annotations. These styles come with default meanings assigned by Meshery, providing a clear visual language for understanding component interactions.\nNote that users have the flexibility to customize these arrow styles to suit their design preferences.\nEdge Relationships\nThese are used to indicate traffic flow and relationships between components. The dashed lines represent dynamic interactions and data movement, while the arrowheads show the direction of the flow or relationship.\nAnnotations\nThese are used to denote annotations or static connections between components. They serve to illustrate fixed relationships or highlight specific details without implying a flow of data or interaction.\nExamples of Edge Relationships 🔗 Edge-Network Relationship: Represented by a dashed arrow with port/network protocol. Dashed arrow: Indicates that the service is linked to the pod, exposing network access to it through a specified port. Port/Network Protocol: Indicates the Port exposed by the service and its corresponding network protocol. Creating Edge-Network Relationships Between Kubernetes Components\nEdge-Permission Relationships: Represented by a dashed arrow with an intermediary component, this type of edge indicates a binding relationship between two components. The intermediary component (such as a RoleBinding) connects the two, defining how permissions are assigned. For example, a dashed arrow from a Role to a ServiceAccount with a RoleBinding in the middle shows the connection established by the RoleBinding, which links the specific role to the service account and grants the appropriate permissions.\nCreating Edge-Permission Relationships Between Components\nTo see more examples on Edge relationships, See Relationships.\nMeshery Integration Components 🔗 Meshery extends and offers support for numerous integrations with Cloud native infrastructure and applications.\nIn Meshery, these integrated components are distinctly represented by their specific logos as icons and various shapes. These components can be found in Components in the dock.\nExample of integration components in Meshery 🔗 Below is a Meshery Design with AWS Components.\nMethods to Identify Components on the Playground Design Canvas 🔗 Search on the Dock: Start by navigating to the Dock at the bottom of the design canvas. To locate Kubernetes components, click on the Kubernetes icon. You can then either search directly for the resource name you are interested in or scroll through to find it.\nConfiguration Tab: Click on a component within the design, and a configuration tab will open. The name of the component will appear at the top of this tab.\nAnnotations 🔗 In a Meshery design, annotations can take various forms, such as labels, arrows, sections used to group components, or non-configurable components like plain shapes, AWS icons, GCP icons, and flowchart shapes.\nThese annotations are used solely for design purposes within Meshery. While these annotations are not deployable or functional components, they serve to enhance the design by providing additional context and illustrating connections between components.\nAnnotations can be created using Meshery’s Diagramming tools found in the dock at the bottom of the design canvas.\nAnnotation Examples 🔗 Descriptions/labels: This gives details on components. Can be created using the Textbox in the dock.\nArrows: These are used to show relationships between components.\nSections: This can be used to visually group components into sections to have a better understanding of your infrastructure. This is created using Sections in the dock.\nShapes: These include the plain shapes, AWS icons, GCP icons, and flowchart shapes. They can be founf in Shapes in the dock. These are non-configurable and are used solely for design purposes.\nImages: Images can be added to the design to represent components, logos, or any visual aid that supports understanding the architecture.\nThe design below includes annotations such as sections, images, arrows, and various shapes.\nCustomizing Components and Annotation 🔗 While default shapes, colors, and icons are provided for components and annotations, you have the flexibility to customize these components to better suit your design needs.\nCustomizing Kubernetes Components\nCreating and Customizing Arrow Annotations\nEdge Style Guide To learn more about the available arrow edge styles, see the Edge Style Guide. Every other Component and Annotation that has been discussed can be customized using the method shown above.\nConclusion 🔗 In this guide, you’ve explored the diverse components within Meshery designs and learned to interpret their various meanings. You’ve gained insights into identifying Kubernetes components and other integrations supported by Meshery, understanding their unique roles and functions.\nWe’ve also covered how to differentiate between functional components, their relationships, and annotations within your designs. By recognizing these elements and their visual representations, you can better understand and manage your Meshery designs. This knowledge will enable you to create more effective and insightful designs, enhancing your ability to visualize and manage complex systems.\n","categories":"","description":"This chapter delves into the process of understanding the visually expressive language of Meshery designs.","excerpt":"This chapter delves into the process of understanding the visually …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-meshery/introduction-to-meshery/interpreting-meshery-designs/","tags":"","title":"Interpreting Meshery Designs"},{"body":"","categories":"","description":"One-off and scheduled tasks","excerpt":"One-off and scheduled tasks","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/job/","tags":"","title":"Jobs \u0026 CronJobs"},{"body":" Longhorn Theory 🔗 Longhorn provides a storage class that Kubernetes can use to define a persistent volume claim (PVC).\nLonghorn is a technology that enables local, non-persistent storage provided by the worker nodes as highly available storage for stateful applications with the need for persistent storage, e.g., a database.\nLonghorn Theory 🔗 Video: Longhorn Theory Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Longhorn Theory 🔗 Longhorn provides a storage class that Kubernetes …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/storage/content/longhorn-theory/","tags":"","title":"Longhorn Theory"},{"body":" Described 🔗 Accountability Standards and Frameworks Sustainability Performance Reporting sustainability performance and strategies is vital for accountability and transparency. Tools like the Global Reporting Initiative (GRI) and Sustainable Accounting Standards Board (SASB) and frameworks like the EU Corporate Sustainability Reporting Directive (CSRD) set standards for reporting on environmental, social, and governance (ESG) criteria.\n7 Factors to Make it Work! 🔗 Leadership commitment: A firm commitment from top management is essential to prioritizing sustainability goals and driving change throughout the organization.\nEmployee engagement: Employees are critical in implementing sustainability initiatives and driving continuous improvement. Companies should actively engage and empower their employees to participate in sustainability efforts.\nIntegration into business strategy: Sustainability should be integrated into the overall business strategy rather than treated as a separate initiative. This ensures that sustainability goals are aligned with the organization’s core objectives and priorities.\nMeasurement and reporting: Companies should establish key performance indicators (KPIs) and regularly measure and report on their sustainability performance. This allows them to track progress, identify areas for improvement, and communicate their achievements to stakeholders.\nCollaboration and partnerships: Collaboration with stakeholders, including suppliers, customers, and local communities, is essential for advancing sustainability goals. Companies should seek opportunities to collaborate with others to leverage resources, share best practices, and drive collective action.\n**Innovation and continuous **improvement: Companies should continuously innovate and seek new ways to improve their sustainability performance. This may involve adopting new technologies, processes, or business models to minimize environmental impact and enhance social and economic benefits.\nTransparency and accountability: Companies should be transparent about their sustainability efforts and communicate openly with stakeholders about their challenges, progress, and achievements. Accountability mechanisms should be in place to ensure that the organization is held responsible for meeting its sustainability.\n","categories":"","description":"","excerpt":" Described 🔗 Accountability Standards and Frameworks Sustainability …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/corporate-sustainability/content/make-it-work/","tags":"","title":"Make it Work"},{"body":" Manifest Theory 🔗 The beauty of Kubernetes lies in its descriptive/declarative nature of infrastructure management and operation. Instead of writing a series of single commands on the CLI, you write it down in a stateless manifest; the format used is a .yaml - file.\nThe manifest concept details and how to compose them are laid out in the video below.\nManifest Theory 🔗 Video: Manifest Theory Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Manifest Theory 🔗 The beauty of Kubernetes lies in its …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/manifest-theory/","tags":"","title":"Manifest Theory"},{"body":" Explained 🔗 Microservice architecture is a software development approach that structures an application as a collection of small, independent, and loosely coupled services.\nEach service is designed to perform a specific business function and communicates with other services through well-defined APIs. Microservices are typically deployed independently and can be scaled individually, allowing for greater flexibility and agility in software development. The microservice architecture emphasizes modularity, resilience, and fault tolerance and is often used in cloud-native and DevOps environments.\nOne of the main advantages of microservice architecture is that it allows for greater agility and flexibility in software development. Since each service is independent, changes can be made to one service without affecting the others, making testing and deploying new features more accessible. Additionally, microservices can be scaled independently, allowing businesses to allocate resources where they are needed most. This can result in cost savings and improved performance. However, there are also some potential drawbacks to microservice architecture. For example, managing multiple services can be complex and require high team coordination and communication. Additionally, microservices can introduce additional points of failure, making it more difficult to troubleshoot and diagnose issues. Despite these challenges, microservice architecture is becoming increasingly popular in modern software development, allowing businesses to build more flexible, scalable, and resilient applications.\n","categories":"","description":"","excerpt":" Explained 🔗 Microservice architecture is a software development …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/cloud-native/content/microservices/","tags":"","title":"Microservices"},{"body":"This section is a refresher that provides an overview of the main Kubernetes resources related to networking. At the end of this section, please complete the exercises to put these concepts into practice.\nKubernetes Networking Model 🔗 In Kubernetes, a network plugin ensures communication between the Pods. Each network plugin must implement the following requirements:\nall Pods can communicate with all other Pods without NAT all Nodes can communicate with all Pods without NAT the IP that a Pod sees itself as is the same IP that others see it as Communication Types 🔗 There are different types of communication within a cluster:\nContainer to container Pod to Pod on the same Node on different Nodes Pod to Service External to Service NodePort service LoadBalancer Service Ingress Controller Container to Container Communication 🔗 When a Pod is created, it has its own network namespace which is set up by a pause container. This container is special as it does not run any workload and is not visible from the kubectl commands. The other containers of this Pod are all attached to the pause container’s network namespace and are thus communicating through localhost.\nPod to Pod on the Same Node 🔗 Each Pod has its own network namespace and they communicate via a virtual Ethernet (veth) pair connected to a bridge on the host. This setup allows Pod-to-Pod traffic to be switched locally without leaving the Node.\nPod to Pod Across Different Nodes 🔗 The network plugin ensures that each Pod’s IP is routable across the cluster, using encapsulation, overlays, or native routing. Packets travel across the network infrastructure between Nodes before reaching the destination Pod’s virtual interface.\nNetwork Plugin 🔗 A Network plugin is mandatory in a Kubernetes cluster. It ensures the communication between Pods across the cluster, whether they are on the same Node or on different Nodes.\nAmong the network plugins available, Kubenet is a basic and simple one, it has a limited set of functionalities and cannot be used with kubeadm. All other major plugins implement the Container Networking Interface (CNI) specification.\nhttps://github.com/containernetworking - CNCF incubated project https://cncf.io/projects - Manages containers’ network connectivity More info: https://bit.ly/about-cni-plugins Container Network Interface (CNI) 🔗 CNI provides a standard interface for configuring network interfaces in Linux containers, allowing plugins to implement their own advanced functionalities such as routing, network security, and more.\nPopular CNI plugins include:\nCilium - advanced security, eBPF-based Calico - policy engine, supports BGP Flannel - simple, uses VXLAN by default These plugins are typically:\nInstalled as a DaemonSet Designed for different use cases (low latency, enhanced security, observability, etc.) Can use encapsulation (L2/VXLAN) or non-encapsulated (L3/BGP) modes depending on your setup and requirements Communication Types 🔗 Encapsulated (VXLAN) 🔗 Traffic between Pods is wrapped (encapsulated) in another packet and routed through an overlay network.\nUnencapsulated (BGP) 🔗 Traffic is routed directly between nodes without encapsulation, using protocols like BGP to advertise Pod networks.\nService 🔗 A Service is a Kubernetes resource that provides a stable networking endpoint to expose a group of Pods. Services ensure that communication to a group of Pods is reliable, even as Pods are dynamically created or destroyed.\nMain types of Services:\nClusterIP (default): exposes the Service internally within the cluster. Not accessible from outside NodePort: exposes the Service on a static port on each Node of the cluster LoadBalancer: creates an external load balancer (only available when using a cloud provider) to expose the Service to the internet Key Characteristics:\nEach Service is assigned a Virtual IP (VIP), which stays the same during the lifecycle of the Service Services use labels and selectors to dynamically group Pods kube-proxy configures network rules on Nodes to route traffic from the Service to the appropriate Pods Service of Type ClusterIP 🔗 A Service of type ClusterIP exposes a group of Pods inside the cluster, so that other Pods can reach them.\nService of Type NodePort 🔗 A Service of type NodePort exposes a group of Pods to the external world, opening the same port on each Node.\nService of Type LoadBalancer 🔗 A Service of type LoadBalancer exposes a group of Pods to the external world through a load balancer. This feature is only available for clusters running on cloud providers.\nEndpoints 🔗 Endpoints resources are the list of IP:PORT of the pods exposed by a Service. Endpoints are updated each time a Pod is created/updated. The commands below create a Deployment with 3 Pods and expose them with a Service.\nCreation of a deployment kubectl create deploy ghost --replicas=3 --image=ghost:4 Exposition through a service kubectl expose deploy/ghost --port=2368 We can query the Endpoints directly.\n$ kubectl get endpoints ghost NAME ENDPOINTS AGE ghost 10.0.0.210:2368,10.0.0.25:2368,10.0.1.252:2368 51s Or, get the list of Endpoints from the Service’s details.\n$ kubectl describe svc ghost Name: ghost Namespace: default Labels: app=ghost Annotations: \u003cnone\u003e Selector: app=ghost Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.107.128.221 IPs: 10.107.128.221 Port: \u003cunset\u003e 2368/TCP TargetPort: 2368/TCP Endpoints: 10.0.1.252:2368,10.0.0.210:2368,10.0.0.25:2368 Session Affinity: None Internal Traffic Policy: Cluster Events: \u003cnone\u003e Pod to Service Communication 🔗 By default, kube-proxy sets the network rules when we create a Service. These rules allow access to the backend Pods when accessing the Service.\nkube-proxy currently uses iptables as the default data-plane mode, but another mode can be enabled instead, such as IPVS or eBPF (via Calico or Cilium network plugins).\nThis article provides additional information about how the iptables rules are created.\nIngress Controller 🔗 An Ingress Controller is a reverse proxy exposing ClusterIP services to the outside. It’s usually the single entry point exposed by an external load balancer.\nAn Ingress Controller is configured with resources of type Ingress which allows L7 routing via the domain name or a path within the URL. Below is an example of Ingress specification. It redirects all traffic targeting the /api endpoint to the api Service listening on port 80.\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - http: paths: - path: /api pathType: Prefix backend: service: name: api port: number: 80 ","categories":"","description":"Understand Pod to Pod communication, Service discovery, Ingress resources.","excerpt":"Understand Pod to Pod communication, Service discovery, Ingress …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/networking/","tags":"","title":"Networking"},{"body":"","categories":"","description":"A scalable storage system that manages data as objects with metadata, perfect for unstructured data like media and backups. ","excerpt":"A scalable storage system that manages data as objects with metadata, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/object-storage/","tags":"","title":"Object Storage"},{"body":" Positoning 🔗 Kubernetes is … 🔗 … providing the building blocks for creating developer and infrastructure platforms but preserves user choice and flexibility where it is essential.\n… extensible, and lets users integrate their logging, monitoring, alerting, and many more solutions because it is not monolithic, and these solutions are optional and pluggable.\nKubernetes is NOT … 🔗 … a traditional, all-inclusive PaaS system.\nKubernetes operates at the container level rather than at the hardware level. It provides some generally helpful features common to PaaS offerings, such as deployment, scaling, load balancing.\n… a mere orchestration system.\nIt eliminates the need for orchestration. The definition of orchestration is executing a defined workflow:\nfirst, do A, then B, then C → imperative\nKubernetes comprises independent, composable control processes that continuously drive the current state:\ntowards the desired state → declarative\nKubernetes does NOT … 🔗 … limit the types of applications supported.\nKubernetes aims to support a highly diverse workload, including stateless, stateful, and data-processing workloads. If an application can run in a container, it should run great on Kubernetes.\n… deploy source code and does not build your application.\nOrganizational cultures determine Continuous Integration, Delivery, and Deployment (CI/CD) workflows and preferences and technical requirements.\n… provide application-level services;\nsuch as middleware, data-processing frameworks, databases, caches, nor cluster storage systems as built-in services. Application access the components mentioned above through portable mechanisms - both are running Kubernetes.\n…provide nor adopt any comprehensive machine management.\nThe task requires additional components for system configuration, system management \u0026 maintenance, etc…\n","categories":"","description":"","excerpt":" Positoning 🔗 Kubernetes is … 🔗 … providing the building blocks for …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/kubernetes/content/positioning/","tags":"","title":"Positoning"},{"body":" In this chapter, we are going to get our hands on some of the traffic management capabilities of Istio.\nApply default destination rules 🔗 Before we start playing with Istio’s traffic management capabilities, we need to define the available versions of the deployed services. In Istio parlance, versions are called subsets. Subsets are defined in destination rules.\nRun the following in the custom yaml section to create default destination rules for the Bookinfo services:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage subsets: - name: v1 labels: version: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: ratings spec: host: ratings subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v2-mysql labels: version: v2-mysql - name: v2-mysql-vm labels: version: v2-mysql-vm --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: details spec: host: details subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 --- Using Meshery, navigate to the Istio management page:\nEnter default in the Namespace field. Click the (+) icon on the Apply Service Mesh Configuration card and select Bookinfo subsets from the list. This will deploy the destination rules for all the Book info services defining their subsets. Verify the destination rules created by using the command below:\nkubectl get destinationrules kubectl get destinationrules -o yaml Configure the default route for all services to V1 🔗 As part of the bookinfo sample app, there are multiple versions of reviews service. When we load the /productpage in the browser multiple times we have seen the reviews service round robin between v1, v2 or v3. As the first exercise, let us first restrict traffic to just V1 of all the services.\nUsing Meshery, navigate to the Istio management page:\nEnter default in the Namespace field. Click the (+) icon on the Apply Custom Configuration card and paste the configuration below. To view the applied rule:\nkubectl get virtualservice To take a look at a specific one:\nkubectl get virtualservice reviews -o yaml Please note: In the place of the above command, we can either use kubectl or istioctl.\nConfig:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 --- Now when we reload the /productpage several times, we will ONLY be viewing the data from v1 of all the services, which means we will not see any ratings (any stars).\nContent-based routing 🔗 Let’s replace our first rules with a new set. Enable the ratings service for a user jason by routing productpage traffic to reviews v2:\nUsing Meshery, navigate to the Istio management page:\nEnter default in the Namespace field. Click the (+) icon on the Apply Custom Configuration card and paste the configuration below. This will update the existing virtual service definition for reviews to route all traffic for user jason to review V2.\nIn a few, we should be able to verify the virtual service by using the command below:\nkubectl get virtualservice reviews -o yaml Config:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1 --- Now if we login as your jason, you will be able to see data from reviews v2. While if you NOT logged in or logged in as a different user, you will see data from reviews v1.\nCanary Testing - Traffic Shifting 🔗 Canary testing w/50% load 🔗 To start canary testing, let’s begin by transferring 50% of the traffic from reviews:v1 to reviews:v3 with the following command:\nUsing Meshery, navigate to the Istio management page:\nEnter default in the Namespace field. Click the (+) icon on the Apply Custom Configuration card and paste the configuration below. This will update the existing virtual service definition for reviews to route 50% of all traffic to review V3.\nIn a few, we should be able to verify the virtual service by using the command below:\nkubectl get virtualservice reviews -o yaml Config:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 50 - destination: host: reviews subset: v3 weight: 50 --- Now, if we reload the /productpage in your browser several times, you should now see red-colored star ratings approximately 50% of the time.\nShift 100% to v3 🔗 When version v3 of the reviews microservice is considered stable, we can route 100% of the traffic to reviews:v3:\nUsing Meshery, navigate to the Istio management page:\nEnter default in the Namespace field. Click the (+) icon on the Apply Custom Configuration card and paste the configuration below. This will update the existing virtual service definition for reviews to route 100% of all traffic to review V3.\nIn a few, we should be able to verify the virtual service by using the command below:\nkubectl get virtualservice reviews -o yaml Config:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v3 --- Now, if we reload the /productpage in your browser several times, you should now see red-colored star ratings 100% of the time.\nAlternative: Manual installation 🔗 Follow these steps if the above steps did not work\nDefault destination rules 🔗 Run the following command to create default destination rules for the Bookinfo services:\nkubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml Route all traffic to version V1 of all services 🔗 kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml Route all traffic to version V2 of reviews for user Jason 🔗 kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml Route 50% of traffic to version V3 of reviews service 🔗 kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml Route 100% of traffic to version V3 of reviews service 🔗 kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-v3.yaml ","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/introduction-to-service-meshes/routing-and-canary/","tags":"","title":"Request Routing and Canary Testing"},{"body":"","categories":"","description":"This STARTER - Learning Path is the foundation for building Exoscale knowledge. It will help you learn the terminology associated, the related cloud computing, and the Exoscale-specific benefits for customers.","excerpt":"This STARTER - Learning Path is the foundation for building Exoscale …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/","tags":"","title":"STARTER"},{"body":" Deploy the Resources 🔗 Click Actions in the top right corner and click on Deploy. Figure: Actions dropdown menu\nThe design will be validated to make sure there are no errors. Figure: Validate design\nChoose the Kubernetes cluster you want to deploy to. Figure: Choose deployment Environment\nNote: The Meshery Playground is connected to live Kubernetes cluster(s) and allows users full control over these clusters but you can also have the option to add your own Kubernetes cluster to the Playground.\nFinally click Deploy to deploy the application to the cluster. Figure: Deploy resources\nTo check the status of your deployment, click on the notification icon on the top right corner. You can click on Open In visualizer to navigate to the Visualize section or follow the steps below. Figure: Deploy resources\n","categories":"","description":"In this section you will validate your design and deploy the resources to a Kubernetes cluster","excerpt":"In this section you will validate your design and deploy the resources …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/deploying-wordpress-and-mysql-with-persistent-volumes-with-meshery/sql/deploy/","tags":"","title":"Validate and Deploy Resources"},{"body":"The VotingApp is mainly used for demos and follows a microservices architecture. While it may not adhere to all architectural best practices, it is a good example of an application that utilizes various languages and databases. It helps in learning concepts related to Docker and Kubernetes. The VotingApp consists of 7 microservices, as illustrated in the following diagram:\nvote-ui: A Vue.js frontend that allows users to choose between Cat and Dog vote: A backend API built with Python / Flask redis: A database where votes are stored worker: A service that retrieves votes from Redis and stores the results in a Postgres database db: The Postgres database where vote results are stored result: A backend that sends the scores to a user interface via websocket result-ui: An Angular frontend that displays the voting results The container images for each microservice are available in the DockerHub. Their tags follow the Semantic Versioning pattern (vX.Y.Z). You can see the application running at https://vote.votingapp.xyz.\nSince the VotingApp is packaged in a Helm Chart and distributed in the Docker Hub, it’s ready to be deployed on Kubernetes.\n","categories":"","description":"Introduction to a toy application","excerpt":"Introduction to a toy application","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/votingapp/","tags":"","title":"Voting App"},{"body":"","categories":"","description":"Learn why using a managed Kubernetes service can simplify your container orchestration and enhance your development workflow.","excerpt":"Learn why using a managed Kubernetes service can simplify your …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-managed-kubernetes/","tags":"","title":"Why Managed Kubernetes?"},{"body":" Basic Commands 🔗 To get a feeling for this new world, let’s have a look at some simple applications of the kubectl command.\nBasic K8s Commands 🔗 Video: Basic K8s Commands Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Basic Commands 🔗 To get a feeling for this new world, let’s have a …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/kubernetes/content/basic-commands/basic-commands/","tags":"","title":"Basic Commands"},{"body":" Benefits of Databases 🔗 Why use computerized Databases? 🔗 Because it makes it easier to:\nsort data search and find data add, edit or delete data store large data sets efficiently access data at the same time by multiple users import and export data from and to other applications If there are advantages, then there are also disadvantages, and to get a complete picture of the database situation, let us contrast them.\nAdvantages of Databases: 🔗 Data Sharing Data Security Data Abstraction Concurrent Access Easy Data Manipulation Support Multi-User Views Data Redundancy Controlling Data Inconsistency Minimizing Disadvantages of Databases: 🔗 Cost of Software Cost of Hardware Cost of Staff Training Cost of Data Conversion Complexity of High Availability Complexity of Backup \u0026 Restore ","categories":"","description":"","excerpt":" Benefits of Databases 🔗 Why use computerized Databases? 🔗 Because it …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/databases/content/benefits/","tags":"","title":"Benefits of Databases"},{"body":"","categories":"","description":"A low-latency storage option that stores data in fixed-size blocks, ideal for databases and high-performance applications.","excerpt":"A low-latency storage option that stores data in fixed-size blocks, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/block-storage/","tags":"","title":"Block Storage"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-sustainability/csrd-and-exoscale/","tags":"","title":"CSRD \u0026 Exoscale"},{"body":" Locations 🔗 All Exoscale zones are hosted in carefully selected state-of-the-art data centers.\nCertifications 🔗 Data centers must pass stringent criteria defined in our requirements, including various security and quality certifications.\n","categories":"","description":"","excerpt":" Locations 🔗 All Exoscale zones are hosted in carefully selected …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/exoscale-compliance/content/data-center/","tags":"","title":"Data Center"},{"body":" Overview 🔗 We ship our servers directly from the distributor to our data centers, avoiding unnecessary equipment transport and reducing our carbon footprint.\nBenefits 🔗 Direct shipping refers to shipping goods directly from the manufacturer or supplier to the customer, without any intermediaries such as wholesalers or retailers. In this method, the manufacturer or supplier handles the entire shipping process, including packaging, labeling, and delivery. Direct shipping is often used by e-commerce businesses that sell products online, allowing them to reduce costs and streamline their operations. It also allows for faster delivery times and greater control over the customer experience.\nDirect shipping can have a positive impact on sustainability in several ways:\nReduced carbon footprint - Direct shipping eliminates the need for intermediaries, which can reduce the carbon footprint associated with transportation and logistics.\nLess packaging waste - Direct shipping can lead to less packaging waste, as the manufacturer or supplier can use more sustainable packaging materials and avoid the need for additional packaging from intermediaries.\nEfficient use of resources - Direct shipping allows manufacturers and suppliers to be more efficient in using resources, as they can control the entire shipping process and optimize their operations accordingly.\n","categories":"","description":"","excerpt":" Overview 🔗 We ship our servers directly from the distributor to our …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/sustainable-cloud/content/direct-shipping/","tags":"","title":"Direct Shipping"},{"body":" Dockerfile 🔗 FROM node:12-alpine * to be based on which other Docker image? RUN apk add --no-cache python g++ make * install additional software WORKDIR /app * create and use directory /app inside the container COPY . . * 1st parameter: local directory (./ is the directory where the Dockerfile is) * 2nd parameter: target directory inside the container * → copy everything where the Dockerfile is in, into the container under /app RUN npm install * dependencies for the application CMD [\"node\", \"src/index.js\"] * run the app - just like when running outside a container Dockerfile Theory 🔗 Video: Dockerfile Theory Your browser does not support the video tag. Dockerfile Practice 🔗 Video: Dockerfile Practice Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Dockerfile 🔗 FROM node:12-alpine * to be based on which other Docker …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/containers/content/dockerfile/","tags":"","title":"Dockerfile"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/technical-security/","tags":"","title":"Exoscale's Technical Security"},{"body":" In this chapter we will learn how to test the resiliency of an application by injecting systematic faults. Before we start, we will need to reset the virtual services.\nUsing Meshery, navigate to the Istio management page:\nEnter default in the Namespace field. Click the (+) icon on the Apply Custom Configuration card and paste the configuration below. Config:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 Inject a route rule to create a fault using HTTP delay 🔗 To start, we will inject a 7s delay for accessing the ratings service for a user jason. reviews v2 service has a 10s hard-coded connection timeout for its calls to the ratings service configured globally.\nUsing Meshery, navigate to the Istio management page:\nEnter default in the Namespace field. Click the (+) icon on the Apply Custom Configuration card and paste the configuration below. This will update the existing virtual service definition for ratings to inject a delay for user jason to access the ratings V1.\nIn a few, we should be able to verify the virtual service by using the command below:\nkubectl get virtualservice ratings -o yaml Config:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ratings spec: hosts: - ratings http: - fault: delay: fixedDelay: 7s percentage: value: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1 Now we login to /productpage as user jason and observe that the page loads but because of the induced delay between services the reviews section will show :\nError fetching product reviews! Sorry, product reviews are currently unavailable for this book.\nIf you logout or login as a different user, the page should load normally without any errors.\nInject a route rule to create a fault using HTTP abort 🔗 In this section, , we will introduce an HTTP abort to the ratings microservices for user jason.\nUsing Meshery, navigate to the Istio management page:\nEnter default in the Namespace field. Click the (+) icon on the Apply Custom Configuration card and paste the configuration below. This will update the existing virtual service definition for ratings to inject a HTTP abort for user jason to access the ratings V1.\nIn a few, we should be able to verify the virtual service by using the command below:\nkubectl get virtualservice ratings -o yaml Config:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ratings spec: hosts: - ratings http: - fault: abort: httpStatus: 500 percentage: value: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1 Now we login to /productpage as user jason and observe that the page loads without any new delays but because of the induced fault between services the reviews section will show:\nRatings service is currently unavailable.\nVerify fault injection 🔗 Verify the fault injection by logging out (or logging in as a different user), the page should load normally without any errors.\nAlternative: Manual installation 🔗 Follow these steps if the above steps did not work Route all traffic to version V1 of all services 🔗 kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml Route all traffic to version V2 of reviews for user Jason 🔗 kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml Inject 7s delay for ratings service 🔗 kubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml Inject HTTP abort for ratings service 🔗 kubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml ","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/introduction-to-service-meshes/fault-injection/","tags":"","title":"Fault Injection"},{"body":" Details 🔗 Exoscale provides various interaction methods with its platform, including programmatic access via the command line, your preferred programming language, integrations with third-party tools, and a user-friendly web portal. Regardless of the method, Identity and Access Management (IAM) will define permissions and actions for individuals and services on your platform.\nIAM is composed of 2 primary building blocks:\nRoles act as a container for a single policy and add some options. Policies are rules describing what can and cannot be done. Exoscale IAM, or Identity and Access Management, is a system that manages access to resources within the Exoscale cloud environment. Exoscale is a cloud service provider that offers various services, including computing, storage, and network solutions. Exoscale IAM enables administrators to control who has access to specific resources, manage user permissions, and enforce security policies. Here are some key features and functions of Exoscale IAM:\nUser Management Roles and Policies Access Control Security and Compliance API Access Using Exoscale IAM, organizations can effectively safeguard their cloud resources, comply with regulatory requirements, and streamline user access management, ultimately enhancing the security and efficiency of their cloud operations. IAM Users 🔗 So far, IAM has allowed you to create keys that could be restricted and fine-tuned according to their permissions. While practical and powerful, IAM Keys have always been intended for programmatic usage, while users could not be limited in scope beyond the predefined roles:\nOwner Tech Billing (former Admin) Now, we are enhancing the IAM functionality, bringing the same powerful features to organizations’ users, offering you more control and flexibility. This means you can now limit a user’s scope of action in the web portal like you would for an IAM Key, with precise and fine-grained IAM Roles. Typical use cases include:\ngive a user read-only access\ngenerally, fine-tune what a user can see or do in the web portal It is important to note that:\nAll new organizations will immediately start with IAM users\nAll existing organizations will be migrated NOTE! Here, you can find all the details in the online documentation for IAM.\n","categories":"","description":"","excerpt":" Details 🔗 Exoscale provides various interaction methods with its …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/products/content/iam/","tags":"","title":"IAM"},{"body":" INGRESS 🔗 Route traffic to and from the cluster. Provide a single SSL endpoint for multiple applications. Many implementations of an ingress allow you to customize your platform. Ingresses provide a way to declare that they should channel traffic from the outside of the cluster into destination points within the cluster. One single external Ingress point can accept traffic destined to many internal services.\n","categories":"","description":"","excerpt":" INGRESS 🔗 Route traffic to and from the cluster. Provide a single SSL …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-building-blocks/content/ingress/","tags":"","title":"INGRESS"},{"body":" Available for Exoscale 🔗 DevOps and Automation Tools we use and support.\nContainer Orchestration 🔗 Cloud native tools are embracing containers as a great way to build, ship, and run applications.\nKubernetes: One of the most active open-source projects, Kubernetes or k8s for short, offers container cluster management with powerful yet simple concepts. Exoscale provides a managed Kubernetes offering.\nCloud Controller Manager: The Kubernetes Cloud Controller Manager (CCM) implementation for Exoscale. This component enables a tighter integration of Kubernetes clusters with the Exoscale Compute platform.\nNGINX Ingress Controller: The official way of deploying the NGINX Ingress Controller on Exoscale.\nAutomation and Configuration Management 🔗 As DevOps ourselves, we use automation and configuration management tools every day. On our platform you will be able to use the best tools available.\nTerraform: Written in GO, Terraform enables you to configure the full spectrum of your infrastructure from compute instances to their DNS entries. It takes your infrastructure and makes it converge towards its desired configuration state.\nPacker: The Exoscale Packer builder plugin can be used to build Exoscale instance templates.\nVault: The Vault plugin enables a secret’s backend for Exoscale. This plugin generates Exoscale IAM API keys which can be restricted to specific operations according to predefined roles.\nPulumi: Pulumi, an open source IaC tool, helps you to build, deploying and managing cloud applications and infrastructure. It allows easier collaboration and reduces cloud complexity.\nCrossplane: Crossplane is a cloud native control plane framework to easily build control planes without needing to write code. The integration is provided by VSHN on the Marketplace.\nClients and Libraries 🔗 We support some of the best clients out there. Follow the link and find out how to integrate them your workflow with the Exoscale cloud platform.\nTraefik: Traefik is a modern HTTP reverse proxy and load balancer that easily integrates with your existing infrastructure components and runs smoothly on Exoscale.\nCLI: Easy to use CLI interface written in Golang that lets you browse and control all Exoscale resources.\nEgoscale: The official Go wrapper for the Exoscale public cloud API. Egoscale powers our CLI and many other libraries and integrations based on Go, like Kubernetes External DNS or Docker Machine.\nPython: The official Python wrapper for the Exoscale API.\nLego: Lego is an ACME library and standalone application written in Go, enabling you to automate ACME challenges and certificates deployments on Exoscale, eventually using our DNS service.\nThird party platforms integration 🔗 Discover how to integrate our cloud services with some great third-party platforms.\nGuruSquad: GS RichCopy 360 Enterprise provides enterprise-class unstructured data backup, data migration, and data replication. Whether the data is being migrated from another cloud provider or on-premise, GuruSquad can simplify all the data migration and backup jobs from a single pane of glass.\nNuvla.io: Nuvla.io, by technology partner Sixsq, is an edge-to-cloud application deployment platform. Real edge and multi-cloud solution built on open source software, Nuvla.io can deploy simple to complex containerized applications repeatedly with orchestration.\nExtend Simple Object Storage 🔗 Using these set of certified tools with SOS, you can browse, backup or map object storage from various OS and environments.\nCyberduck: This free software provides a versatile GUI for S3 compatible transfers as well as Duck, a CLI tool for power users. Both are available on Windows, Linux and macOS platforms.\nFlexify.IO Flexify.IO is a great way to migrate data to and from on-premises or other cloud storage to Exoscale Object Storage. Flexify.IO does the magic - and optimizes traffic and bandwidth on the way.\nCloudBerry: CloudBerry provides an extensive palette of tools for integrating Object Storage with Windows. You can browse storage, map a drive like a simple share or even backup your complete OS and application.\n","categories":"","description":"","excerpt":" Available for Exoscale 🔗 DevOps and Automation Tools we use and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/exoscale/content/integrations/","tags":"","title":"Integrations"},{"body":" Longhorn Praxis 🔗 This section explains how to install Longhorn with a manifest using kubectl and looks at Longhorn’s graphical UI— using the CLI and the UI to do configurations and learn about the management of the solution.\nLonghorn Praxis 🔗 Video: Longhorn Praxis Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Longhorn Praxis 🔗 This section explains how to install Longhorn with …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/storage/content/longhorn-praxis/","tags":"","title":"Longhorn Praxis"},{"body":" Manifest Praxis 🔗 Let’s look at the actual usage of the manifest concept and how to apply it to a Kubernetes cluster and create, configure, and re-configure resources with it.\nThe manifest usage details and how to execute them are laid out in the video below.\nManifest Praxis 🔗 Video: Manifest Praxis Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Manifest Praxis 🔗 Let’s look at the actual usage of the manifest …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/manifest-praxis/","tags":"","title":"Manifest Praxis"},{"body":"","categories":"","description":"Building blocks for running containers","excerpt":"Building blocks for running containers","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/pod/","tags":"","title":"Pod"},{"body":" Introduction 🔗 When working with Kanvas’ Designer Mode, collaboration is key. Design reviews allow team members to examine, provide feedback, and approve design proposals, ensuring that all components are thoroughly evaluated and potential issues are identified early. This process leads to more robust designs, informed decision-making, and a unified understanding among team members, ultimately contributing to the success of the project.\nLifecycle of Design Reviews 🔗 Submitting Designs 🔗 This initial step involves preparing your design to be reviewed by your peers. Start by submitting your design for review, ensuring all components are accurately placed and configured. It is crucial to double-check that every aspect of the design is ready for feedback. In the context of Kanvas, you can submit designs for review by:\nEnsuring Collaboration is Enabled: Make sure that collaboration mode is turned on within Kanvas so that your team members can access and review the design. Having the Design in the Same Workspace/Team: Place your design in a workspace or team where other members have access. This ensures that all relevant team members can easily find and review the design. Kanvas allows you to manage design access permissions and visibility, ensuring that only the intended collaborators can view and edit the design. Sharing the Design Link: Generate and share a link to your design with your team members. This direct approach ensures that anyone with the link can access the design immediately. Ensure that the designs’s visibility status is set appropriately (Private, Public, or Published) based on who you want to have access. Private: Only you, the creator, and the user or team with granted access permission can view and edit the design. Public: Anyone within your organization can view, comment on, and edit the design. You can still restrict permissions for individual users or teams. Published: The design becomes discoverable to other Meshery users, allowing them to view, download, and clone the design. By following these steps, you can effectively submit your designs for review within Kanvas, ensuring that they are accessible to the right team members and ready for collaborative feedback.\nReviewing Designs 🔗 Team members can review designs using comments or annotations.\nDesign Reviews with Comments 🔗 You can add comments in two convenient ways:\nCommenting via the Dock: Click on the comment icon in the dock to create a new comment, or drag and drop it anywhere on the canvas to position it. Commenting via Context Menu in Canvas:\" Right-click anywhere on your canvas and select “Add Comment Here” from the context menu or use the shortcut Ctrl + M (Command + M for Mac users). This allows for swift comment placement exactly where you need it. Tips for using Comments as a Design Review Tool 🔗 Initiating Threads: You can initiate a comment thread by adding a comment. Your comment may request a design review or provide feedback on a design. Team members can reply directly to comments, creating a structured dialogue around each point of feedback for easy referencing. Utilize Mentions: Easily draw the attention of relevant team members by tagging them in your comment. To activate this, type @ in the comment box, and a list of your team members will appear. Select who you want to mention. This ensures that the right people are notified and can contribute to the discussion. Enable Email Notifications: Customize your notification preferences to receive emails for comments on your design. When this is enabled, you’ll be notified when comments are made on your design, you’re mentioned in a comment, or someone adds a comment to a thread you’ve previously engaged with. Mute Comment Notifications: You can also customize your notification preferences to mute email notifications for comments on your design. This can be useful if you want to temporarily pause notifications or reduce email clutter. You’ll still have access to all comments directly within Kanvas. Comment on Specific Elements: Place comments to specific components, whether it’s a shape, text, or an entire component. This specificity ensures that feedback is targeted and directly related to the part of the design under discussion.\nResolving Comments 🔗 Once the feedback has been addressed, you can mark the comment as resolved. For resolving comments, simply right click on the comment component and select the “resolve” comment action. It closes the comment thread and signifies that the conversation around that feedback has concluded. It helps keep the review process organized and ensures that completed discussions are neatly archived.\nView Comment History\nEven after comments are resolved, you can view the history to understand the decisions and discussions that took place. This feature is beneficial for future reference and maintaining a clear record of the review process.\nReopening Comments 🔗 After a comment has been resolved, there might be situations where you need to reopen the discussion. Unresolving a comment allows you to reinitiate conversations, address additional concerns, or make further changes.\nTo unresolve a comment:\nClick on the comment icon at the top right of the design canvas. Select “resolved” from the dropdown menu. This will show the resolved conversations which can be reopened. This flexibility ensures that ongoing iterations and continuous improvements can be managed effectively.\nBy following these steps, you can create a thorough and collaborative review process within Kanvas, ensuring that your designs are vetted comprehensively and efficiently.\nPinning and Locking Comments in Kanvas 🔗 Pinning of Comments 🔗 This image illustrates the feature of pinning comments to specific nodes within Kanvas. This functionality allows users to attach a comment to a particular node, ensuring that the comment remains associated with the node regardless of its position within the design. This is particularly useful for providing context-specific feedback or instructions that should always be visible alongside the relevant component. You can take a comment icon near to a node and it automatically gets pinned to that node.\nHow It Works:\n* **Attach Comments:** When a comment is pinned to a component, it stays linked to that node. This means that no matter where the node is moved within the design, the comment will follow. * **Contextual Feedback:** This feature helps in maintaining the relevance of the feedback, as the comments are always displayed in relation to the specific node they refer to. * **Collaborative Consistency:** Ensures that all collaborators see the comments in the correct context, facilitating clearer communication and understanding. Locking Comments 🔗 This image shows the locking and unlocking interface for nodes in Kanvas. Right-click on the node to access the “lock” option. Locking a comment ensures that it remains fixed in its position, even if the overall design pattern changes. This is useful for highlighting and securing key components of the design that should not be altered or moved during collaborative sessions.\nHow It Works:\nLock Comments: By locking a comment, you ensure that it stays in place. This can be particularly useful for critical components that form the backbone of your design. Shared Among Collaborators: When a comment is locked, collaborators working on that design will be unable to move or reposition the locked component. This is an excellent opportunity to brainstorm suggestions for easily identifying locked components. Unlocking Comments: Comments can be unlocked when necessary, allowing for flexibility in design adjustments. This can be done through the radial menu shown in the image, which provides options for copying, resetting styles, duplicating, deleting, and unlocking nodes. Design Reviews with Annotations 🔗 Annotations in Kanvas are a powerful way to add detailed notes and explanations directly onto your design canvas. They are crucial for effective communication, detailed feedback, and collaborative design processes. Here’s how you can effectively use annotations to enhance collaboration and streamline design reviews.\nWhiteboard in Kanvas\nThe whiteboarding feature in Kanvas introduces versatile, freestyle drawing capabilities. Enabling the whiteboard feature augments your ability to diagram with a suite of predefined shapes, and pen and pencil annotations, allowing you full freedom of expression in your engineering diagrams. Annotations are a key part of this feature, enabling you to add contextual information directly to your designs.\nKey Functionalities\nDrawing Tools Integration:\nThe whiteboarding feature incorporates a comprehensive set of drawing tools resembling popular whiteboard applications. It enables you to draw shapes, group components, annotate, and highlight specific elements within the design canvas to help convey complex ideas clearly Real-time Collaboration:\nFacilitates simultaneous collaboration among multiple users within the tool. Identification of collaborators through avatars ensures clarity in collaborative sessions. Supports live editing and instant visualization of changes made by collaborators, ensuring everyone is on the same page. Non-Invasive Annotations: Annotations, shapes, or drawings created within the tool remain separate visual aids, not altering the actual infrastructure components. This means you can annotate freely without worrying about modifying the core design elements.\nUndo/Redo Functionalities: Allows for design exploration without permanent modifications, enabling you to experiment with different ideas safely.\nEnabling the Whiteboarding Feature\nIf the whiteboarding feature is not already enabled, follow these steps to access it within Kanvas:\nRight-click to open the context menu or Click on the settings icon at the top right of the design canvas. Select Options to access the Preferences menu. Enable Whiteboard. A dock will now appear at the bottom of the canvas, providing access to the whiteboard tools. Experiment with drawing, annotation, and collaboration functionalities available in the whiteboard toolbar. Share access with collaborators or team members to engage in real-time collaborative drawing sessions. Customizing Annotations and Shapes\nCustomizing annotations and shapes is crucial in design reviews as it enhances clarity and communication within the team. By tailoring annotations and shapes to fit specific needs, team members can highlight important aspects, provide detailed feedback, and visually differentiate components within the design. This level of customization ensures that feedback is clear, actionable, and directly tied to the relevant parts of the design, making the review process more efficient and effective.\nShape Customization: Shapes within the canvas offer flexibility. Select any shape to access a tooltip with options to resize, reshape, and change colors. This allows for precise adjustments similar to popular design software. Text Annotation Customization: Text annotations come with various options. Customize fonts, sizes, alignments, and styles easily. Text boxes resize for seamless integration with the canvas. Interactive Tooltip Interface: The tooltip is your gateway to customization. It’s simple and intuitive, offering a range of editing options upon selection. It’s designed for easy navigation, mirroring popular design software. Advanced Customization Features:\nManage layers and group elements to organize your design better. These advanced tools ensure collaborative work while preserving design integrity. Lines and edges are easily manipulated. Adjust thickness, style, and endpoints effortlessly. Drag line segments to create bends or curves, adding detail to visual representations. Consistent Functionality: The customization tools apply uniformly to all annotation types. Changes made within the tooltip reflect instantly on the canvas, facilitating real-time collaboration.\nAnnotation Use Cases in Design Reviews\nTeam Collaboration: Multiple users collaborating on infrastructure designs, adding annotations and insights simultaneously. This helps in gathering diverse perspectives and improving the design quality.\nEducational Context: Instructors and students using the tool for visualizing concepts or workflows in remote learning environments. Annotations can be used to highlight key points and provide additional explanations.\nDocumenting Design Decisions: Annotations help in documenting design decisions and ensuring all stakeholders are aligned.\nAnnotations in Kanvas’s Designer Mode are an invaluable tool for enhancing collaboration and streamlining the design review process. By integrating versatile drawing tools, enabling real-time collaboration, and preserving the integrity of the infrastructure, annotations provide a powerful way to communicate, document, and iterate on design ideas. Whether you are working in a team, educating others, or planning complex projects, using annotations effectively can significantly improve the quality and clarity of your designs.\nBest Practices for Effective Design Reviews\nProvide clear and actionable feedback Provide specific feedbackthat the designer can act on. Vague comments can lead to misunderstandings and delays in the design process. Support your feedback with examples or references. This can help clarify your point and provide the designer with tangible suggestions for improvement.\nBalance positive and contructive feedback Frame feedback in a constructive manner, focusing on how things can be improved rather than just pointing out flaws. Acknowledge what works well in the design before delving into areas that need improvement. This helps maintain a positive and collaborative atmosphere.\nRegularly check and respond to comments Stay engaged in the review process. Regularly check and respond to comments to keep the conversation alive and ensure a smooth workflow.\nPrioritize feedback Highlight the most critical feedback first. This ensures that the designer focuses on the most important aspects for improvement.\nIn conclusion, effective design reviews in Kanvas’ Designer Mode are essential for fostering collaboration, ensuring clear communication, and achieving high-quality outcomes. By leveraging features like comments, annotations, and customization, teams can provide actionable feedback, maintain design integrity, and align on project goals. Following best practices for feedback and utilizing the full range of tools Kanvas offers will streamline the review process, leading to more robust and well-documented designs.\n","categories":"","description":"This chapter explores the essential aspects of reviewing designs in Kanvas, covering the lifecycle of design reviews, the use of comments and annotations, and the best practices for providing constructive feedback. It also details how to effectively collaborate with your team to ensure thorough evaluations and high-quality design outcomes.","excerpt":"This chapter explores the essential aspects of reviewing designs in …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-meshery/introduction-to-meshery/reviewing-designs/","tags":"","title":"Reviewing Designs"},{"body":" Access Control 🔗 You will start by denying all traffic.\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: deny-all namespace: default spec: {} And then begin poking holes in your service mesh “firewall”. 🔗 --- apiVersion: \"security.istio.io/v1beta1\" kind: \"AuthorizationPolicy\" metadata: name: \"allow-get\" namespace: default spec: selector: matchLabels: app: httpbin rules: - to: - operation: methods: [\"GET\"] Create AuthorizationPolicy for each BookInfo service. 🔗 --- apiVersion: \"security.istio.io/v1beta1\" kind: \"AuthorizationPolicy\" metadata: name: \"view-productpage\" namespace: default spec: selector: matchLabels: app: productpage rules: - to: - operation: methods: [\"GET\", \"POST\"] # try login with just GET (fails) --- apiVersion: \"security.istio.io/v1beta1\" kind: \"AuthorizationPolicy\" metadata: name: \"view-details\" namespace: default spec: selector: matchLabels: app: details rules: - from: - source: principals: [\"cluster.local/ns/default/sa/bookinfo-productpage\"] to: - operation: methods: [\"GET\", \"POST\"] --- apiVersion: \"security.istio.io/v1beta1\" kind: \"AuthorizationPolicy\" metadata: name: \"view-reviews\" namespace: default spec: selector: matchLabels: app: reviews rules: - from: - source: principals: [\"cluster.local/ns/default/sa/bookinfo-productpage\"] to: - operation: methods: [\"GET\", \"POST\"] --- apiVersion: \"security.istio.io/v1beta1\" kind: \"AuthorizationPolicy\" metadata: name: \"view-ratings\" namespace: default spec: selector: matchLabels: app: ratings rules: - from: - source: principals: [\"cluster.local/ns/default/sa/bookinfo-reviews\"] to: - operation: methods: [\"GET\", \"POST\"] Allow per user access 🔗 --- apiVersion: \"security.istio.io/v1beta1\" kind: \"AuthorizationPolicy\" metadata: name: \"view-reviews\" namespace: default spec: selector: matchLabels: app: reviews rules: - from: - source: principals: [\"cluster.local/ns/default/sa/bookinfo-productpage\"] to: - operation: methods: [\"GET\"] when: - key: request.headers[end-user] values: [\"naruto\"] Reset BookInfo Subsets (reset destination rules) 🔗 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage subsets: - name: v1 labels: version: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: ratings spec: host: ratings subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v2-mysql labels: version: v2-mysql - name: v2-mysql-vm labels: version: v2-mysql-vm --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: details spec: host: details subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 --- Identity Verification 🔗 Note: this lab uses the sample application HTTPbin.\nUsing Meshery, deploy the HTTPbin sample application.\nAdd Claims 🔗 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: require-jwt namespace: foo spec: selector: matchLabels: app: httpbin action: ALLOW rules: - from: - source: requestPrincipals: [\"testing@secure.istio.io/testing@secure.istio.io\"] when: - key: request.auth.claims[groups] values: [\"group1\"] Def 🔗 apiVersion: \"security.istio.io/v1beta1\" kind: \"RequestAuthentication\" metadata: name: \"jwt\" namespace: default spec: selector: matchLabels: app: httpbin jwtRules: - issuer: \"testing@secure.istio.io\" jwksUri: \"https://raw.githubusercontent.com/istio/istio/release-1.7/security/tools/jwt/samples/jwks.json\" Mutual TLS 🔗 Using Meshery, you can change mTLS enforcement for a namespace.\nTo configure mTLS on more selective level, you can change and apply this configuration:\napiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"istio-system\" spec: # selector: # matchLabels: # app: httpbin mtls: mode: STRICT #ISTIO_MUTUAL,DISABLE # portLevelMtls: # 80: # mode: DISABLE ","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/advance-concepts-of-service-meshes/service-security-capabilities/","tags":"","title":"Service Security Capabilities of Istio"},{"body":"This section is a refresher that provides an overview of the primary Kubernetes resources related to storage. At the end of this section, please complete the exercises to put these concepts into practice.\nVolume 🔗 Volume is a property that can be defined in a Pod’s specification (at .spec.volumes) and mounted in a container’s filesystem (.spec.containers.volumeMounts). It allows decoupling a container from the storage solution and containers of the same pod to share data.\nAmong the available types of volumes:\nemptyDir configMap Secret hostPath downwardAPI The following specification defines a MongoDB Pod with an emptyDir volume mounted at /data/db in the container filesystem. It allows the data to persist outside the container’s filesystem but still on the host filesystem.\napiVersion: v1 kind: Pod metadata: name: mongo spec: containers: - name: mongo image: mongo:7.0 volumeMounts: - name: data mountPath: /data/db volumes: - name: data emptyDir: {} Note: We should not use a volume for persistent storage; instead we should use PersistentVolume and PersistentVolumeClaim.\nPersistentVolume 🔗 A PersistentVolume (PV) is a resource used to provide storage, either statically or dynamically. It decouples an application from the storage solution.\nVarious types of PersistentVolume are available, including:\ncephfs csi fc hostPath iscsi local nfs rbd The main properties of a PV are the following:\nvolumeMode 🔗 Defines how the volume is presented to a Pod:\nFilesystem (default) Block accessModes 🔗 Specifies how we can mount the volume in the Pod:\nReadWriteOnce (RWO): the volume can be mounted as read-write by a single Node ReadOnlyMany (ROX): the volume can be mounted as read-only by multiple Nodes at the same time ReadWriteMany (RWX): the volume can be mounted as read-write by multiple Nodes at the same time ReadWriteOncePod (RWOP): the volume can be mounted as read-write by a single Pod only persistentVolumeReclaimPolicy 🔗 Defines what happens to the PersistentVolume after the associated PersistentVolumeClaim is deleted:\nRetain (default if manually created): the PersistentVolume is not deleted, a manual data recovery is possible Delete (default if dynamically created): the PersistentVolume and the underlying storage are deleted The specification below defines a PersistentVolume that will be tied to a single Node and offer 1G of storage from this Node’s filesystem.\napiVersion: v1 kind: PersistentVolume metadata: name: pv spec: accessModes: - ReadWriteOnce capacity: storage: 1Gi hostPath: path: /tmp/data PersistentVolumeClaim 🔗 A PersistentVolumeClaim (PVC) is a storage request. It specifies the storage requirements regarding size and access mode, and is bound to a PersistentVolume that meets those requirements.\nThe following specification defines a PersistentVolumeClaim, which requests 500 MB of storage with a RWO mode.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Mi A Pod can request storage by referencing the PVC in its specification as follows. When a PVC is bound to a PV, the PV is mounted into the container’s filesystem.\napiVersion: v1 kind: Pod metadata: name: db spec: containers: - image: mongo:5.0 name: mongo volumeMounts: - name: data mountPath: /data/db volumes: - name: data persistentVolumeClaim: claimName: pvc StorageClass 🔗 A StorageClass defines how to dynamically create PersistentVolumes without needing to pre-provision them.\nMore information in the official documentation at https://kubernetes.io/docs/concepts/storage/\n","categories":"","description":"Understand StorageClass, PV and PVCs stateful applications.","excerpt":"Understand StorageClass, PV and PVCs stateful applications.","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/storage/","tags":"","title":"Storage"},{"body":"In this chapter, we will explore how Dapr utilizes its API constructs to facilitate communication and manage the state of application data within this architecture by observing the container logs.\nPython Application Logs 🔗 The Python application code above generates messages that contain data with an orderId that increments once per second.\nHere’s a snippet from the Python script in app.js:\ndapr_port = os.getenv(\"DAPR_HTTP_PORT\", 3500) dapr_url = \"http://localhost:{}/neworder\".format(dapr_port) n = 0 while True: n += 1 message = {\"data\": {\"orderId\": n}} try: response = requests.post(dapr_url, json=message) except Exception as e: print(e) time.sleep(1) The Dapr sidecar for the Python application sends a POST request to the Dapr sidecar of the Node.js application using the Dapr service invocation API. Here’s a breakdown of what happens:\nThe Python app sends a POST request to its sidecar at http://localhost:3500/v1.0/invoke/nodeapp/method/neworder. Note: http://localhost:3500 is the default listening port for Dapr. The sidecar for the Python app invokes the nodeapp service through its own Dapr sidecar. The Python app does not need to know the exact address or port of the Node.js service, it simply makes a request to its sidecar, which handles the routing.\nSteps to Stream Python Application Logs:\nClick on the python-app pod. On the right sidebar, click on Action. Click on Stream Container logs. The logs show the daprd container logs with a POST request made to /neworder endpoint.\nNode.js Application Logs 🔗 Follow the same steps above to get the logs for the Node.js application. The logs show API calls made to the state store for persisting order data.\nHere’s what can be observed from the logs:\nThe Dapr sidecar of the Node.js application listens on port 3500 for incoming HTTP requests. The sidecar receives the invocation request sent from the sidecar of the Python app and routes it to the /neworder endpoint in the Node.js application. The sidecar makes a POST request to the state store endpoint (/v1.0/state/statestore) to persist the state information in Redis. This endpoint is part of the Dapr state management API and is mapped to the configured state store component. By analyzing these logs, we gained a deeper understanding of how Dapr’s APIs such as the state management API and service invocation API work together to enable service-service communication and efficient management of application state.\n","categories":"","description":"Use Meshery's interactive terminal to view logs of applications","excerpt":"Use Meshery's interactive terminal to view logs of applications","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/explore-dapr-with-meshery/dapr/view-application-logs/","tags":"","title":"View Application Logs"},{"body":" Visualizing the Deployed Resources 🔗 To view the resources deployed, use the Visualize section of the Kanvas. In this section, you can apply filters to display the specific resources you want to see.\nMove to the Visualize tab.\nClick the filter icon and choose the appropriate filters:\nFor “Kinds” select Deployment, Service, PersistentVolume, PersistentVolumeClaim and Secret\nFor the “label” select dev=tutorial\nYou can also use the icons on the right sidebar to filter the resources.\nAfter selecting your filters, you should see a view displaying only your relevant resources, as shown in the image below.\nFigure: Filter resources\nResource Details 🔗 You can click on any resource to view more details about it in the “Details” section on the right sidebar.\nDeployment\nFigure: Get more details about deployment\nService\nThe Service details only display the cluster IP for now. So there is no way to access the application externally.\nFigure: Get more details about service\nDeleting Resources 🔗 To delete the resources, use the Undeploy option from the Action dropdown in the Design tab.\n","categories":"","description":"In this section you will use the Visualize tool to see the resources in the cluster","excerpt":"In this section you will use the Visualize tool to see the resources …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/deploying-wordpress-and-mysql-with-persistent-volumes-with-meshery/sql/visualize/","tags":"","title":"Visualize Deployed Resources"},{"body":"","categories":"","description":"Discover the benefits of using Exoscale's managed Kubernetes service (SKS) for your container orchestration needs. Learn how SKS simplifies deployment, scaling, and management of Kubernetes clusters while providing robust security and performance features.","excerpt":"Discover the benefits of using Exoscale's managed Kubernetes service …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/why-exoscale-sks/","tags":"","title":"Why Exoscale SKS?"},{"body":" Build \u0026 Run 🔗 docker build In our example, the first step is to build the hello-world container with the \u003e docker build command.\ndocker run In our example, the second step is to run the hello-world container with the \u003e docker run command.\nBuild \u0026 Run 🔗 Video: Build \u0026 Run Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Build \u0026 Run 🔗 docker build In our example, the first step is to build …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/containers/content/build-and-run/","tags":"","title":"Build \u0026 Run"},{"body":"Congratulations! You’ve successfully completed the tutorial on deploying a WordPress site and MySQL database with Persistent Volumes using Meshery Playground. This hands-on experience should have given you valuable insights into importing manifest files, visualizing resources, creating persistent volumes, and managing deployments in Meshery Playground.\n","categories":"","description":"","excerpt":"Congratulations! You’ve successfully completed the tutorial on …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/deploying-wordpress-and-mysql-with-persistent-volumes-with-meshery/sql/conclusion/","tags":"","title":"Conclusion"},{"body":"Congratulations! You have reached the end of this course on understanding how Dapr works in a Kubernetes cluster, using Meshery as a visual guide. Throughout this course, you learned how to import Dapr, Redis, and application configuration files into Meshery and visualize the resources and their relationships.\nYou successfully created and configured a Dapr state store component using Meshery. You deployed the designs and visualized the deployed resources in the cluster using Operator mode. Additionally, you viewed the Dapr sidecars and observed the resources deployed in the dapr-system and default namespaces.\nFinally, you streamed the logs and observed the API calls, gaining a deeper understanding of how Dapr’s APIs, such as the state management API and service invocation API, work together to facilitate seamless inter-service communication and efficient management of application state.\nWell done on completing this course and enhancing your knowledge of Dapr and Meshery!\n","categories":"","description":"","excerpt":"Congratulations! You have reached the end of this course on …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/explore-dapr-with-meshery/dapr/conclusion/","tags":"","title":"Conclusion"},{"body":" CRONJOBs 🔗 Use familiar cron syntax to schedule tasks. CronJobs are part of the Batch API for creating short-lived non-server tools. CronJobs provide a method for scheduling the execution of Pods. They are excellent for running periodic tasks like backups, reports, and automated tests.\n","categories":"","description":"","excerpt":" CRONJOBs 🔗 Use familiar cron syntax to schedule tasks. CronJobs are …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-kubernetes/kubernetes-building-blocks/content/cronjob/","tags":"","title":"CRONJOBs"},{"body":" Sustainability \u0026 Reliability 🔗 At Exoscale, we understand that the physical infrastructure of our data centers plays a critical role in the sustainability of our cloud computing services. Therefore, we carefully select our data centers based on several factors, including energy efficiency. In addition, we work with only the most environmentally responsible and forward-thinking providers to ensure our operations are as sustainable. Our stringent energy efficiency requirements include renewable energy sources, highly efficient cooling systems, and best-in-class power usage effectiveness (PUE) ratings.\nBy choosing Exoscale, you can be confident that your cloud computing needs are being met in a highly sustainable and reliable way.\n","categories":"","description":"","excerpt":" Sustainability \u0026 Reliability 🔗 At Exoscale, we understand that the …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/sustainable-cloud/content/data-center/","tags":"","title":"Data Center"},{"body":" Overview 🔗 Remote-First Approach Waste Heat Utilization Personal Commitment Leading Sustainability Remote-First Approach 🔗 Exoscale is a broadly distributed company. We believe in a remote-first approach to work, which helps reduce the impact of maintaining a traditional office space, such as energy consumption, waste production, and commuting emissions.\nWaste Heat Utilization 🔗 Always looking for new and innovative ways to minimize our environmental impact, we have started using the waste heat generated by our lab infrastructure to heat our Lausanne office space in the winter months.\nPersonal Commitment 🔗 Our employees are eager to reduce their environmental impact. For example, many of our employees have been increasingly relying on trains to travel to our offsite meetings, which helps reduce air travel emissions. In addition, many employees use public transportation, biking, and electric vehicles to commute to work.\nLeading Sustainability 🔗 This combination of remote work and environmental awareness is just one of the many ways Exoscale leads the way in sustainable business practices.\nOur commitment to sustainability is a core part of our company culture. It drives everything we do, from our technology to our work.\n","categories":"","description":"","excerpt":" Overview 🔗 Remote-First Approach Waste Heat Utilization Personal …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/sustainable-cloud/content/believes/","tags":"","title":"Exoscale's Believes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/contractual-setup/","tags":"","title":"Exoscale's Contractual Setup"},{"body":" Manifest Tricks 🔗 Building a manifest from scratch can be tedious. However, you can use many cool tricks to develop your manifests, like automatically creating a manifest with the kubectl command and the –dry-run option or using a pre-build manifest from the Kubernetes documentation to start with.\nManifest Tricks 🔗 Video: Manifest Tricks Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Manifest Tricks 🔗 Building a manifest from scratch can be tedious. …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/manifest-tricks/","tags":"","title":"Manifest Tricks"},{"body":" Details 🔗 Scale up your applications Access a curated collection of solution templates Leverage ready-to-use managed services Web 🔗 The compplete marketplace portfolio with description can be found here: exoscale.com/marketplace\nPortal 🔗 The tighly integrated marketplace products are easy to reach in the product portal: portal.exoscale.com/marketplace\nNOTE! You need to be logged in to your portal account!\n","categories":"","description":"","excerpt":" Details 🔗 Scale up your applications Access a curated collection of …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/products/content/marketplace/","tags":"","title":"Marketplace"},{"body":" Istio provides transparent mutual TLS to services inside the service mesh where both the client and the server authenticate each others’ certificates as part of the TLS handshake. As part of this course, we have deployed Istio with mTLS.\nBy default istio sets mTLS in PERMISSIVE mode which allows plain text traffic to be sent and accepted by a mesh. We first disallow plain text traffic using PeerAuthentication and setting mTLS mode to STRICT.\nConfirm mTLS is being enforced 🔗 This can be easily done by executing a simple command:-\nkubectl get peerauthentication --all-namespaces Verify mTLS 🔗 Citadel is Istio’s key management service. As a first step, confirm that Citadel is up and running:\nkubectl get deploy -l istio=citadel -n istio-system Output will be similar to:\nNAME READY UP-TO-DATE AVAILABLE AGE istio-citadel 1/1 1 1 3m23s To experiment with mTLS, let’s do so by logging into the sidecar proxy of the productpage pod by executing this command:\nkubectl exec -it $(kubectl get pod | grep productpage | awk '{ print $1 }') -c istio-proxy -- /bin/bash We are now in the proxy of the productpage pod. Check that all the ceritificates are loaded in this proxy:\nls /etc/certs/ You should see 3 entries:\ncert-chain.pem key.pem root-cert.pem Now, try to make a curl call to the details service over HTTP:\ncurl http://details:9080/details/0 Since, we have TLS between the sidecar’s, an HTTP call will not work. The request will timeout. You will see an error like the one below:\ncurl: (7) Failed to connect to details port 9080: Connection timed out Let us try to make a curl call to the details service over HTTPS but WITHOUT certs:\ncurl https://details:9080/details/0 -k The request will be denied and you will see an error like the one below:\ncurl: (16) SSL_write() returned SYSCALL, errno = 104 Now, let us use curl over HTTPS with certificates to the details service:\ncurl https://details:9080/details/0 -v --key /etc/certs/key.pem --cert /etc/certs/cert-chain.pem --cacert /etc/certs/root-cert.pem -k Output will be similar to this:\n* Trying 10.107.35.26... * Connected to details (10.107.35.26) port 9080 (#0) * found 1 certificates in /etc/certs/root-cert.pem * found 0 certificates in /etc/ssl/certs * ALPN, offering http/1.1 * SSL connection using TLS1.2 / ECDHE_RSA_AES_128_GCM_SHA256 * server certificate verification SKIPPED * server certificate status verification SKIPPED * error fetching CN from cert:The requested data were not available. * common name: (does not match 'details') * server certificate expiration date OK * server certificate activation date OK * certificate public key: RSA * certificate version: #3 * subject: O=#1300 * start date: Thu, 26 Oct 2018 14:36:56 GMT * expire date: Wed, 05 Jan 2019 14:36:56 GMT * issuer: O=k8s.cluster.local * compression: NULL * ALPN, server accepted to use http/1.1 \u003e GET /details/0 HTTP/1.1 \u003e Host: details:9080 \u003e User-Agent: curl/7.47.0 \u003e Accept: */* \u003e \u003c HTTP/1.1 200 OK \u003c content-type: application/json \u003c server: envoy \u003c date: Thu, 07 Jun 2018 15:19:46 GMT \u003c content-length: 178 \u003c x-envoy-upstream-service-time: 1 \u003c x-envoy-decorator-operation: default-route \u003c * Connection #0 to host details left intact {\"id\":0,\"author\":\"William Shakespeare\",\"year\":1595,\"type\":\"paperback\",\"pages\":200,\"publisher\":\"PublisherA\",\"language\":\"English\",\"ISBN-10\":\"1234567890\",\"ISBN-13\":\"123-1234567890\"} This proves the existence of mTLS between the services on the Istio mesh.\nNow lets come out of the container before we go to the next section:\nexit Secure Production Identity Framework for Everyone (SPIFFE) 🔗 Istio uses [SPIFFE](https://spiffe.io/) to assert the identify of workloads on the cluster. SPIFFE consists of a notion of identity and a method of proving it. A SPIFFE identity consists of an authority part and a path. The meaning of the path in spiffe land is implementation defined. In k8s it takes the form `/ns/$namespace/sa/$service-account` with the expected meaning. A SPIFFE identify is embedded in a document. This document in principle can take many forms but currently the only defined format is x509. To start our investigation, let us check if the certs are in place in the productpage sidecar:\nkubectl exec $(kubectl get pod -l app=productpage -o jsonpath={.items..metadata.name}) -c istio-proxy -- ls /etc/certs Output will be similar to:\ncert-chain.pem key.pem root-cert.pem Mac users, MacOS should have openssl available. If your machine does not have openssl install, install it using your preferred method.\nHere is one way to install it on RHEL or CentOS or its derivatives:\nsudo yum install -y openssl-devel Here is one way to install it on Ubuntu or Debian or its derivatives:\nsudo apt install -y libssl-dev Now that we have found the certs, let us verify the certificate of productpage sidecar by running this command:\nkubectl exec $(kubectl get pod -l app=productpage -o jsonpath={.items..metadata.name}) -c istio-proxy -- cat /etc/certs/cert-chain.pem | openssl x509 -text -noout | grep Validity -A 2 Output will be similar to:\nNot Before: Sep 23 17:32:28 2019 GMT Not After : Dec 22 17:32:28 2019 GMT Lets also verify the URI SAN:\nkubectl exec $(kubectl get pod -l app=productpage -o jsonpath={.items..metadata.name}) -c istio-proxy -- cat /etc/certs/cert-chain.pem | openssl x509 -text -noout | grep 'Subject Alternative Name' -A 1 Output will be similar to:\nX509v3 Subject Alternative Name: critical URI:spiffe://cluster.local/ns/default/sa/bookinfo-productpage ","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/introduction-to-service-meshes/mutual-tls/","tags":"","title":"Mutual TLS \u0026 Identity Verification"},{"body":"","categories":"","description":"A comparison of object and block storage, highlighting their differences in structure, performance, and use cases.","excerpt":"A comparison of object and block storage, highlighting their …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-storage/object-vs-block/","tags":"","title":"Object vs Block"},{"body":" Overview 🔗 CERN - powered by Exoscale 🔗 scientific computing at scale\nHelix Nebula Science Cloud (HNSciCloud) initiative Exoscale provided 10,000 CPU cores An environment for 1.7 million scientists Storing, sharing, analyzing, and reusing research data whalebone - powered by Exoscale 🔗 keep the malware out\nMalware Defense Elasticsearch Cluster \u0026 Log Processing Backups and Data Storage Servers are dynamically started on demand SchoolFox - powered by Exoscale 🔗 safe and compliant school communication\ncommunication between teachers and parents Re-platformed to Exoscale GDPR was a critical factor in the move PaaS to IaaS move ","categories":"","description":"","excerpt":" Overview 🔗 CERN - powered by Exoscale 🔗 scientific computing at scale …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/exoscale/content/references/","tags":"","title":"References"},{"body":"This section is a refresher that provides an overview of the main concepts of security in Kubernetes. At the end of this section, please complete the exercises to put these concepts into practice.\nAuthentication \u0026 Authorization 🔗 Authentication: several methods 🔗 Kubernetes offers multiple methods to authenticate users against the API Server:\nClient certificates Bearer tokens HTTP basic auth OpenID Connect Proxy To authenticate an application running in a Pod, Kubernetes relies on ServiceAccounts resources.\nAuthentication: admin kubeconfig 🔗 When we create a cluster, an admin kubeconfig file is generated, similar to the following one.\napiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CR…0tLS0tCg== server: https://10.55.133.216:6443 name: kubernetes contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: LS0tLS1CRU...0tLS0tCg== client-key-data: LS0tLS1CRU...0tLS0tCg== This file contains a public/private key pair used for authentication against the API Server. We can use OpenSSL commands to get details about the public key (x509 certificate).\nThe following screenshot shows the Subject used in the certificate:\n“O = system:master” indicates this certificate is related to the system:master group “CN= kubernetes-admin” indicates it is related to the kubernetes-admin user Using this certificate to communicate with the API Server will authenticate us as the kubernetes-admin belonging to the system:master group. This is a specific case, as the group system:master provides full access to the cluster.\nThe admin kubeconfig file is not the only kubeconfig file generated during the cluster creation step. As we’ll see in the next section, each component that needs to communicate with the API Server has its kubeconfig file (and associated access rights).\nAuthentication: control-plane components 🔗 The following picture illustrates how the control plane components communicate with each other.\nThe /etc/kubernetes folder contains the following files to ensure this communication is secured.\nkubeconfig files to authenticate internal components against the API Server Certificates and private keys to ensure communication is using TLS; they are located in /etc/kubernetes/pki $ sudo tree /etc/kubernetes /etc/kubernetes ├── admin.conf ├── controller-manager.conf ├── kubelet.conf ├── manifests │ ├── etcd.yaml │ ├── kube-apiserver.yaml │ ├── kube-controller-manager.yaml │ └── kube-scheduler.yaml ├── pki │ ├── apiserver-etcd-client.crt │ ├── apiserver-etcd-client.key │ ├── apiserver-kubelet-client.crt │ ├── apiserver-kubelet-client.key │ ├── apiserver.crt │ ├── apiserver.key │ ├── ca.crt │ ├── ca.key │ ├── etcd │ │ ├── ca.crt │ │ ├── ca.key │ │ ├── healthcheck-client.crt │ │ ├── healthcheck-client.key │ │ ├── peer.crt │ │ ├── peer.key │ │ ├── server.crt │ │ └── server.key │ ├── front-proxy-ca.crt │ ├── front-proxy-ca.key │ ├── front-proxy-client.crt │ ├── front-proxy-client.key │ ├── sa.key │ └── sa.pub ├── scheduler.conf └── super-admin.conf For information purposes, the following table gives the subject of the certificates embedded in each kubeconfig file.\nfile subject admin.conf O = system:masters, CN=kubernetes-admin super-admin.conf O = system:masters, CN = kubernetes-super-admin controller-manager.conf CN = system:kube-controller-manager kubelet.conf O = system:nodes, CN = system:node:NODE_NAME scheduler.conf CN = system:kube-scheduler Authentication: ServiceAccount 🔗 When a Pod needs to access the API Server, it must use a resource of type ServiceAccount. The following YAML specification defines a ServiceAccount named viewer.\napiVersion: v1 kind: ServiceAccount metadata: name: viewer We can manually create a token for this ServiceAccount.\nkubectl create token viewer This command returns a token similar to the following one:\neyJhbGciOiJSUzI1NiIsImtpZCI6IlRwSU85ZXdWUFp0SlpjaDBjekl6ZTNaNGRuUTZSVDFiV2dyWVhqbGwyRDAifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzQ1NDk5OTUyLCJpYXQiOjE3NDU0OTYzNTIsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiMTE1OTgzZjYtOWE3Ny00ZmY1LWE4OGQtMTc2ODg3N2YxYmE3Iiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJkZWZhdWx0Iiwic2VydmljZWFjY291bnQiOnsibmFtZSI6InZpZXdlciIsInVpZCI6IjY2NmE3NWNkLWRkZGUtNDAzYi1iZmE0LWM0MjIxNWI1OTA1YiJ9fSwibmJmIjoxNzQ1NDk2MzUyLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDp2aWV3ZXIifQ.CGYbqWDj3KaEGPgU_pV6sL1wRf3IU56AlpljLxUO6tvpbkK7Z6le8FI5zdwp_04LgcWnHLo5-hsZiyJxmeKYXhsb3CASkI0Vvumfsb8kahIiJxVXIE-PfzKNlxampuubc3mG4q9h1s0M_Y-PubMdl4TkBoLMjujxbsTtPqpD2joxyZ2YB7ys7DiGp-BjQwXwwaxOniSwd0l_tyEAlX0UTy0qjmjjuMBJKQTLDzwPJXWCAXbeAMULsnsosS21sWyimmVMz6HQ8S4MttkMSg8eZ1IW-LPPn3Hfs0lBLRYeVRBn6qe4l7qxgCfgj57GfYgEWGy5BO9uaAAGcHVBdTacAQ From jwt.io we can get the content of this JWT token and see that it authenticates the ServiceAccount named viewer within the default Namespace. We could use this token manually to call the API Server, but when a Pod is using a ServiceAccount a dedicated token is automatically created and mounted into its containers’ filesystem.\nAuthorization: RBAC (Role Based Access Control) 🔗 In the previous section, we covered the authentication mechanisms that allow the API Server to verify a user’s or an application’s identity. Now, we’ll look at the resources used to grant permissions.\nAuthorization: Role / RoleBinding 🔗 A Role resource defines permissions in a Namespace. A RoleBinding associates this Role with an identity which can be a user, a group, or a ServiceAccount.\nThe following example defines a Role that grants read-only access to Pods in the development Namespace. The RoleBinding associates this Role with a user named bob: if a user has a certificate whose subject is bob, then he will be granted the permission to list and get information about Pods in the development Namespace.\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: development name: dev-pod-reader rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: dev-pod-reader namespace: development subjects: - kind: User name: bob apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: dev-pod-reader apiGroup: rbac.authorization.k8s.io Authorization: ClusterRole / ClusterRoleBinding 🔗 The ClusterRole and ClusterRoleBinding resources are similar to the Role and RoleBinding ones, except that they are global to the cluster instead of being limited to a Namespace.\nThe following specifications define a ClusterRole which grants read access to resources of type Secret in the entire cluster and a ClusterRoleBinding that associates this ClusterRole to the ServiceAccount named viewer. If a Pod uses the viewer ServiceAccount, then its containers will have the right to read the cluster’s Secrets.\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: secret-reader rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: dev-pod-reader subjects: - kind: ServiceAccount name: viewer apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io Authorization: usage of a ServiceAccount 🔗 To use a ServiceAccount, we can define the serviceAccountName property in the Pod’s specification as follows.\napiVersion: v1 kind: Pod metadata: name: monitoring spec: containers: - image: lucj/mon:1.2 name: mon serviceAccountName: viewer SecurityContext 🔗 The SecurityContext property defines privileges and access controls at the Pod or container level.\nWhen defined at the Pod level, we can use the following properties:\nfsGroup: specifies a group ID that all containers in the Pod use for accessing mounted volumes runAsGroup: sets the primary group ID for all container processes runAsNonRoot: ensures that containers must run as a non-root user runAsUser: sets the user ID to run all processes in the Pod seLinuxOptions: defines SELinux labels to apply to the Pod supplementalGroups: specifies additional group IDs for processes in the Pod sysctls: allows setting kernel parameters (sysctls) for the Pod When defined at the container level, we can use the following properties:\nallowPrivilegeEscalation: indicates whether a process can gain more privileges than its parent capabilities: adds or removes Linux capabilities for the container process privileged: grants the container full access to the host procMount: controls how /proc is mounted in the container readOnlyRootFilesystem: if set to true, makes the container’s root filesystem read-only runAsGroup: sets the primary group ID for the container process runAsNonRoot: ensures the container runs as a non-root user runAsUser: sets the user ID to run the container process seLinuxOptions: applies SELinux labels to the container Example 🔗 The following Pod specification defines a securityContext property both at the Pod and at the container level.\napiVersion: v1 kind: Pod metadata: name: demo spec: securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 containers: - name: api image: registry.gitlab.com/web-hook/api:v1.0.39 securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 10000 runAsNonRoot: true seccompProfile: type: RuntimeDefault capabilities: drop: - ALL Network Policy 🔗 By default, Pods created in separate Namespaces can communicate with each other. To control Pod-to-Pod communications, Kubernetes has a NetworkPolicy resource. Based on Pod’s labels, it can restrict ingress and egress communication for the selected Pods.\nThe example below defines a NetworkPolicy restricting the database Pod to receive traffic from the backend Pod only.\nThe example below (from Kubernetes documentation) is more complex. It illustrates the full capabilities of NetworkPolicies.\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 It defines a NetworkPolicy for Pods with the label role: db managing incoming and outgoing traffic for those Pods.\nIt authorizes incoming traffic from (logical OR):\nPods with IP addresses in a specific range Pods within a namespace Pods with specific labels in the current namespace It also authorizes outgoing traffic to (logical OR):\nPods with IP addresses in a specific range Pods within a specific namespace Pods with specific labels in the current namespace Both incoming and outgoing traffic are limited to specific ports.\n","categories":"","description":"Create Network Policies and RBAC rules.","excerpt":"Create Network Policies and RBAC rules.","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/security/","tags":"","title":"Security"},{"body":" Selection of Databases 🔗 The prolific database situation makes the pick of a particular technology not easy. Many theorems, models, and concepts support that selection process, meaning no universal solution for database choices is available.\nThe CAP theorem gives guidance and is still relevant when designing distributed applications and choosing a data store for such scenarios. Still, you have to decide if availability is the preference or consistency gets the vote.\nDue to the broad spectrum of database types, a different approach for selecting the right technology for the data store is to choose based on the kind of data that needs storage. Let’s have a look at some examples:\nManaged PostgreSQL 🔗 PostgreSQL is a popular open source relational database known for its variety of features. It supports both SQL and JSON querying. The database offers a high level of integrity, correctness, and reliability. Rich features like MVCC, point in time recovery and asynchronous replication are part of the PostgreSQL database. You find more details about this database here.\nManaged MySQL 🔗 MySQL is the most widely used open source, object-relational database. It serves as a primary database for many known applications and is well known for its reliability and stability. MySQL has a very active developer community that continuously expand the MySQL functionalities. You find more details about this database here.\nManaged Apache Kafka 🔗 Apache Kafka is a distributed, open source data source optimized for real-time processing of streaming data. The database enables low latency due to decoupled data streams, which makes it extremely high performing. Apache Kafka is highly scalable thanks to its distributed nature and makes it easily scalable. You find more details about this database here.\nManaged Redis™ 🔗 Redis™ is an open source, key-value data store used as database, cache or message broker. The in-memory dataset allows for top performance, making it a good choice for caching, session management or real-time analytics. Redis™ supports atomic operations, rich data types, and Lua scripting. You find more details about this database here.\nManaged OpenSearch 🔗 OpenSearch is a community-driven, open source search and analytics suite derived from Apache 2.0 licensed Elasticsearch 7.10.2 \u0026 Kibana 7.10.2. It consists of a search engine daemon, OpenSearch, and a visualization and user interface, OpenSearch Dashboards. OpenSearch enables people to easily ingest, secure, search, aggregate, view, and analyze data. You find more details about this database here.\nManaged OpenSearch 🔗 For example, are you dealing with structured or unstructured data, or do you have a mixed data environment?\nIs one database technology sufficient to provide the functionality, or is it a blend of database technologies that deliver the solution?\nGuidance is challenging to find, especially if you want to see the complete picture of available technology offers and not choosing a vendor first and then picking what is available in the portfolio.\nFortunately, a book supports in a very structured way with a map of all the relevant topics this vital selection process—this desired holistic approach to data store selection for data-intensive applications is a source of wisdom.\n","categories":"","description":"","excerpt":" Selection of Databases 🔗 The prolific database situation makes the …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-dbaas/databases/content/selections/","tags":"","title":"Selection of Databases"},{"body":"","categories":"","description":"Network exposure and load balancing","excerpt":"Network exposure and load balancing","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/service/","tags":"","title":"Service"},{"body":" Simple Examples 🔗 Let’s look at two examples. First, a simple hello-world container and second, a simple Ubuntu container.\nsimple hello-world container simple ubuntu container Watch these two examples below in the videos.\nSimple Hello-World Container 🔗 Video: Simple Hello-World Container Your browser does not support the video tag. Simple Ubuntu Container 🔗 Video: Simple Ubuntu Container Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Simple Examples 🔗 Let’s look at two examples. First, a simple …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/kubernetes/content/simple-examples/","tags":"","title":"Simple Examples"},{"body":" In this lab, you will use the sample application Image Hub. This version of the Image Hub filter has been simplified for your lab. To self-study deeper functionality, try the other version of the Image Hub filter that is available in the Image Hub repo.\nDeploy Sample Application 🔗 Using Meshery, select Istio from the Management menu.\nIn the Istio management page:\nType default into the namespace field. Click the (+) icon on the Manage Sample Application Lifecycle card and select Image Hub Application to install the latest version of Image Hub Load the filter 🔗 Next, load the custom Envoy filter. This filter is written in Rust and is compiled against WebAssembly as it’s target runtime.\nUsing Meshery, select Istio from the Management menu.\nIn the Istio management page:\nType default into the namespace field. Click the (+) icon on the Apply Service Mesh Configuration card and select Envoy Filter for Image Hub to deploy the custom filter. ## Send traffic ## Analyze behavior Alternative, manual installation steps are provided for reference below. No need to execute these if you have performed the steps above.\nManually deploy the Image Hub filter.\napiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: custom-filter spec: configPatches: - applyTo: HTTP_FILTER match: context: SIDECAR_OUTBOUND # will match outbound listeners in all sidecars listener: portNumber: 9080 filterChain: name: envoy.http_connection_manager filter: name: \"envoy.tcp_proxy\" patch: operation: INSERT_BEFORE value: # This is the full filter config including the name and config or typed_config section. name: \"envoy.filters.http.wasm\" config: config: name: custom-filter rootId: my_root_id vmConfig: code: local: filename: /var/lib/imagehub/filter.wasm runtime: envoy.wasm.runtime.v8 vmId: custom-filter allow_precompiled: true workloadSelector: labels: app: api-v1 version: v1 Manually patch the Image Hub Deployment.\n---( // kubectl patch deployment/api-v1 -p ' { \"spec\": { \"template\": { \"metadata\": { \"annotations\": { \"sidecar.istio.io/userVolumeMount\": \"[{\\\"mountPath\\\":\\\"/var/lib/imagehub\\\",\\\"name\\\":\\\"wasm-filter\\\"}]\" } }, \"spec\": { \"initContainers\": [ { \"command\": [ \"curl\", \"-L\", \"-o\", \"/var/lib/imagehub/filter.wasm\", \"https://github.com/layer5io/advanced-istio-service-mesh-workshop/raw/master/lab-7/ratelimiter/ratelimit-filter.wasm\" ], \"image\": \"curlimages/curl\", \"imagePullPolicy\": \"Always\", \"name\": \"add-wasm\", \"resources\": {}, \"terminationMessagePath\": \"/dev/termination-log\", \"terminationMessagePolicy\": \"File\", \"volumeMounts\": [ { \"mountPath\": \"/var/lib/imagehub\", \"name\": \"wasm-filter\" } ] } ], \"volumes\": [ { \"emptyDir\": {}, \"name\": \"wasm-filter\" } ] } } } } )// ' A future version of Meshery will allow you to deploy any filter from the wasm-filters repo. PR the repo to upload your custom filter and have Meshery deploy it.\n","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/advance-concepts-of-service-meshes/webassembly-and-intelligent-data-planes/","tags":"","title":"WebAssembly and Intelligent Data Planes"},{"body":" Basic Concepts 🔗 The four foundational Kubernetes concepts listed below are essential to run your modern applications on scalable cloud infrastructure.\nPODs DEPLOYMENTs STATEFULSETs SERVICEs These four concepts are explained in more detail in the videos below.\nPODs 🔗 Video: PODs Your browser does not support the video tag. DEPLOYMENTs 🔗 Video: DEPLOYMENTs Your browser does not support the video tag. STATEFULSETs 🔗 Video: STATEFULSETs Your browser does not support the video tag. SERVICEs 🔗 Video: SERVICEs Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Basic Concepts 🔗 The four foundational Kubernetes concepts listed …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/kubernetes/content/basic-comments/","tags":"","title":"Basic Concepts"},{"body":"","categories":"","description":"This learning path prepares you for the Certified Kubernetes Administrator (CKA) exam, covering essential topics such as cluster architecture, installation, configuration, and troubleshooting.","excerpt":"This learning path prepares you for the Certified Kubernetes …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/","tags":"","title":"Certified Kubernetes Administrator (CKA) Preparation"},{"body":" In this chapter we will configure circuit breaking using Istio. Circuit breaking allows developers to write applications that limit the impact of failures, latency spikes, and other undesirable effects of network peculiarities. This task will show how to configure circuit breaking for connections, requests, and outlier detection.\nPreparing for circuit breaking 🔗 Before we can configure circuit breaking, please try to access the product page app from within Meshery to ensure all the calls are making it through without errors as we did in Observability chapter\nConfigure circuit breaking 🔗 Now that we have the needed services in place, it is time to configure circuit breaking using a destination rule. Using Meshery, navigate to the Istio management page:\nEnter default in the Namespace field. Click the (+) icon on the Apply Custom Configuration card and paste the configuration below. This will update the existing destination rule definition for product page service to break the circuit if there are more than one connection and more than one pending request. In a few, we should be able to verify the destination rule by using the command below: ```sh kubectl get destinationrule productpage -o yaml Config:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage subsets: - /obserlabels: version: v1 name: v1 trafficPolicy: connectionPool: http: http1MaxPendingRequests: 1 maxRequestsPerConnection: 1 tcp: maxConnections: 1 outlierDetection: baseEjectionTime: 3m consecutiveErrors: 1 interval: 1s maxEjectionPercent: 100 tls: mode: ISTIO_MUTUAL Time to trip the circuit 🔗 In the circuit-breaker settings, we specified maxRequestsPerConnection: 1 and http1MaxPendingRequests: 1. This should mean that if we exceed more than one request per connection and more than one pending request, we should see the istio-proxy sidecar open the circuit for further requests/connections. Let us now use Meshery to make several calls to product page app by changing the number of concurrent connections to 5 from within Meshery’s Performance page.\nOnce you have updated the fields, you now click on Run Test.\nThis will run the load test and show the results in a chart.\nYou should only see a percentage of the requests succeed and the rest trapped by the configured circuit breaker.\nManual Steps 🔗 Configure circuit breaking 🔗 kubectl apply -f - \u003c\u003cEOF apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage subsets: - labels: version: v1 name: v1 trafficPolicy: tls: mode: ISTIO_MUTUAL connectionPool: tcp: maxConnections: 1 http: http1MaxPendingRequests: 1 maxRequestsPerConnection: 1 outlierDetection: consecutiveErrors: 1 interval: 1s baseEjectionTime: 3m maxEjectionPercent: 100 EOF ","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/introduction-to-service-meshes/circuit-breaking/","tags":"","title":"Circuit Breaking"},{"body":" Congratulations! 🔗 You have successfully completed the course on “Advanced concepts of service meshes - Hands on” using Istio. 🔗 ","categories":"","description":"As a self-service engineering platform, Meshery enables collaborative design and operation of cloud native infrastructure.","excerpt":"As a self-service engineering platform, Meshery enables collaborative …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/advance-concepts-of-service-meshes/conclusion/","tags":"","title":"Conclusion"},{"body":" Docker Hub 🔗 Using a publicly available repository to manage, store, retrieve, and share (if you want to) your container images is a real added value. In traditional IT scenarios, such a solution has to be custom-built most of the time. In the container world, such solutions are part of the ecosystem. With docker containers, the Docker Hub is a natural solution as a repository for our scenarios.\nDocker Hub 🔗 Video: Docker Hub Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Docker Hub 🔗 Using a publicly available repository to manage, store, …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/containers/content/dockerhub/","tags":"","title":"Docker Hub"},{"body":" Overview 🔗 Multi-Criteria Life Cycle Assessment by 2023 Certified Energy Management by 2024 Impact Calculator for Our Clients by 2024 100% Renewable Energy by 2025 Multi-Criteria Life Cycle Assessment by 2023 🔗 Exoscale is committed to conducting a life cycle assessment of our products and services under ISO 14040. This analysis will show our products and services environmental impact throughout their life cycle, from raw materials to disposal. We aim to complete this analysis by the end of 2023.\nLife Cycle Assessment (LCA) is a methodology used to evaluate the environmental impact of a product or service throughout its entire life cycle, from raw material extraction to disposal. ISO 14040 is a standard that provides guidelines for conducting LCA studies.\nThe LCA process consists of four phases:\ngoal and scope definition inventory analysis impact assessment interpretation Goal and scope definition: This phase defines the goal and scope of the study, including the purpose, intended audience, and system boundaries. The study’s goal may be to identify the environmental impact of a product or service, compare different products or services, or identify opportunities for improvement.\nInventory analysis: This phase involves collecting data on the inputs and outputs of the product or service throughout its life cycle. This includes raw material extraction, manufacturing, distribution, use, and disposal. The data collected is then used to create a life cycle inventory (LCI).\nImpact assessment: This phase evaluates the potential environmental impacts of the product or service based on the LCI data. This includes assessing the impact on climate change, resource depletion, ecosystem quality, and human health.\nInterpretation: This phase involves interpreting the results of the LCA study and communicating them to stakeholders. This includes identifying areas for improvement and making recommendations for reducing the environmental impact of the product or service.\nOverall, the LCA process provides a comprehensive approach to evaluating the environmental impact of products and services and can be used to inform decision-making and improve sustainability.\nCertified Energy Management by 2024 🔗 Exoscale is in the process of implementing an ISO 50001 Energy Management System (EnMS). This standard provides a framework for managing organizations’ energy efficiency, reducing energy costs, and minimizing environmental impact. Our goal is to complete an audit against this standard by 2024.\nISO 50001 Energy Management System (EnMS) is a globally recognized standard that provides a framework for organizations to manage their energy consumption and improve energy efficiency. The standard outlines a systematic approach to energy management, including establishing energy policies, setting energy targets and objectives, implementing energy action plans, and monitoring and reviewing energy performance.\nThe goal of ISO 50001 is to help organizations reduce their energy consumption and associated costs while reducing greenhouse gas emissions and improving sustainability. The standard applies to all types of organizations, regardless of size or industry.\nISO 50001 certification can benefit organizations, including improved energy efficiency, reduced energy costs, enhanced environmental performance, and increased competitiveness. It can also help organizations to comply with energy-related regulations and demonstrate their commitment to sustainability and corporate social responsibility.\nImpact Calculator for Our Clients by 2024 🔗 Exoscale is committed to providing the necessary information and tooling, including an impact calculator, for our clients to make quantifiable GreenOps decisions about their cloud services’ sustainability and environmental impact. Our goal is to launch this calculator by 2024.\n100% Renewable Energy by 2025 🔗 Our locations in Germany and Switzerland are powered by 100% renewable energy, in Austria by 99%, and in Bulgaria by 45%. We are committed to sourcing 100% of our energy from renewable sources for all our locations by 2025.\n","categories":"","description":"","excerpt":" Overview 🔗 Multi-Criteria Life Cycle Assessment by 2023 Certified …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/sustainable-cloud/content/goals/","tags":"","title":"Exoscale's Goals"},{"body":"","categories":"","description":"","excerpt":"","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/intro-compliance/response-support/","tags":"","title":"Exoscale's Response \u0026 Support"},{"body":" Namespace Theory 🔗 Namespaces provide separation of components and are a construct to introduce additional structure to your configurations.\nHow the construct namespace works is shown in the video below.\nNamespace Theory 🔗 Video: Namespace Theory Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Namespace Theory 🔗 Namespaces provide separation of components and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/namespace-theory/","tags":"","title":"Namespace Theory"},{"body":" Overview 🔗 In this section of the Portal, you find:\nBilling Credit Cards Invoices Subscriptions Audit-Trail Quotas Legal Billing Info 🔗 Billing Details The organization display name is used for invoices. It must be between 4 and 225 characters, cannot be composed of only numbers, and cannot be a UUID. It is not currently possible to modify the country associated with your organization. Please contact support if you need assistance. It is not currently possible to modify your organization’s VAT number.\nCredit Threshold You will receive an email notification when your credit balance drops below the specified threshold, set by default to 15 CHF/EUR/UDS. To avoid service disruptions, top up your balance regularly according to your consumption needs.\nUsage Overview \u0026 Detail Usage Overview: outlines your consumption for a specific time frame and your current billing mode. If your billing mode is set to Post-Paid, you will receive an invoice based on your monthly consumption, and your default credit card will be charged for the due amount.\nUsage Detail: provides itemized views of your consumption for the same time frame stated under Usage Overview.\nBilling Mode 🔗 Post-Paid: You will receive an invoice based on your consumption every month. Your default credit card will then be charged for the due amount. To activate the Post-Paid billing mode, you need to meet the following requirements:\nYour account must be older than 90 days All your invoices must be paid You need to have a saved credit card and set it as the default Wire-Transfer: You will receive an invoice based on your consumption every month. You have 30 days to pay your invoice by wire transfer.\nNOTE! The Wire-Transfer billing mode is activated upon request after a case-by-case examination.\nRedeem Coupon 🔗 If you have a promotional coupon, you can redeem it by entering the code in the Coupon Code field.\nCredit Cards 🔗 It is the location for determining which credit card is associated with the organization. Our payment processing partner, Adyen, safely stores credit card details.\nInvoices 🔗 You can look up all your invoices in excellent tabular form (Invoice Number, Total, Emission Date, Due Date, Status, Actions). Clicking on the table headers enables a different sorted view of the invoices.\nSubscriptions 🔗 It is the location where you can view and manage your DNS Zones and Support Plans subscriptions in excellent tabular form.\nAudit-Trail 🔗 You can see all the tracked security-relevant user activity and API usage here. The tool allows you to list and search for events that interact with Exoscale resources.\nQuotas 🔗 Is the location where you can view and manage quotas on the following specific resources:\nInstances Custom Templates Snapshots GPUs SKS Clusters Elastic IPs Private Networks Network Load Balancers IAM Access Keys DBaaS Services Object Storage Buckets Block Storage Volumes Block Storage cumulative size (GiB) Max size of a Block Storage Volume (GiB) Legal 🔗 It is structured into two Tabs:\nTerms: Here, you find the Legal Documents for your organization, including the Terms \u0026 Conditions you excepted and when, as well as the version of the Data Processing Addendum you excepted and when. Compliance Center: Exoscale is committed to helping our customers comply with industry and government regulations. Our Compliance Center contains all the information you need about our compliance posture, including information about our security controls, policies, procedures, certificates, attestations, and compliance reports. We will continue to update this center as our compliance posture evolves. For some of the reports, a Non-Disclosure Agreement (NDA) is necessary, which can be done by clicking the REVIEW AND ACCEPT button. ","categories":"","description":"","excerpt":" Overview 🔗 In this section of the Portal, you find:\nBilling Credit …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/products/content/organisation/","tags":"","title":"Organization"},{"body":" Cases 🔗 wintercloud - powered by Exoscale 🔗 Exoscale scores with an open-source approach\nThe German company wintercloud accompanies its customers to the cloud and shows them what cloud-native means. Furthermore, wintercloud demonstrates how the cloud computing architecture’s unique features can be used to their customer’s advantage. For example, the Continuous Integration / Continuous Delivery (CI / CD) method automatically enables new software development and delivery approaches that allow customers to release new versions weekly, daily, or even several times a day.\nVSHN - powered by Exoscale 🔗 Exoscale for the container platform APPUiO\nVSHN AG is the leading Swiss partner for DevOps, Docker, Kubernetes, OpenShift, and 24/7 Cloud Operations. Since 2014 VSHN has supported more than 200 different partners with more than 900 servers in the cloud. Based on the DevOps concept, VSHN’s systems engineers work with developers to create scalable, easy-to-deploy web applications. In addition, VSHN operates its web applications focusing on security, availability, and automation of development processes on various clouds in Switzerland and worldwide.\nembedded data - powered by Exoscale 🔗 Exoscale convinces with services and support\nThe embedded data GmbH, based in Saarlouis, is a software company accompanying its customers on their way into Industry 4.0 and building up the Internet of Things (IoT). With the software framework da³vid, developed by embedded data, companies can collect and evaluate machine and process data, visualize it in real time, and control devices. The goal is to increase transparency, reduce downtime through predictive maintenance, and thus increase efficiency.\n","categories":"","description":"","excerpt":" Cases 🔗 wintercloud - powered by Exoscale 🔗 Exoscale scores with an …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/exoscale/content/partner/","tags":"","title":"Partner"},{"body":"","categories":"","description":"Persistent storage solutions","excerpt":"Persistent storage solutions","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/kubernetes-workshop/main-resources/content/storage/","tags":"","title":"Storage"},{"body":"This section is a refresher that provides an overview of the main concepts used to troubleshoot a Kubernetes cluster. At the end of this section, please complete the exercises to put these concepts into practice.\nLog management 🔗 Logs allow users to:\nfollow the cluster’s activity analyze errors We must decouple log management from the workload (Containers, Pods) and from the Nodes.\nAmong the best practices:\na container must log on stdout/stderr we should ship logs to a centralized logging solution Different levels 🔗 The following picture illustrates how we can configure logging on a cluster:\nNode level: the container runtime stores the logs on the Node’s filesystem\nCluster level:\nthe container directly ships logs to a centralized logging system the container runtime stores the logs on the Node’s filesystem, and then an external process (usually deployed as a DaemonSet) reads these logs and ships them to a centralized system a sidecar container is used to generate the logs on stdin/stdout, next the container runtime stores the logs on the Node’s filesystem, and then an external process reads these logs and ships them to a centralized system Pods \u0026 Containers logs 🔗 The common way to get a Pod’s logs is using kubectl. First, we run a Pod based on the ghost image.\nkubectl run ghost --image=ghost:4 Next, we can query the Pod’s logs.\n$ kubectl logs ghost [2025-04-24 13:05:39] INFO Ghost is running in production... [2025-04-24 13:05:39] INFO Your site is now available on http://localhost:2368/ [2025-04-24 13:05:39] INFO Ctrl+C to shut down [2025-04-24 13:05:39] INFO Ghost server started in 0.974s ... We can also find these logs on the filesystem of the Node this Pod is running on. The following command tells us the Pod is running on worker1.\nkubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ghost 1/1 Running 0 6m32s 10.0.0.96 worker1 \u003cnone\u003e \u003cnone\u003e The /var/log/pods folder on that Node contains the logs of all the Pods running on that Node, including the logs of the ghost Pod.\n$ sudo ls /var/log/pods default_ghost_c502bf3e-7671-4488-af0c-5a2f0908db41 kube-system_cilium-envoy-c5vhw_6ac2a2af-3945-4069-b1cc-bb257ace3884 default_mongo_331fa933-e9cf-42f8-94c0-93fe5e5d6e82 kube-system_cilium-vhtbk_b9548366-e92e-4c97-ba78-87c7b702cf28 default_podinfo_c146441a-ba30-41cd-8e99-80feb7f12afe kube-system_kube-proxy-szfrd_17d684c2-a4ff-4316-b855-6f1ff63e5a0b We get the same content we had using kubectl.\n$ sudo cat /var/log/pods/default_ghost_c502bf3e-7671-4488-af0c-5a2f0908db41/ghost/0.log 2025-04-24T13:05:39.159890159Z stdout F [2025-04-24 13:05:39] INFO Ghost is running in production... 2025-04-24T13:05:39.161339474Z stdout F [2025-04-24 13:05:39] INFO Your site is now available on http://localhost:2368/ 2025-04-24T13:05:39.161693705Z stdout F [2025-04-24 13:05:39] INFO Ctrl+C to shut down 2025-04-24T13:05:39.165232139Z stdout F [2025-04-24 13:05:39] INFO Ghost server started in 0.974s Still from worker1, we can get containers’ logs in /var/log/containers\n$ sudo ls /var/log/containers cilium-envoy-c5vhw_kube-system_cilium-envoy-e73d79abd769cfa05f392807ca4ebacf7103b0049c19b2787e0e5128afe42f4d.log cilium-vhtbk_kube-system_apply-sysctl-overwrites-7ac45e4b75302714bdbcc44da611b733a40a199a19d160e01a9496749920f043.log cilium-vhtbk_kube-system_cilium-agent-9226fca5f50ebdc539e8045cece3e3cc9606b82e66ae67fb87a35b270fb71b96.log cilium-vhtbk_kube-system_clean-cilium-state-5d9e303fbc85ad62e0f2e41be4b74ecfe6e7d989160dcdcef915006f5b6b308d.log cilium-vhtbk_kube-system_config-3b3d66e2580dcf07baefe4dd9c792a08f7a41e16789ff07d6cb737b461b6b1a2.log cilium-vhtbk_kube-system_install-cni-binaries-c039337274f5f8e09f0d886e2be9ae71356dff5cf25a2c0ed153b3d3bf2fe656.log cilium-vhtbk_kube-system_mount-bpf-fs-935a17f160e800340dd1b9a7bdc294be3eec8628208fd0c36bb924b6345e9ed4.log cilium-vhtbk_kube-system_mount-cgroup-b20b1292b4a4daed58ec5b7291b1a3744b249b19eca208ede650761b85a2f7fa.log ghost_default_ghost-f0154dc4e4572d3827ba70717fba1caf19e0dcbea9060cde65965a424f9f3a3e.log \u003c- this one kube-proxy-szfrd_kube-system_kube-proxy-d2301ac47955299ea54ed4ed53a19d3af51b1f52156f01271a15a417db5fdd8c.log mongo_default_mongo-70628c097a2032abd76d0716e62635378befb7efb077b787581eb86d195535f4.log podinfo_default_podinfo-5e99fed167e8338e2d11b8e337fb3490522dc7c601ee83e60f85e80c5d7d4f4a.log Control plane logs 🔗 We can get the logs of the control plane components (API Server, etcd, controller-manager, and scheduler) with kubectl. The following command allows us to get the logs of the API Server running on the controlplane Node.\nkubectl -n kube-system logs kube-apiserver-controlplane The logs of these components are available on the controlplane Node, which is the Node they are running on.\n$ sudo ls -al /var/log/pods total 36 drwxr-xr-x 9 root root 4096 Apr 22 15:10 kube-system_cilium-cm954_1af09dcb-9738-458f-bb94-335505f7d713 drwxr-xr-x 3 root root 4096 Apr 22 15:10 kube-system_cilium-envoy-ksqxr_d3a09feb-0827-4e80-84e6-1a960377bf0c drwxr-xr-x 3 root root 4096 Apr 22 15:05 kube-system_etcd-controlplane_05261863f509698b43b78850b9ccfe8f drwxr-xr-x 3 root root 4096 Apr 22 15:05 kube-system_kube-apiserver-controlplane_80c4d1003f6284601e0aa670932b5ee7 drwxr-xr-x 3 root root 4096 Apr 22 15:05 kube-system_kube-controller-manager-controlplane_2c3d35add706c540cb5a3ad3a246bee9 drwxr-xr-x 3 root root 4096 Apr 22 15:05 kube-system_kube-proxy-n5lct_6b0c2017-b4b1-4ef3-9678-e3d3dc8687e8 drwxr-xr-x 3 root root 4096 Apr 22 15:05 kube-system_kube-scheduler-controlplane_4a834c796528f3fc43f3dadb50f3bd73 ... Kubelet logs 🔗 The kubelet agent, running on each Node of the cluster, is managed by systemd. We can use journalctl to get its logs.\nsudo journalctl -u kubelet | less Metrics management 🔗 In Kubernetes, metrics come in various types, originate from different layers of the stack, and are exposed by multiple components.\nTypes of Metrics:\nCPU / RAM usage Disk I/O Network activity Request and error rates Sources of Metrics:\nCluster-wide Control plane Individual Nodes Pods and containers Applications Metrics Producers:\ncAdvisor (embedded in kubelet) Metrics Server Kubernetes API Server Node Exporter kube-state-metrics Prometheus-based solution 🔗 The Prometheus stack is a widely used solution to manage Metrics in a Kubernetes cluster.\nMetrics server 🔗 The metrics-server is a lightweight component that is not installed by default in Kubernetes. It gets CPU / RAM usage in real time but does not store history. Other resources, such as HorizontalPodAutoscaler (HPA), use it to increase/decrease the number of Pods based on resource consumption.\nThe metrics-server brings additional kubectl commands to get the usage of resources in the cluster.\nGetting the CPU and RAM in use by the cluster’s Nodes kubectl top nodes Getting the CPU and RAM in use by individual Pods kubectl top pods Cluster components 🔗 Each control plane component is a static Pod. The /etc/kubernetes/manifests folder of the controlplane Node contains all their YAML specifications. These Pods are directly managed by kubelet.\n$ ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml For each static Pod, kubelet automatically creates a mirror Pod that appears in the Kubernetes API.\n$ kubectl get po -n kube-system coredns-64897985d-gfslg 1/1 Running 0 46h coredns-64897985d-q7qd2 1/1 Running 0 107s etcd-controlplane 1/1 Running 0 5d17h \u003c- mirror Pod kube-apiserver-controlplane 1/1 Running 0 5d17h \u003c- mirror Pod kube-controller-manager-controlplane 1/1 Running 1 (3d1h ago) 5d17h \u003c- mirror Pod kube-proxy-25w94 1/1 Running 0 5d17h kube-proxy-778cb 1/1 Running 0 5d17h kube-proxy-h4hbh 1/1 Running 0 5d17h kube-scheduler-controlplane 1/1 Running 1 (3d1h ago) 5d17h \u003c- mirror Pod weave-net-66dtm 2/2 Running 1 (5d17h ago) 5d17h weave-net-pfcrp 2/2 Running 1 (5d17h ago) 5d17h weave-net-zxchk 2/2 Running 1 (5d17h ago) 5d17h Troubleshooting - Examples 🔗 Application failure - Example 1 🔗 The following specification seems valid for deploying a Pod based on the elasticsearch image.\napiVersion: v1 kind: Pod metadata: name: es spec: containers: - image: elasticsearch:7.6.2 name: es After waiting a few dozen seconds following the creation of this Pod, we begin to see some errors.\n$ kubectl get pods/es -w NAME READY STATUS RESTARTS AGE es 0/1 ContainerCreating 0 10s es 1/1 Running 0 22s es 0/1 Error 1 58s es 0/1 CrashLoopBackOff 1 70s To understand the origin of these errors, we first need to use the describe command to get more details.\n$ kubectl describe po/es Events: Type Reason Age From Message ---- ------ ---- ---- ------- … Normal Pulled 52s (x4 over 3m16s) kubelet, workers-1i2u Container image \"elasticsearch:7.6.2\" already present on machine Warning BackOff 19s (x9 over 2m57s) kubelet, workers-1i2u Back-off restarting failed container Next, we verify the application logs. It shows the root cause: the value of the kernel property vm.max_map_count is too low.\n$ kubectl logs po/es [2020-03-15T17:42:15,417][INFO ][o.e.b.BootstrapChecks ] [hK4xzxV] bound or publishing to a non-loopback address, enforcing bootstrap checks ERROR: [1] bootstrap checks failed [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] [2020-03-15T17:42:15,429][INFO ][o.e.n.Node ] [hK4xzxV] stopping ... [2020-03-15T17:42:15,483][INFO ][o.e.n.Node ] [hK4xzxV] stopped We should use an initContainer and an env var to fix the thing for this specific example.\napiVersion: v1 kind: Pod metadata: name: es spec: initContainers: - name: increase-vm-max-map image: busybox command: [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"] securityContext: privileged: true containers: - image: elasticsearch:7.6.2 name: es env: - name: discovery.type value: single-node Application failure - Example 2 🔗 Let’s consider a Pod exposed with a Service.\nkubectl get po,svc -l app=ghost NAME READY STATUS RESTARTS AGE pod/ghost 1/1 Running 0 88s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ghost NodePort 10.43.121.142 \u003cnone\u003e 2368:30526/TCP 88s It happens that the application is not reachable via the NodePort provided.\nIn this case, we can first describe the Service and check the Endpoints. In the following example, Endpoints is empty, which indicates we did not configure the Service correctly.\nk describe svc ghost Name: ghost Namespace: default Labels: app=ghost Annotations: \u003cnone\u003e Selector: run=ghost Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.43.121.142 IPs: 10.43.121.142 Port: \u003cunset\u003e 2368/TCP TargetPort: 2368/TCP NodePort: \u003cunset\u003e 30526/TCP Endpoints: \u003cnone\u003e Session Affinity: None External Traffic Policy: Cluster Events: \u003cnone\u003e The list of Endpoints is empty, so the service does not expose a single Pod.\nThere is a mismatch between the Service’s selector and the pod labels:\nService’s selector is run: ghost Pod’s label is app: ghost In this case, we need to change one of them to ensure they match.\nFailure of the API Server 🔗 If kubectl commands hang, that may be because the API Server is not available.\n$ kubectl get po ... hanging From the controlplane, we first check the kubelet’s logs. In this example, the logs indicate the API Server encounters a problem to start.\nChecking kubelet logs $ sudo journalctl -u kubelet | less sudoMar 31 09:49:17 controlplane kubelet[72558]: E0331 09:49:17.443398 72558 pod_workers.go:919] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 2m40s restarting failed container=kube-apiserver pod=kube-apiserver-controlplane_kube-system(1379f6cdef52f9b598e745122eb20d6f)\\\"\" pod=\"kube-system/kube-apiserver-controlplane\" podUID=1379f6cdef52f9b598e745122eb20d6f Mar 31 09:49:18 controlplane kubelet[72558]: E0331 09:49:18.426742 72558 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"controlplane\\\": Get \\\"https://194.182.171.68:6443/api/v1/nodes/controlplane?timeout=10s\\\": context deadline exceeded\" … We retrieve the name of the API Server’s log file from the /var/log/pods folder on the controlplane Node.\n$ ls -al /var/log/pods total 40 drwxr-xr-x 10 root root 4096 Mar 31 09:45 . drwxrwxr-x 10 root syslog 4096 Mar 31 00:00 .. drwxr-xr-x 3 root root 4096 Mar 29 10:38 kube-system_coredns-64897985d-gfslg_adaa9cfe-42a4-4bc7-b5aa-eb0313b59fe7 drwxr-xr-x 3 root root 4096 Mar 28 08:55 kube-system_coredns-64897985d-mvp4t_bcfea69a-d6cc-4baf-a795-acad8fab2e47 drwxr-xr-x 3 root root 4096 Mar 25 16:05 kube-system_etcd-controlplane_6d694021cab77267a88779a2268199e6 drwxr-xr-x 3 root root 4096 Mar 31 09:44 kube-system_kube-apiserver-controlplane_1379f6cdef52f9b598e745122eb20d6f \u003c- this one drwxr-xr-x 3 root root 4096 Mar 26 13:46 kube-system_kube-controller-manager-controlplane_94d947d1226129a82876a3b7d829bbfc drwxr-xr-x 3 root root 4096 Mar 25 16:06 kube-system_kube-proxy-25w94_0c17e655-c491-43f6-b012-0eab0c7f8071 drwxr-xr-x 3 root root 4096 Mar 26 13:46 kube-system_kube-scheduler-controlplane_415ed7d85341035184628df29257fa2f drwxr-xr-x 5 root root 4096 Mar 25 16:06 kube-system_weave-net-66dtm_cef2efd7-9ea6-4604-a871-53ab915a7a84 From that file, we directly understand why the API Server cannot start: an invalid configuration option is used in its specification.\nsudo cat kube-system_kube-apiserver-controlplae_1379f6cdef52f9b598e745122eb20d6f/kube-apiserver/8.log 2022-03-31T10:00:27.785052657Z stderr F I0331 10:00:27.784813 1 server.go:565] external host was not specified, using 10.62.50.215 2022-03-31T10:00:27.785838518Z stderr F E0331 10:00:27.785689 1 run.go:74] \"command failed\" err=\"enable-admission-plugins plugin \\\"WRONG_STUFF_HERE\\\" is unknown\" Still, from the controlplane Node, we can check the API Server specification (/etc/kubernetes/manifests/kube-apiserver.yaml) and fix the incorrect configuration.\napiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.62.50.215:6443 creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --advertise-address=10.62.50.215 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/etc/kubernetes/pki/ca.crt - --enable-admission-plugins=NodeRestriction,WRONG_STUFF_HERE \u003c- ,WRONG_STUFF_HERE needs to be removed - --enable-bootstrap-token-auth=true ... Once the specification is changed, kubelet automatically restarts the API Server Pod.\nFailure of a worker node 🔗 Sometimes, a Node may not be in the Ready state as illustrated below.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION controlplane Ready control-plane 5d19h v1.32.2 worker1 NotReady \u003cnone\u003e 5d19h v1.32.2 \u003c- This Node does not seem to work worker2 Ready \u003cnone\u003e 5d19h v1.32.2 We start by getting more information about this Node to troubleshoot this issue.\nkubectl describe nodes worker1 Name: worker1 Taints: node.kubernetes.io/unreachable:NoExecute node.kubernetes.io/unreachable:NoSchedule … Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Fri, 25 Mar 2022 16:06:39 +0000 Fri, 25 Mar 2022 16:06:39 +0000 WeaveIsUp Weave pod has set this MemoryPressure Unknown Thu, 31 Mar 2022 11:40:16 +0000 Thu, 31 Mar 2022 11:43:35 +0000 NodeStatusUnknown Kubelet stopped posting node status. DiskPressure Unknown Thu, 31 Mar 2022 11:40:16 +0000 Thu, 31 Mar 2022 11:43:35 +0000 NodeStatusUnknown Kubelet stopped posting node status. PIDPressure Unknown Thu, 31 Mar 2022 11:40:16 +0000 Thu, 31 Mar 2022 11:43:35 +0000 NodeStatusUnknown Kubelet stopped posting node status. Ready Unknown Thu, 31 Mar 2022 11:40:16 +0000 Thu, 31 Mar 2022 11:43:35 +0000 NodeStatusUnknown Kubelet stopped posting node status. … The result above indicates that the kubelet process running on worker1 has stopped posting the Node’s status, which might indicate that process no longer runs. In that case, we can check the status of the kubelet systemd service on worker1.\nChecking the status of the node's kubelet sudo systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: inactive (dead) since Thu 2022-03-31 11:42:53 UTC; 4min 29s ago Docs: https://kubernetes.io/docs/home/ Process: 66511 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=0/SUCCESS) Main PID: 66511 (code=exited, status=0/SUCCESS) Mar 31 11:35:14 worker1 kubelet[66511]: I0331 11:35:14.894783 66511 image_gc_manager.go:327] \"Attempting to delete unused images\" Mar 31 11:35:14 worker1 kubelet[66511]: I0331 11:35:14.916929 66511 eviction_manager.go:349] \"Eviction manager: must evict pod(s) to reclaim\" resourceName=\"ephemeral-storage\" Mar 31 11:35:14 worker1 kubelet[66511]: I0331 11:35:14.916992 66511 eviction_manager.go:367] \"Eviction manager: pods ranked for eviction\" pods=[kube-system/weave-net-zxchk kube-system/kube-proxy-778cb] ... As kubelet is not running, we can restart it.\nsudo systemctl restart kubelet ","categories":"","description":"Troubleshoot clusters components, nodes, network and applications.","excerpt":"Troubleshoot clusters components, nodes, network and applications.","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":" Congratulations! 🔗 You have successfully completed the course on “Introduction to service meshes - Hands on” using Istio.\n","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/introduction-to-service-meshes/conclusion/","tags":"","title":"Conclusion"},{"body":" Namespace Praxis 🔗 How does this separation look like in praxis, and how to leverage the namespace concept for better orchestration.\nSee the application of this construct in praxis in the video below.\nNamespace Praxis 🔗 Video: Namespace Praxis Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Namespace Praxis 🔗 How does this separation look like in praxis, and …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/namespace-praxis/","tags":"","title":"Namespace Praxis"},{"body":"This section details some Day-2 operations, the main ones being the cluster upgrade and etcd backup/restore. At the end of this section, please complete the exercises in the order provided.\nProcess to upgrade a kubeadm cluster 🔗 Upgrading a kubeadm cluster is a very well-documented process. First, we upgrade the control plane Nodes, one at a time, following these steps:\nchange package repository get a version to upgrade to upgrade kubeadm binary run the upgrade drain the Node upgrade kubelet \u0026 kubectl uncordon the Node Next, we upgrade the worker Nodes, one at the time, following these steps:\nchange package repository upgrade kubeadm binary run the upgrade drain the Node upgrade kubelet uncordon the Node About etcd 🔗 etcd is a distributed key-value store. It is considered the brain of Kubernetes as it contains all the information about the cluster’s resources.\nAn highly available Kubernetes cluster requires multiple etcd instances. In that case the RAFT consensus algorithm maintains consistency between these instances.\nThe Secret Live Of Data is a great resource to understand how RAFT works.\nCommunicating with etcd 🔗 From the control plane Node, we can communicate with etcd using the etcdctl utility. This binary is installed on your Node.\nThe example below gets the etcd status.\nsudo ETCDCTL_API=3 etcdctl \\ --endpoints localhost:2379 \\ --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt \\ --key=/etc/kubernetes/pki/apiserver-etcd-client.key \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ endpoint health This one lists the members of the etcd cluster.\nsudo ETCDCTL_API=3 etcdctl \\ --endpoints localhost:2379 \\ --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt \\ --key=/etc/kubernetes/pki/apiserver-etcd-client.key \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ member list Creating a backup 🔗 To back up etcd, we need to run the following command which creates the file snapshot.db in the current folder.\n$ sudo ETCDCTL_API=3 etcdctl snapshot save \\ --endpoints localhost:2379 \\ --cacert /etc/kubernetes/pki/apiserver-etcd-client.crt \\ --cert /etc/kubernetes/pki/apiserver-etcd-client.key \\ --key /etc/kubernetes/pki/etcd/ca.key \\ snapshot.db Next, we verify the backup.\n$ sudo ETCDCTL_API=3 etcdctl \\ --write-out=table snapshot status snapshot.db Restore 🔗 The restoration process requires several steps.\nFirst, we restore a backup in the /var/lib/etcd-snapshot folder, this one has to be different from the one used by default, which is /var/lib/etcd.\n$ sudo ETCDCTL_API=3 etcdctl snapshot restore \\ --endpoints localhost:2379 \\ --cacert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/apiserver-etcd-client.crt \\ --cert /etc/kubernetes/pki/apiserver-etcd-client.key \\ --data-dir /var/lib/etcd-snapshot \\ snapshot.db Next, we stop the API Server. This step is important as we don’t want the API Server to mess up while etcd is not available. As the API Server is a static Pod, directly managed by kubelet, we just need to move its specification to another folder so that kubelet delete the Pod.\nsudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/ Next, we update the etcd manifests, specifying the folder from which it needs to get its data.\n… volumes: - hostPath: path: /etc/kubernetes/pki/etcd type: DirectoryOrCreate name: etcd-certs - hostPath: path: /var/lib/etcd-snapshot \u003c- New location of data type: DirectoryOrCreate name: etcd-data Then, once etcd is running, we restart the API Server, moving its specification back to the /etc/kubernetes/manifests folder.\nsudo mv /etc/kubernetes/kube-apiserver.yaml /etc/kubernetes/manifests/ ","categories":"","description":"Perform cluster upgrade and backup/restore of etcd.","excerpt":"Perform cluster upgrade and backup/restore of etcd.","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/operations/","tags":"","title":"Operations"},{"body":" Details 🔗 In this section of the Portal, you can view and manage your support tickets by status (All, New, Waiting, Open, Closed). Exoscale’s support services are designed to cater to various customer needs, from developers and testers to enterprises running critical workloads. Here’s a breakdown of what each support plan includes:\nBuilt-In Support 🔗 Built-In Support is included for all customers at no additional cost. It is ideal for testers, developers, and non-critical applications.\nInitial Response Time: Best-effort Support Hours: Office Hours Limited Audit Trail: 1 month of mutation events Limited Monthly Usage Reports: Aggregated by resource type Ticket Support Starter Plan 🔗 Starter Plan is suited for startups and SMEs running production infrastructures. It includes everything in the Built-In plan plus additional features.\nInitial Response Time: 4 hours Support Hours: Office Hours Two-Factor Authentication (2FA) Single Sign-On (SSO) Limited Audit Trail: 1 month of mutation events Monthly Usage Reports: Reporting per resource Ticket Support Price: 100.00 EUR/CHF/USD per month Pro Plan 🔗 Pro Plan is tailored for companies running sensitive production infrastructures. It includes everything from the Starter plan plus faster response times and event tracking.\nInitial Response Time: 1 hour Support Hours: Extended Office Hours Two-Factor Authentication (2FA) Single Sign-On (SSO) Comprehensive Audit Trail: All API traffic, retention at customer discretion Monthly Usage Reports: Reporting per resource Ticket Support Phone Support Price: 500.00 EUR/CHF/USD per month\nEnterprise Plan 🔗 Enterprise Plan is designed for companies running critical workloads, offering the highest level of support and fastest response times.\nInitial Response Time: 30 minutes (24/7) Support Hours: 24/7 Two-Factor Authentication (2FA) Single Sign-On (SSO) Comprehensive Audit Trail: All API traffic, retention at customer discretion Monthly Usage Reports: Reporting per resource Dedicated Customer Success Manager Custom Compliance Form Ticket Support Phone Support Price: 5% of IaaS consumption (minimum 2,500 EUR/CHF/USD per month)\nOverview: Support Features \u0026 Plans 🔗 Feature Built-In Starter Pro Enterprise Initial Response Time Best-effort 4 hours 1 hour 30 minutes Support Hours Office Hours Office Hours Extended Office Hours 24/7 Ticket Support ✔️ ✔️ ✔️ ✔️ Chat Support - - Coming soon Coming soon Phone Support - - ✔️ ✔️ Two-Factor Authentication ✔️ ✔️ ✔️ ✔️ Single Sign-On (SSO) - ✔️ ✔️ ✔️ Audit Trail Limited Limited ✔️ ✔️ Monthly Usage Reports Limited ✔️ ✔️ ✔️ Custom Compliance Form - - - ✔️ Customer Success Manager - - - ✔️ Price/month (EUR/CHF/USD) Included 100.00 500.00 Consumption based *) *) 5% of IaaS consumption (minimum 2,500 EUR/CHF/USD per month)\nAdditional Information 🔗 Office Hours: Mon-Fri, 8 am to 6 pm CET/CEST Extended Office Hours: Mon-Fri, 7 am to 8 pm CET/CEST PEN-Testing \u0026 Right to Audit: Available across all plans By choosing the right support plan, you can ensure that your needs are met effectively and promptly, allowing you to focus on what matters most—growing your business. NOTE! Here, you can find details on the case priority schema in the online documentation for SUPPORT.\n","categories":"","description":"","excerpt":" Details 🔗 In this section of the Portal, you can view and manage your …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/starter/products/content/support/","tags":"","title":"Support"},{"body":" Updates Theory 🔗 The manifest concept is very versatile for creating and orchestrating Kubernetes structures but also updating. Declaring the new situation and triggering the update, Kubernetes takes care of the rest, which means following a clearly defined workflow of process steps to update all distributed components and safely removing old versions.\nThe details are explained in the video below.\nUpdates Theory 🔗 Video: Updates Theory Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Updates Theory 🔗 The manifest concept is very versatile for creating …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/updates-theory/","tags":"","title":"Updates Theory"},{"body":" Updates Praxis 🔗 The kubectl command provides rollout and update capabilities for containerized applications and the associated Kubernetes constructs. Users expect applications to be available all the time, and developers deploy new versions of them several times a day.\nThe concept of rolling updates allows an update with zero downtime by incrementally updating Pods with new ones. With the command \u003e kubectl rollout …, you then can check the status of your deployment. You can also undo your deployment in an unsuccessful rollout and set it back to an older version.\nThe declarative nature of Kubernetes and the tracking of your actions makes undo’s and redo’s possible to keep the desired state of your application while extending or fixing the very same application. And, if necessary, make changes to the actual condition in a controlled way to reach the desired state again. Suppose done right with zero downtime for your application.\nThe video below demonstrates this rather complex sounding process in some of its possible variations. Therefore, a more extended sequence was necessary to show all the essential concepts coming together and let you experience the flexibility and stability of a Kubernetes environment.\nUpdates Praxis 🔗 Video: Updates Praxis Your browser does not support the video tag. ","categories":"","description":"","excerpt":" Updates Praxis 🔗 The kubectl command provides rollout and update …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-advance/concepts/content/updates-praxis/","tags":"","title":"Updates Praxis"},{"body":" From Docker To Kubernetes 🔗 If you want to scale all the container benefits to an entire IT environment, you need additional functions to coordinate, scale, manage, and automate. For example, taking containerized applications from your local machine to your local server can be done without additional help. Still, if you want to do this with 10, 100, or even 1000 applications and start distributing the workloads, clustering them for higher availability and stability in operations to serve the 24/7 demands of today’s customers, you need additional help. This help for containerized applications is Kubernetes. This help for a scalable, available, and more flexible infrastructure is Exoscale.\nDocker 2 Kubernetes 🔗 Video: Docker 2 Kubernetes Your browser does not support the video tag. ","categories":"","description":"","excerpt":" From Docker To Kubernetes 🔗 If you want to scale all the container …","ref":"/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/sks-starter/containers/content/docker-kubernetes/","tags":"","title":"Docker / Kubernetes"},{"body":"","categories":"","description":"This course will guide you through the process of deploying and visualizing Edge Stack components with Meshery. You will learn how to install and configure the Ambassador API Gateway and explore its integration with Meshery. Additionally, you will gain hands-on experience with two popular service meshes, Istio and Linkerd. By the end of this course, you will have a solid understanding of Edge Stack deployment and be able to leverage Meshery for managing your API gateway and service mesh configurations.","excerpt":"This course will guide you through the process of deploying and …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/ambassador-edge-stack-api-gateway-with-meshery/","tags":"","title":"Ambassador Edge Stack API Gateway with Meshery"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"A course to help you deploy wordpress and MySQL Database with persistent volumes with Meshery Playground.\n","categories":"","description":"Learn how to deploy WordPress and MySQL with persistent volumes with Meshery Playground","excerpt":"Learn how to deploy WordPress and MySQL with persistent volumes with …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/deploying-wordpress-and-mysql-with-persistent-volumes-with-meshery/","tags":"","title":"Deploying WordPress and MySQL with Persistent Volumes with Meshery"},{"body":"","categories":"","description":"Learn how Dapr works by deploying Dapr and sample applications in a Kubernetes Cluster using Meshery","excerpt":"Learn how Dapr works by deploying Dapr and sample applications in a …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/explore-dapr-with-meshery/","tags":"","title":"Explore Dapr with Meshery"},{"body":" Welcome to Layer5 Academy - Empowering Your Cloud Native Journey Master cloud native technologies through hands-on labs and structured learning paths.\nReference:\nhttps://docs.layer5.io/cloud/academy/ https://docs.layer5.io/cloud/academy/extending-the-academy/ https://docs.layer5.io/cloud/academy/creating-your-learning-path/ Layer5 Academy offers a comprehensive learning platform for cloud native enthusiasts, providing free, interactive courses and hands-on labs. Dive into Kubernetes, CNCF projects, and cloud native infrastructure management with structured learning paths tailored for beginners and advanced practitioners alike.\nInteractive Labs Engage with hands-on labs designed to provide practical experience in managing cloud native infrastructure using tools like Meshery, Kubernetes, and many other cloud technologies. Contributions Welcome! Join our open-source community! Contribute to Layer5 Academy by creating new learning paths or enhancing existing ones. Check out our GitHub repository. Read more\nCustom Learning Paths Create and share your own learning paths or explore community-driven courses to deepen your expertise in cloud native technologies. Explore Cloud Native Learning Paths {.h1 .text-center} Cloud Native Fundamentals Start with the basics of cloud native technologies, including Kubernetes, Docker, and service meshes, through beginner-friendly courses. Community-Driven Learning Join the Layer5 community to collaborate, share knowledge, and contribute to the future of cloud native education. Read more\nGet Started Today {.h1 .text-center} ","categories":"","description":"","excerpt":" Welcome to Layer5 Academy - Empowering Your Cloud Native Journey …","ref":"/","tags":"","title":"Layer5 Academy"},{"body":" Learning Paths 🔗 ","categories":"","description":"Learn about the various learning paths available in the Layer5 ecosystem.","excerpt":"Learn about the various learning paths available in the Layer5 …","ref":"/learning-paths/","tags":"","title":"Learning Paths"},{"body":" Learning Paths 🔗 ","categories":"","description":"Learn about the various learning paths available in the Layer5 ecosystem.","excerpt":"Learn about the various learning paths available in the Layer5 …","ref":"/challenges/","tags":"","title":"Learning Paths"},{"body":" Install Telemetry Add-ons 🔗 Using Meshery, install Istio telemetry add-ons. In the Istio management page:\nClick the (+) icon on the Apply Service Mesh Configuration card. Select each of the following add-ons: Prometheus Grafana Jaeger You will use Prometheus and Grafana for collecting and viewing metrics and Jaeger collecting and viewing distributed traces. Expose each add-on external to the cluster. Each the service network typs are set to “LoadBalancer”.\nService Mesh Performance and Telemetry 🔗 Many of the labs require load to be placed on the sample apps. Let’s generate HTTP traffic against the BookInfo application, so we can see interesting telemetry.\nVerify access through the Ingress Gateway:\nkubectl get service istio-ingressgateway -n istio-system Once we have the port, we can append the IP of one of the nodes to get the host.\nThe URL to run a load test against will be http://\u003cIP/hostname of any of the nodes in the cluster\u003e:\u003cingress port\u003e/productpage\nPlease note: If you are using Docker Desktop, please use the IP address of your host. You can leave the port blank. For example: http://1.2.3.4/productpage\nUse the computed URL above in Meshery, in the browser, to run a load test and see the results.\nConnect Grafana (optionally, Prometheus) to Meshery. 🔗 On the Settings page:\nNavigate to the Metrics tab. Enter Grafana’s URL:port number and submit. Use Meshery to generate load and analyze performance. 🔗 On the Performance page:\ngive this load test a memorable name enter the URL to the BookInfo productpage select Istio in the Service Mesh dropdown enter a valid number for Concurrent requests enter a valid number for Queries per second enter a valid Duration (a number followed by s for seconds (OR) m for minutes (OR) h for hour) use the host IP address in the request Tab and in the advanced options, type in the header as Host:\u003capp-name\u003e Click on Run Test. A performance test will run and statistical analysis performed. Examine the results of the test and behavior of the service mesh.\nNext, you will begin controlling requests to BookInfo using traffic management features.\nAlternative: Manual installation 🔗 Follow these steps if the above steps did not work\nInstall Add-ons: 🔗 Prometheus\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.7/samples/addons/prometheus.yaml Grafana\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.7/samples/addons/grafana.yaml Jaeger\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.7/samples/addons/jaeger.yaml Exposing services 🔗 Istio add-on services are deployed by default as ClusterIP type services. We can expose the services outside the cluster by either changing the Kubernetes service type to NodePort or LoadBalancer or by port-forwarding or by configuring Kubernetes Ingress.\nOption 1: Expose services with NodePort To expose them using NodePort service type, we can edit the services and change the service type from ClusterIP to NodePort\nOption 2: Expose services with port-forwarding Port-forwarding runs in the foreground. We have appended \u0026 to the end of the above 2 commands to run them in the background. If you donot want this behavior, please remove the \u0026 from the end.\nPrometheus 🔗 You will need to expose the Prometheus service on a port either of the two following methods:\nOption 1: Expose services with NodePort\nkubectl -n istio-system edit svc prometheus To find the assigned ports for Prometheus:\nkubectl -n istio-system get svc prometheus Option 2: Expose Prometheus service with port-forwarding: ** Expose Prometheus service with port-forwarding:\nkubectl -n istio-system port-forward \\ $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath='{.items[0].metadata.name}') \\ 9090:9090 \u0026 Browse to http://\u003cip\u003e:\u003cport\u003e and in the Expression input box enter: istio_request_bytes_count. Click the Execute button.\nGrafana 🔗 You will need to expose the Grafana service on a port either of the two following methods:\nkubectl -n istio-system edit svc grafana Once this is done the services will be assigned dedicated ports on the hosts.\nTo find the assigned ports for Grafana:\nkubectl -n istio-system get svc grafana Expose Grafana service with port-forwarding:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana \\ -o jsonpath='{.items[0].metadata.name}') 3000:3000 \u0026 Distributed Tracing 🔗 The sample Bookinfo application is configured to collect trace spans using Zipkin or Jaeger. Although Istio proxies are able to automatically send spans, it needs help from the application to tie together the entire trace. To do this applications need to propagate the appropriate HTTP headers so that when the proxies send span information to Zipkin or Jaeger, the spans can be correlated correctly into a single trace.\nTo do this the application collects and propagates the following headers from the incoming request to any outgoing requests:\nx-request-id x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags x-ot-span-context Exposing services 🔗 Istio add-on services are deployed by default as ClusterIP type services. We can expose the services outside the cluster by either changing the Kubernetes service type to NodePort or LoadBalancer or by port-forwarding or by configuring Kubernetes Ingress. In this lab, we will briefly demonstrate the NodePort and port-forwarding ways of exposing services.\nOption 1: Expose services with NodePort To expose them using NodePort service type, we can edit the services and change the service type from ClusterIP to NodePort\nFor Jaeger, either of tracing or jaeger-query can be exposed.\nkubectl -n istio-system edit svc tracing Once this is done the services will be assigned dedicated ports on the hosts.\nTo find the assigned ports for Jaeger:\nkubectl -n istio-system get svc tracing Option 2: Expose services with port-forwarding\nTo port-forward Jaeger:\nkubectl -n istio-system port-forward \\ $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath='{.items[0].metadata.name}') \\ 16686:16686 \u0026 View Traces 🔗 Let us find the port Jaeger is exposed on by running the following command:\nkubectl -n istio-system get svc tracing You can click on the link at the top of the page which maps to the right port and it will open Jaeger UI in a new tab.\n","categories":"","description":"Meshery, collaborative Kubernetes manager","excerpt":"Meshery, collaborative Kubernetes manager","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-service-meshes-for-developers/advance-concepts-of-service-meshes/observability/","tags":"","title":"Observability with Istio"},{"body":"A course that will help you to install a Scalable PostgreSQL Distribution on Kubernetes with Cloud Native PostgreSQL\n","categories":"","description":"Learn how to Install a Scalable PostgreSQL Distribution on Kubernetes with Cloud Native PostgreSQL","excerpt":"Learn how to Install a Scalable PostgreSQL Distribution on Kubernetes …","ref":"/learning-paths/11111111-1111-1111-1111-111111111111/mastering-kubernetes-for-engineers/scalable-postgres-with-cloudnativepg/","tags":"","title":"Scalable Postgres with CloudNativePG"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"}]