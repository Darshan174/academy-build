<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=alternate type=application/json href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/troubleshooting/index.json><meta name=robots content="index, follow"><link rel="shortcut icon" href=https://cloud.layer5.io/academy/favicons/favicon.ico><link rel=apple-touch-icon href=https://cloud.layer5.io/academy/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=https://cloud.layer5.io/academy/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=https://cloud.layer5.io/academy/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://cloud.layer5.io/academy/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=https://cloud.layer5.io/academy/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=https://cloud.layer5.io/academy/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=https://cloud.layer5.io/academy/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=https://cloud.layer5.io/academy/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=https://cloud.layer5.io/academy/favicons/android-192x192.png sizes=192x192><title>Troubleshooting | Layer5 Academy</title><meta name=description content="Troubleshoot clusters components, nodes, network and applications."><meta property="og:url" content="https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/troubleshooting/"><meta property="og:site_name" content="Layer5 Academy"><meta property="og:title" content="Troubleshooting"><meta property="og:description" content="Troubleshoot clusters components, nodes, network and applications."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Troubleshooting"><meta itemprop=description content="Troubleshoot clusters components, nodes, network and applications."><meta itemprop=wordCount content="2064"><meta name=twitter:card content="summary"><meta name=twitter:title content="Troubleshooting"><meta name=twitter:description content="Troubleshoot clusters components, nodes, network and applications."><link rel=preload href=https://cloud.layer5.io/academy/scss/main.min.8d61368d36274adb6675933a7937cfa090b823abdb581eede811dfeab0c3628e.css as=style integrity="sha256-jWE2jTYnSttmdZM6eTfPoJC4I6vbWB7t6BHf6rDDYo4=" crossorigin=anonymous><link href=https://cloud.layer5.io/academy/scss/main.min.8d61368d36274adb6675933a7937cfa090b823abdb581eede811dfeab0c3628e.css rel=stylesheet integrity="sha256-jWE2jTYnSttmdZM6eTfPoJC4I6vbWB7t6BHf6rDDYo4=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="form-control td-search__input" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=https://cloud.layer5.io/academy/offline-search-index.94a3a33c75bba8d6f2c5d5bab9ea1aaf.json data-offline-search-base-href=https://cloud.layer5.io/academy/ data-offline-search-max-results=10></div><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ms-3 fas fa-bars" type=button data-bs-toggle=collapse data-bs-target=#td-section-nav aria-controls=td-section-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="td-sidebar-nav collapse" id=td-section-nav><ul class="td-sidebar-nav__section pe-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prep-li><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prep><span>Certified Kubernetes Administrator (CKA) Preparation</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentcertifications-li><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/certifications/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentcertifications><span>Certifications</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentcreation-li><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/creation/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentcreation><span>Create a cluster</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentworkload-li><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/workload/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentworkload><span>Workloads</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentscheduling-li><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/scheduling/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentscheduling><span>Scheduling Pods</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentnetworking-li><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/networking/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentnetworking><span>Networking</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentstorage-li><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/storage/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentstorage><span>Storage</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentsecurity-li><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/security/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentsecurity><span>Security</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontenttroubleshooting-li><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/troubleshooting/ class="align-left ps-0 active td-sidebar-link td-sidebar-link__section" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontenttroubleshooting><span class=td-sidebar-nav-active-item>Troubleshooting</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentoperations-li><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/operations/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-learning-paths98e16360-a366-4b78-8e0a-031da07fdacbcka-prepcontentoperations><span>Operations</span></a></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ms-2 pb-1 pt-2 mb-0"></div></aside><main class="col-12 col-md-9 col-xl-8" role=main><div class="page-header d-flex justify-content-between align-items-center"><div class=breadcrumb-container><nav aria-label=breadcrumb class=td-breadcrumbs><ol class=breadcrumb><li class=breadcrumb-item><a href=https://cloud.layer5.io/academy/learning-paths/>Learning Paths</a></li><li class=breadcrumb-item><a href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/>Certified Kubernetes Administrator (CKA) Preparation</a></li><li class="breadcrumb-item active" aria-current=page>Troubleshooting</li></ol></nav></div></div><div class=td-content><h1>Troubleshooting</h1><div class=lead>Troubleshoot clusters components, nodes, network and applications.</div><header class=article-meta><p class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>&nbsp; 10 minute read &nbsp;</p></header><p>This section is a refresher that provides an overview of the main concepts used to troubleshoot a Kubernetes cluster. At the end of this section, please complete the exercises to put these concepts into practice.</p><h2 id=log-management class=heading-link>Log management
<a href=#log-management class=heading-anchor aria-label="Permalink to this heading">🔗</a></h2><hr><p>Logs allow users to:</p><ul><li>follow the cluster&rsquo;s activity</li><li>analyze errors</li></ul><p>We must decouple log management from the workload (Containers, Pods) and from the Nodes.</p><p>Among the <strong>best practices</strong>:</p><ul><li>a container must log on stdout/stderr</li><li>we should ship logs to a centralized logging solution</li></ul><h3 id=different-levels class=heading-link>Different levels
<a href=#different-levels class=heading-anchor aria-label="Permalink to this heading">🔗</a></h3><p>The following picture illustrates how we can configure logging on a cluster:</p><p><strong>Node level</strong>: the container runtime stores the logs on the Node&rsquo;s filesystem</p><p><strong>Cluster level</strong>:</p><ul><li>the container directly ships logs to a centralized logging system</li><li>the container runtime stores the logs on the Node&rsquo;s filesystem, and then an external process (usually deployed as a DaemonSet) reads these logs and ships them to a centralized system</li><li>a sidecar container is used to generate the logs on stdin/stdout, next the container runtime stores the logs on the Node&rsquo;s filesystem, and then an external process reads these logs and ships them to a centralized system</li></ul><p><div class=md__image><img id="[68 246 425 374 410 146]" src=logging-levels.png onclick=openModal(this.id) alt=logging-levels class=md-image-responsive></div></p><h3 id=pods--containers-logs class=heading-link>Pods & Containers logs
<a href=#pods--containers-logs class=heading-anchor aria-label="Permalink to this heading">🔗</a></h3><p>The common way to get a Pod&rsquo;s logs is using kubectl. First, we run a Pod based on the ghost image.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl run ghost --image<span class=o>=</span>ghost:4
</span></span></code></pre></div><p>Next, we can query the Pod&rsquo;s logs.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl logs ghost
</span></span><span class=line><span class=cl><span class=o>[</span>2025-04-24 13:05:39<span class=o>]</span> INFO Ghost is running in production...
</span></span><span class=line><span class=cl><span class=o>[</span>2025-04-24 13:05:39<span class=o>]</span> INFO Your site is now available on http://localhost:2368/
</span></span><span class=line><span class=cl><span class=o>[</span>2025-04-24 13:05:39<span class=o>]</span> INFO Ctrl+C to shut down
</span></span><span class=line><span class=cl><span class=o>[</span>2025-04-24 13:05:39<span class=o>]</span> INFO Ghost server started in 0.974s
</span></span><span class=line><span class=cl>...
</span></span></code></pre></div><p>We can also find these logs on the filesystem of the Node this Pod is running on. The following command tells us the Pod is running on worker1.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get po -o wide
</span></span><span class=line><span class=cl>NAME    READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
</span></span><span class=line><span class=cl>ghost   1/1     Running   <span class=m>0</span>          6m32s   10.0.0.96   worker1   &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div><p>The <code>/var/log/pods</code> folder on that Node contains the logs of all the Pods running on that Node, including the logs of the ghost Pod.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ sudo ls /var/log/pods
</span></span><span class=line><span class=cl>default_ghost_c502bf3e-7671-4488-af0c-5a2f0908db41        kube-system_cilium-envoy-c5vhw_6ac2a2af-3945-4069-b1cc-bb257ace3884
</span></span><span class=line><span class=cl>default_mongo_331fa933-e9cf-42f8-94c0-93fe5e5d6e82        kube-system_cilium-vhtbk_b9548366-e92e-4c97-ba78-87c7b702cf28
</span></span><span class=line><span class=cl>default_podinfo_c146441a-ba30-41cd-8e99-80feb7f12afe      kube-system_kube-proxy-szfrd_17d684c2-a4ff-4316-b855-6f1ff63e5a0b
</span></span></code></pre></div><p>We get the same content we had using kubectl.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ sudo cat /var/log/pods/default_ghost_c502bf3e-7671-4488-af0c-5a2f0908db41/ghost/0.log
</span></span><span class=line><span class=cl>2025-04-24T13:05:39.159890159Z stdout F <span class=o>[</span>2025-04-24 13:05:39<span class=o>]</span> INFO Ghost is running in production...
</span></span><span class=line><span class=cl>2025-04-24T13:05:39.161339474Z stdout F <span class=o>[</span>2025-04-24 13:05:39<span class=o>]</span> INFO Your site is now available on http://localhost:2368/
</span></span><span class=line><span class=cl>2025-04-24T13:05:39.161693705Z stdout F <span class=o>[</span>2025-04-24 13:05:39<span class=o>]</span> INFO Ctrl+C to shut down
</span></span><span class=line><span class=cl>2025-04-24T13:05:39.165232139Z stdout F <span class=o>[</span>2025-04-24 13:05:39<span class=o>]</span> INFO Ghost server started in 0.974s
</span></span></code></pre></div><p>Still from worker1, we can get containers&rsquo; logs in <code>/var/log/containers</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ sudo ls /var/log/containers
</span></span><span class=line><span class=cl>cilium-envoy-c5vhw_kube-system_cilium-envoy-e73d79abd769cfa05f392807ca4ebacf7103b0049c19b2787e0e5128afe42f4d.log
</span></span><span class=line><span class=cl>cilium-vhtbk_kube-system_apply-sysctl-overwrites-7ac45e4b75302714bdbcc44da611b733a40a199a19d160e01a9496749920f043.log
</span></span><span class=line><span class=cl>cilium-vhtbk_kube-system_cilium-agent-9226fca5f50ebdc539e8045cece3e3cc9606b82e66ae67fb87a35b270fb71b96.log
</span></span><span class=line><span class=cl>cilium-vhtbk_kube-system_clean-cilium-state-5d9e303fbc85ad62e0f2e41be4b74ecfe6e7d989160dcdcef915006f5b6b308d.log
</span></span><span class=line><span class=cl>cilium-vhtbk_kube-system_config-3b3d66e2580dcf07baefe4dd9c792a08f7a41e16789ff07d6cb737b461b6b1a2.log
</span></span><span class=line><span class=cl>cilium-vhtbk_kube-system_install-cni-binaries-c039337274f5f8e09f0d886e2be9ae71356dff5cf25a2c0ed153b3d3bf2fe656.log
</span></span><span class=line><span class=cl>cilium-vhtbk_kube-system_mount-bpf-fs-935a17f160e800340dd1b9a7bdc294be3eec8628208fd0c36bb924b6345e9ed4.log
</span></span><span class=line><span class=cl>cilium-vhtbk_kube-system_mount-cgroup-b20b1292b4a4daed58ec5b7291b1a3744b249b19eca208ede650761b85a2f7fa.log
</span></span><span class=line><span class=cl>ghost_default_ghost-f0154dc4e4572d3827ba70717fba1caf19e0dcbea9060cde65965a424f9f3a3e.log &lt;- this one
</span></span><span class=line><span class=cl>kube-proxy-szfrd_kube-system_kube-proxy-d2301ac47955299ea54ed4ed53a19d3af51b1f52156f01271a15a417db5fdd8c.log
</span></span><span class=line><span class=cl>mongo_default_mongo-70628c097a2032abd76d0716e62635378befb7efb077b787581eb86d195535f4.log
</span></span><span class=line><span class=cl>podinfo_default_podinfo-5e99fed167e8338e2d11b8e337fb3490522dc7c601ee83e60f85e80c5d7d4f4a.log
</span></span></code></pre></div><h3 id=control-plane-logs class=heading-link>Control plane logs
<a href=#control-plane-logs class=heading-anchor aria-label="Permalink to this heading">🔗</a></h3><p>We can get the logs of the control plane components (API Server, etcd, controller-manager, and scheduler) with kubectl. The following command allows us to get the logs of the API Server running on the controlplane Node.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl -n kube-system logs kube-apiserver-controlplane
</span></span></code></pre></div><p>The logs of these components are available on the controlplane Node, which is the Node they are running on.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ sudo ls -al /var/log/pods
</span></span><span class=line><span class=cl>total <span class=m>36</span>
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>9</span> root root <span class=m>4096</span> Apr <span class=m>22</span> 15:10 kube-system_cilium-cm954_1af09dcb-9738-458f-bb94-335505f7d713
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>3</span> root root <span class=m>4096</span> Apr <span class=m>22</span> 15:10 kube-system_cilium-envoy-ksqxr_d3a09feb-0827-4e80-84e6-1a960377bf0c
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>3</span> root root <span class=m>4096</span> Apr <span class=m>22</span> 15:05 kube-system_etcd-controlplane_05261863f509698b43b78850b9ccfe8f
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>3</span> root root <span class=m>4096</span> Apr <span class=m>22</span> 15:05 kube-system_kube-apiserver-controlplane_80c4d1003f6284601e0aa670932b5ee7
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>3</span> root root <span class=m>4096</span> Apr <span class=m>22</span> 15:05 kube-system_kube-controller-manager-controlplane_2c3d35add706c540cb5a3ad3a246bee9
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>3</span> root root <span class=m>4096</span> Apr <span class=m>22</span> 15:05 kube-system_kube-proxy-n5lct_6b0c2017-b4b1-4ef3-9678-e3d3dc8687e8
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>3</span> root root <span class=m>4096</span> Apr <span class=m>22</span> 15:05 kube-system_kube-scheduler-controlplane_4a834c796528f3fc43f3dadb50f3bd73
</span></span><span class=line><span class=cl>...
</span></span></code></pre></div><h3 id=kubelet-logs class=heading-link>Kubelet logs
<a href=#kubelet-logs class=heading-anchor aria-label="Permalink to this heading">🔗</a></h3><p>The kubelet agent, running on each Node of the cluster, is managed by systemd. We can use journalctl to get its logs.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo journalctl -u kubelet <span class=p>|</span> less
</span></span></code></pre></div><h2 id=metrics-management class=heading-link>Metrics management
<a href=#metrics-management class=heading-anchor aria-label="Permalink to this heading">🔗</a></h2><hr><p>In Kubernetes, metrics come in various types, originate from different layers of the stack, and are exposed by multiple components.</p><p><strong>Types of Metrics</strong>:</p><ul><li>CPU / RAM usage</li><li>Disk I/O</li><li>Network activity</li><li>Request and error rates</li></ul><p><strong>Sources of Metrics</strong>:</p><ul><li>Cluster-wide</li><li>Control plane</li><li>Individual Nodes</li><li>Pods and containers</li><li>Applications</li></ul><p><strong>Metrics Producers</strong>:</p><ul><li>cAdvisor (embedded in kubelet)</li><li>Metrics Server</li><li>Kubernetes API Server</li><li>Node Exporter</li><li>kube-state-metrics</li></ul><h3 id=prometheus-based-solution class=heading-link>Prometheus-based solution
<a href=#prometheus-based-solution class=heading-anchor aria-label="Permalink to this heading">🔗</a></h3><p>The Prometheus stack is a widely used solution to manage Metrics in a Kubernetes cluster.</p><p><div class=md__image><img id="[395 148 495 93 339 301]" src=monitoring-prometheus.png onclick=openModal(this.id) alt=monitoring-prometheus class=md-image-responsive></div></p><h3 id=metrics-server class=heading-link>Metrics server
<a href=#metrics-server class=heading-anchor aria-label="Permalink to this heading">🔗</a></h3><p>The metrics-server is a lightweight component that is not installed by default in Kubernetes. It gets CPU / RAM usage in real time but does not store history. Other resources, such as HorizontalPodAutoscaler (HPA), use it to increase/decrease the number of Pods based on resource consumption.</p><p>The metrics-server brings additional kubectl commands to get the usage of resources in the cluster.</p><ul><li><strong>Getting the CPU and RAM in use by the cluster&rsquo;s Nodes</strong></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl top nodes
</span></span></code></pre></div><ul><li><strong>Getting the CPU and RAM in use by individual Pods</strong></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl top pods
</span></span></code></pre></div><h2 id=cluster-components class=heading-link>Cluster components
<a href=#cluster-components class=heading-anchor aria-label="Permalink to this heading">🔗</a></h2><hr><p>Each control plane component is a static Pod. The <code>/etc/kubernetes/manifests</code> folder of the controlplane Node contains all their YAML specifications. These Pods are directly managed by kubelet.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ls /etc/kubernetes/manifests
</span></span><span class=line><span class=cl>etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
</span></span></code></pre></div><p>For each static Pod, kubelet automatically creates a mirror Pod that appears in the Kubernetes API.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl get po -n kube-system
</span></span><span class=line><span class=cl>coredns-64897985d-gfslg                  1/1     Running   <span class=m>0</span>          46h
</span></span><span class=line><span class=cl>coredns-64897985d-q7qd2                  1/1     Running   <span class=m>0</span>          107s
</span></span><span class=line><span class=cl>etcd-controlplane                        1/1     Running   <span class=m>0</span>          5d17h   &lt;- mirror Pod
</span></span><span class=line><span class=cl>kube-apiserver-controlplane              1/1     Running   <span class=m>0</span>          5d17h   &lt;- mirror Pod
</span></span><span class=line><span class=cl>kube-controller-manager-controlplane     1/1     Running   <span class=m>1</span> <span class=o>(</span>3d1h ago<span class=o>)</span>   5d17h   &lt;- mirror Pod
</span></span><span class=line><span class=cl>kube-proxy-25w94                         1/1     Running   <span class=m>0</span>          5d17h
</span></span><span class=line><span class=cl>kube-proxy-778cb                         1/1     Running   <span class=m>0</span>          5d17h
</span></span><span class=line><span class=cl>kube-proxy-h4hbh                         1/1     Running   <span class=m>0</span>          5d17h
</span></span><span class=line><span class=cl>kube-scheduler-controlplane              1/1     Running   <span class=m>1</span> <span class=o>(</span>3d1h ago<span class=o>)</span>   5d17h   &lt;- mirror Pod
</span></span><span class=line><span class=cl>weave-net-66dtm                          2/2     Running   <span class=m>1</span> <span class=o>(</span>5d17h ago<span class=o>)</span>   5d17h
</span></span><span class=line><span class=cl>weave-net-pfcrp                          2/2     Running   <span class=m>1</span> <span class=o>(</span>5d17h ago<span class=o>)</span>   5d17h
</span></span><span class=line><span class=cl>weave-net-zxchk                          2/2     Running   <span class=m>1</span> <span class=o>(</span>5d17h ago<span class=o>)</span>   5d17h
</span></span></code></pre></div><h2 id=troubleshooting---examples class=heading-link>Troubleshooting - Examples
<a href=#troubleshooting---examples class=heading-anchor aria-label="Permalink to this heading">🔗</a></h2><hr><h3 id=application-failure---example-1 class=heading-link>Application failure - Example 1
<a href=#application-failure---example-1 class=heading-anchor aria-label="Permalink to this heading">🔗</a></h3><p>The following specification seems valid for deploying a Pod based on the elasticsearch image.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>es</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>elasticsearch:7.6.2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>es</span><span class=w>
</span></span></span></code></pre></div><p>After waiting a few dozen seconds following the creation of this Pod, we begin to see some errors.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl get pods/es -w
</span></span><span class=line><span class=cl>NAME   READY   STATUS              RESTARTS   AGE
</span></span><span class=line><span class=cl>es     0/1     ContainerCreating   <span class=m>0</span>          10s
</span></span><span class=line><span class=cl>es     1/1     Running             <span class=m>0</span>          22s
</span></span><span class=line><span class=cl>es     0/1     Error               <span class=m>1</span>          58s
</span></span><span class=line><span class=cl>es     0/1     CrashLoopBackOff    <span class=m>1</span>          70s
</span></span></code></pre></div><p>To understand the origin of these errors, we first need to use the describe command to get more details.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl describe po/es
</span></span><span class=line><span class=cl>Events:
</span></span><span class=line><span class=cl>  Type     Reason     Age                   From               Message
</span></span><span class=line><span class=cl>  ----     ------     ----                  ----               -------
</span></span><span class=line><span class=cl>  …
</span></span><span class=line><span class=cl>  Normal   Pulled     52s <span class=o>(</span>x4 over 3m16s<span class=o>)</span>   kubelet, workers-1i2u   Container image <span class=s2>&#34;elasticsearch:7.6.2&#34;</span> already present on machine
</span></span><span class=line><span class=cl>  Warning  BackOff    19s <span class=o>(</span>x9 over 2m57s<span class=o>)</span>   kubelet, workers-1i2u   Back-off restarting failed container
</span></span></code></pre></div><p>Next, we verify the application logs. It shows the root cause: the value of the kernel property <code>vm.max_map_count</code> is too low.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl logs po/es
</span></span><span class=line><span class=cl><span class=o>[</span>2020-03-15T17:42:15,417<span class=o>][</span>INFO <span class=o>][</span>o.e.b.BootstrapChecks    <span class=o>]</span> <span class=o>[</span>hK4xzxV<span class=o>]</span> bound or publishing to a non-loopback address, enforcing bootstrap checks
</span></span><span class=line><span class=cl>ERROR: <span class=o>[</span>1<span class=o>]</span> bootstrap checks failed
</span></span><span class=line><span class=cl><span class=o>[</span>1<span class=o>]</span>: max virtual memory areas vm.max_map_count <span class=o>[</span>65530<span class=o>]</span> is too low, increase to at least <span class=o>[</span>262144<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=o>[</span>2020-03-15T17:42:15,429<span class=o>][</span>INFO <span class=o>][</span>o.e.n.Node               <span class=o>]</span> <span class=o>[</span>hK4xzxV<span class=o>]</span> stopping ...
</span></span><span class=line><span class=cl><span class=o>[</span>2020-03-15T17:42:15,483<span class=o>][</span>INFO <span class=o>][</span>o.e.n.Node               <span class=o>]</span> <span class=o>[</span>hK4xzxV<span class=o>]</span> stopped
</span></span></code></pre></div><p>We should use an initContainer and an env var to fix the thing for this specific example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>es</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>initContainers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>increase-vm-max-map</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>busybox</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;sysctl&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;-w&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;vm.max_map_count=262144&#34;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>securityContext</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>privileged</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>elasticsearch:7.6.2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>es</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>discovery.type</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=l>single-node</span><span class=w>
</span></span></span></code></pre></div><h3 id=application-failure---example-2 class=heading-link>Application failure - Example 2
<a href=#application-failure---example-2 class=heading-anchor aria-label="Permalink to this heading">🔗</a></h3><p>Let&rsquo;s consider a Pod exposed with a Service.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get po,svc -l <span class=nv>app</span><span class=o>=</span>ghost
</span></span><span class=line><span class=cl>NAME        READY   STATUS    RESTARTS   AGE
</span></span><span class=line><span class=cl>pod/ghost   1/1     Running   <span class=m>0</span>          88s
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>NAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>          AGE
</span></span><span class=line><span class=cl>service/ghost   NodePort   10.43.121.142   &lt;none&gt;        2368:30526/TCP   88s
</span></span></code></pre></div><p>It happens that the application is not reachable via the NodePort provided.</p><p><div class=md__image><img id="[498 459 420 157 259 382]" src=application-failure-2.png onclick=openModal(this.id) alt=application-failure-2 class=md-image-responsive></div></p><p>In this case, we can first describe the Service and check the Endpoints. In the following example, <strong>Endpoints is empty</strong>, which indicates we did not configure the Service correctly.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>k describe svc ghost
</span></span><span class=line><span class=cl>Name:                     ghost
</span></span><span class=line><span class=cl>Namespace:                default
</span></span><span class=line><span class=cl>Labels:                   <span class=nv>app</span><span class=o>=</span>ghost
</span></span><span class=line><span class=cl>Annotations:              &lt;none&gt;
</span></span><span class=line><span class=cl>Selector:                 <span class=nv>run</span><span class=o>=</span>ghost
</span></span><span class=line><span class=cl>Type:                     NodePort
</span></span><span class=line><span class=cl>IP Family Policy:        SingleStack
</span></span><span class=line><span class=cl>IP Families:              IPv4
</span></span><span class=line><span class=cl>IP:                       10.43.121.142
</span></span><span class=line><span class=cl>IPs:                      10.43.121.142
</span></span><span class=line><span class=cl>Port:                     &lt;unset&gt;  2368/TCP
</span></span><span class=line><span class=cl>TargetPort:               2368/TCP
</span></span><span class=line><span class=cl>NodePort:                 &lt;unset&gt;  30526/TCP
</span></span><span class=line><span class=cl>Endpoints:                &lt;none&gt;
</span></span><span class=line><span class=cl>Session Affinity:         None
</span></span><span class=line><span class=cl>External Traffic Policy:  Cluster
</span></span><span class=line><span class=cl>Events:                   &lt;none&gt;
</span></span></code></pre></div><p>The list of Endpoints is empty, so the service does not expose a single Pod.</p><p>There is a mismatch between the Service&rsquo;s selector and the pod labels:</p><ul><li>Service&rsquo;s selector is <code>run: ghost</code></li><li>Pod&rsquo;s label is <code>app: ghost</code></li></ul><p>In this case, we need to change one of them to ensure they match.</p><h3 id=failure-of-the-api-server class=heading-link>Failure of the API Server
<a href=#failure-of-the-api-server class=heading-anchor aria-label="Permalink to this heading">🔗</a></h3><p>If kubectl commands hang, that may be because the API Server is not available.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl get po
</span></span><span class=line><span class=cl>... hanging
</span></span></code></pre></div><p>From the controlplane, we first check the kubelet&rsquo;s logs. In this example, the logs indicate the API Server encounters a problem to start.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Checking kubelet logs
</span></span><span class=line><span class=cl>$ sudo journalctl -u kubelet <span class=p>|</span> less
</span></span><span class=line><span class=cl>sudoMar <span class=m>31</span> 09:49:17 controlplane kubelet<span class=o>[</span>72558<span class=o>]</span>: E0331 09:49:17.443398 <span class=m>72558</span> pod_workers.go:919<span class=o>]</span> <span class=s2>&#34;Error syncing pod, skipping&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to \&#34;StartContainer\&#34; for \&#34;kube-apiserver\&#34; with CrashLoopBackOff: \&#34;back-off 2m40s restarting failed container=kube-apiserver pod=kube-apiserver-controlplane_kube-system(1379f6cdef52f9b598e745122eb20d6f)\&#34;&#34;</span> <span class=nv>pod</span><span class=o>=</span><span class=s2>&#34;kube-system/kube-apiserver-controlplane&#34;</span> <span class=nv>podUID</span><span class=o>=</span>1379f6cdef52f9b598e745122eb20d6f
</span></span><span class=line><span class=cl>Mar <span class=m>31</span> 09:49:18 controlplane kubelet<span class=o>[</span>72558<span class=o>]</span>: E0331 09:49:18.426742 <span class=m>72558</span> kubelet_node_status.go:460<span class=o>]</span> <span class=s2>&#34;Error updating node status, will retry&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;error getting node \&#34;controlplane\&#34;: Get \&#34;https://194.182.171.68:6443/api/v1/nodes/controlplane?timeout=10s\&#34;: context deadline exceeded&#34;</span>
</span></span><span class=line><span class=cl>…
</span></span></code></pre></div><p>We retrieve the name of the API Server&rsquo;s log file from the <code>/var/log/pods</code> folder on the controlplane Node.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ls -al /var/log/pods
</span></span><span class=line><span class=cl>total <span class=m>40</span>
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>10</span> root root <span class=m>4096</span> Mar <span class=m>31</span> 09:45 .
</span></span><span class=line><span class=cl>drwxrwxr-x <span class=m>10</span> root syslog <span class=m>4096</span> Mar <span class=m>31</span> 00:00 ..
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>3</span> root root <span class=m>4096</span> Mar <span class=m>29</span> 10:38 kube-system_coredns-64897985d-gfslg_adaa9cfe-42a4-4bc7-b5aa-eb0313b59fe7
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>3</span> root root <span class=m>4096</span> Mar <span class=m>28</span> 08:55 kube-system_coredns-64897985d-mvp4t_bcfea69a-d6cc-4baf-a795-acad8fab2e47
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>3</span> root root <span class=m>4096</span> Mar <span class=m>25</span> 16:05 kube-system_etcd-controlplane_6d694021cab77267a88779a2268199e6
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>3</span> root root <span class=m>4096</span> Mar <span class=m>31</span> 09:44 kube-system_kube-apiserver-controlplane_1379f6cdef52f9b598e745122eb20d6f  &lt;- this one
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>3</span> root root <span class=m>4096</span> Mar <span class=m>26</span> 13:46 kube-system_kube-controller-manager-controlplane_94d947d1226129a82876a3b7d829bbfc
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>3</span> root root <span class=m>4096</span> Mar <span class=m>25</span> 16:06 kube-system_kube-proxy-25w94_0c17e655-c491-43f6-b012-0eab0c7f8071
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>3</span> root root <span class=m>4096</span> Mar <span class=m>26</span> 13:46 kube-system_kube-scheduler-controlplane_415ed7d85341035184628df29257fa2f
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>5</span> root root <span class=m>4096</span> Mar <span class=m>25</span> 16:06 kube-system_weave-net-66dtm_cef2efd7-9ea6-4604-a871-53ab915a7a84
</span></span></code></pre></div><p>From that file, we directly understand why the API Server cannot start: an invalid configuration option is used in its specification.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo cat kube-system_kube-apiserver-controlplae_1379f6cdef52f9b598e745122eb20d6f/kube-apiserver/8.log
</span></span><span class=line><span class=cl>2022-03-31T10:00:27.785052657Z stderr F I0331 10:00:27.784813       <span class=m>1</span> server.go:565<span class=o>]</span> external host was not specified, using 10.62.50.215
</span></span><span class=line><span class=cl>2022-03-31T10:00:27.785838518Z stderr F E0331 10:00:27.785689       <span class=m>1</span> run.go:74<span class=o>]</span> <span class=s2>&#34;command failed&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;enable-admission-plugins plugin \&#34;WRONG_STUFF_HERE\&#34; is unknown&#34;</span>
</span></span></code></pre></div><p>Still, from the controlplane Node, we can check the API Server specification (<code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>) and fix the incorrect configuration.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint</span><span class=p>:</span><span class=w> </span><span class=m>10.62.50.215</span><span class=p>:</span><span class=m>6443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>creationTimestamp</span><span class=p>:</span><span class=w> </span><span class=kc>null</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>component</span><span class=p>:</span><span class=w> </span><span class=l>kube-apiserver</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>tier</span><span class=p>:</span><span class=w> </span><span class=l>control-plane</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>kube-apiserver</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>command</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>kube-apiserver</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- --<span class=l>advertise-address=10.62.50.215</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- --<span class=l>allow-privileged=true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- --<span class=l>authorization-mode=Node,RBAC</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- --<span class=l>client-ca-file=/etc/kubernetes/pki/ca.crt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- --<span class=l>enable-admission-plugins=NodeRestriction,WRONG_STUFF_HERE  &lt;- ,WRONG_STUFF_HERE needs to be removed</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- --<span class=l>enable-bootstrap-token-auth=true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div><p>Once the specification is changed, kubelet automatically restarts the API Server Pod.</p><h3 id=failure-of-a-worker-node class=heading-link>Failure of a worker node
<a href=#failure-of-a-worker-node class=heading-anchor aria-label="Permalink to this heading">🔗</a></h3><p>Sometimes, a Node may not be in the Ready state as illustrated below.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl get nodes
</span></span><span class=line><span class=cl>NAME           STATUS     ROLES           AGE     VERSION
</span></span><span class=line><span class=cl>controlplane   Ready      control-plane   5d19h   v1.32.2
</span></span><span class=line><span class=cl>worker1        NotReady   &lt;none&gt;          5d19h   v1.32.2  &lt;- This Node does not seem to work
</span></span><span class=line><span class=cl>worker2        Ready      &lt;none&gt;          5d19h   v1.32.2
</span></span></code></pre></div><p>We start by getting more information about this Node to troubleshoot this issue.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl describe nodes worker1
</span></span><span class=line><span class=cl>Name:               worker1
</span></span><span class=line><span class=cl>Taints:             node.kubernetes.io/unreachable:NoExecute
</span></span><span class=line><span class=cl>                   node.kubernetes.io/unreachable:NoSchedule
</span></span><span class=line><span class=cl>…
</span></span><span class=line><span class=cl>Conditions:
</span></span><span class=line><span class=cl>  Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
</span></span><span class=line><span class=cl>  ----                 ------    -----------------                 ------------------                ------              -------
</span></span><span class=line><span class=cl>  NetworkUnavailable   False     Fri, <span class=m>25</span> Mar <span class=m>2022</span> 16:06:39 +0000   Fri, <span class=m>25</span> Mar <span class=m>2022</span> 16:06:39 +0000   WeaveIsUp           Weave pod has <span class=nb>set</span> this
</span></span><span class=line><span class=cl>  MemoryPressure       Unknown   Thu, <span class=m>31</span> Mar <span class=m>2022</span> 11:40:16 +0000   Thu, <span class=m>31</span> Mar <span class=m>2022</span> 11:43:35 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
</span></span><span class=line><span class=cl>  DiskPressure         Unknown   Thu, <span class=m>31</span> Mar <span class=m>2022</span> 11:40:16 +0000   Thu, <span class=m>31</span> Mar <span class=m>2022</span> 11:43:35 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
</span></span><span class=line><span class=cl>  PIDPressure          Unknown   Thu, <span class=m>31</span> Mar <span class=m>2022</span> 11:40:16 +0000   Thu, <span class=m>31</span> Mar <span class=m>2022</span> 11:43:35 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
</span></span><span class=line><span class=cl>  Ready                Unknown   Thu, <span class=m>31</span> Mar <span class=m>2022</span> 11:40:16 +0000   Thu, <span class=m>31</span> Mar <span class=m>2022</span> 11:43:35 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
</span></span><span class=line><span class=cl>…
</span></span></code></pre></div><p>The result above indicates that the kubelet process running on worker1 has stopped posting the Node&rsquo;s status, which might indicate that process no longer runs. In that case, we can check the status of the kubelet systemd service on worker1.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Checking the status of the node<span class=err>&#39;</span>s kubelet
</span></span><span class=line><span class=cl>sudo systemctl status kubelet
</span></span><span class=line><span class=cl>● kubelet.service - kubelet: The Kubernetes Node Agent
</span></span><span class=line><span class=cl>   Loaded: loaded <span class=o>(</span>/lib/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
</span></span><span class=line><span class=cl>  Drop-In: /etc/systemd/system/kubelet.service.d
</span></span><span class=line><span class=cl>           └─10-kubeadm.conf
</span></span><span class=line><span class=cl>   Active: inactive <span class=o>(</span>dead<span class=o>)</span> since Thu 2022-03-31 11:42:53 UTC<span class=p>;</span> 4min 29s ago
</span></span><span class=line><span class=cl>     Docs: https://kubernetes.io/docs/home/
</span></span><span class=line><span class=cl>  Process: <span class=m>66511</span> <span class=nv>ExecStart</span><span class=o>=</span>/usr/bin/kubelet <span class=nv>$KUBELET_KUBECONFIG_ARGS</span> <span class=nv>$KUBELET_CONFIG_ARGS</span> <span class=nv>$KUBELET_KUBEADM_ARGS</span> <span class=nv>$KUBELET_EXTRA_ARGS</span> <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited, <span class=nv>status</span><span class=o>=</span>0/SUCCESS<span class=o>)</span>
</span></span><span class=line><span class=cl> Main PID: <span class=m>66511</span> <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited, <span class=nv>status</span><span class=o>=</span>0/SUCCESS<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Mar <span class=m>31</span> 11:35:14 worker1 kubelet<span class=o>[</span>66511<span class=o>]</span>: I0331 11:35:14.894783   <span class=m>66511</span> image_gc_manager.go:327<span class=o>]</span> <span class=s2>&#34;Attempting to delete unused images&#34;</span>
</span></span><span class=line><span class=cl>Mar <span class=m>31</span> 11:35:14 worker1 kubelet<span class=o>[</span>66511<span class=o>]</span>: I0331 11:35:14.916929   <span class=m>66511</span> eviction_manager.go:349<span class=o>]</span> <span class=s2>&#34;Eviction manager: must evict pod(s) to reclaim&#34;</span> <span class=nv>resourceName</span><span class=o>=</span><span class=s2>&#34;ephemeral-storage&#34;</span>
</span></span><span class=line><span class=cl>Mar <span class=m>31</span> 11:35:14 worker1 kubelet<span class=o>[</span>66511<span class=o>]</span>: I0331 11:35:14.916992   <span class=m>66511</span> eviction_manager.go:367<span class=o>]</span> <span class=s2>&#34;Eviction manager: pods ranked for eviction&#34;</span> <span class=nv>pods</span><span class=o>=[</span>kube-system/weave-net-zxchk kube-system/kube-proxy-778cb<span class=o>]</span>
</span></span><span class=line><span class=cl>...
</span></span></code></pre></div><p>As kubelet is not running, we can restart it.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo systemctl restart kubelet
</span></span></code></pre></div><div class=section-index></div><nav class="pagination-nav mt-5" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/security/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label><i class="fa fa-chevron-left"></i> Security</div></a><a class="pagination-nav__link pagination-nav__link--next" href=https://cloud.layer5.io/academy/learning-paths/98e16360-a366-4b78-8e0a-031da07fdacb/cka-prep/content/operations/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>Operations <i class="fa fa-chevron-right"></i></div></a></nav></div></main></div></div></div><script src=https://cloud.layer5.io/academy/js/main.min.93a3bf75eba7f67cb62933bdc16aff4dc235ec3930d0090a0bdf2a9941f29237.js integrity="sha256-k6O/deun9ny2KTO9wWr/TcI17Dkw0AkKC98qmUHykjc=" crossorigin=anonymous></script><script defer src=https://cloud.layer5.io/academy/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://cloud.layer5.io/academy/js/tabpane-persist.js></script><div id=myModal class=modal><button class=modal-close onclick=closeModal()>close</button><div class=modal-cont><img class=modal-pic id=modalPic onclick=closeModal() style=max-width:100%;max-height:80vh;margin:auto alt=Modal-pic></div></div><script>function openModal(e){var t;typeof e=="string"?t=document.getElementById(e).src:e instanceof HTMLImageElement&&(t=e.src),t&&t.includes("#")&&(t=t.substring(0,t.indexOf("#"))),document.getElementById("modalPic").src=t,document.getElementById("myModal").style.display="block"}function closeModal(){document.getElementById("modalPic").src="",document.getElementById("myModal").style.display="none"}document.addEventListener("keydown",function(e){e.key==="Escape"&&closeModal()}),document.addEventListener("DOMContentLoaded",function(){var e,t=document.querySelectorAll("img");t.forEach(function(e){e.onclick=function(){e.dataset.modal!=="false"&&openModal(e)}}),e=document.getElementById("myModal"),e.addEventListener("click",function(){closeModal()})})</script></body></html>